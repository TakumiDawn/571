# Analysis of Covariance (ANCOVA)

```{r}
library(faraway)   # for the data 
library(ggplot2)   # my favorite graphing package
library(dplyr)     # for the %>% operator
```


One way that we could extend the ANOVA and regression models is to have both categorical and continuous predictor variables. This model is commonly called ANCOVA which stands for *Analysis of Covariance*.

The dataset `teengamb` in the package `faraway` has data regarding the rates of gambling among teenagers in Britain and their gender and socioeconomic status. One question we might be interested in is how gender and income relate to how much a person gambles. But what should be the effect of gender be?

There are two possible ways that gender could enter the model. Either:

1. We could fit two lines to the data one for males and one for females but require that the lines be parallel (i.e. having the same slopes for income). This is accomplished by having a separate y-intercept for each gender. In effect, the line for the females would be offset by a constant amount from the male line. 

2. We could fit two lines but but allow the slopes to differ as well as the y-intercept. This is referred to as an “interaction” between income and gender.

```{r, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(faraway)
teengamb$sex <- factor(teengamb$sex, labels=c('Male','Female')) 
m1 <- lm( gamble ~ sex + income, data=teengamb )  
m2 <- lm( gamble ~ sex * income, data=teengamb )  
yhat.1 <- predict(m1) 
yhat.2 <- predict(m2) 
temp <- rbind( cbind(teengamb,yhat=yhat.1,method='Additive'),
               cbind(teengamb,yhat=yhat.2,method='Interaction') ) 
ggplot(temp, aes(x=income, col=sex)) + 
      geom_point(aes(y=gamble)) +
      geom_line(aes(y=yhat)) +
      facet_wrap(~ method) 
```

We will now see how to go about fitting these two models. As might be imagined, these can be fit in the same fashion we have been solving the linear models, but require a little finesse in defining the appropriate design matrix $\boldsymbol{X}$.

## Offset parallel Lines (aka additive models)

In order to get offset parallel lines, we want to write a model 
$$y_{i}=\begin{cases}
\beta_{0}+\beta_{1}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if female}\\
\beta_{0}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if male}
\end{cases}$$
where $\beta_{1}$ is the vertical offset of the female group regression line to the reference group, which is the males regression line. Because the first $19$ observations are female, we can this in in matrix form as 
$$\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{19}\\
y_{20}\\
\vdots\\
y_{47}
\end{array}\right]=\left[\begin{array}{ccc}
1 & 1 & x_{1}\\
\vdots & \vdots & \vdots\\
1 & 1 & x_{19}\\
1 & 0 & x_{20}\\
\vdots & \vdots & \vdots\\
1 & 0 & x_{47}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\vdots\\
\epsilon_{19}\\
\epsilon_{20}\\
\vdots\\
\epsilon_{47}
\end{array}\right]$$

I like this representation where $\beta_{1}$ is the offset from the male regression line because it makes it very convenient to test if the offset is equal to zero. The second column of the design matrix referred to as a “dummy variable” or “indicator variable” that codes for the female gender. Notice that even though I have two genders, I only had to add one additional variable to my model because we already had a y-intercept $\beta_{0}$ and we only added one indicator variable for females.

What if we had a third group? Then we would fit another column of indicator variable for the third group. The new beta coefficient in the model would be the offset of the new group to the reference group. For example we consider $n=9$ observations with $n_i=3$ observations per group where $y_{i,j}$is the $j$ th replication of the $i$th group.
$$\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 0 & 0 & x_{1,1}\\
1 & 0 & 0 & x_{1,2}\\
1 & 0 & 0 & x_{1,3}\\
1 & 1 & 0 & x_{2,1}\\
1 & 1 & 0 & x_{2,2}\\
1 & 1 & 0 & x_{2,3}\\
1 & 0 & 1 & x_{3,1}\\
1 & 0 & 1 & x_{3,2}\\
1 & 0 & 1 & x_{3,3}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{3,3}\\
\epsilon_{3,2}\\
\epsilon_{3,3}
\end{array}\right]
$$ 

In this model, $\beta_0$ is the y-intercept for group $1$. The paramter $\beta_1$ is the vertical offset from the reference group (group $1$) for the second group.  Similarly $\beta_2$ is the vertical offset for group $3$. All groups will share the same slope, $\beta_4$.  

## Lines with different slopes (aka Interaction model)

We can now include a discrete random variable and create regression lines that are parallel, but often that is inappropriate, such as in the teenage gambling dataset. We want to be able to fit a model that has different slopes.
$$y_{i}=\begin{cases}
\left(\beta_{0}+\beta_{1}\right)+\left(\beta_{2}+\beta_{3}\right)x_{i}+\epsilon_{i} & \;\;\;\textrm{if female}\\
\beta_{0}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if male}
\end{cases}
$$
Where $\beta_{1}$ is the offset in y-intercept of the female group from the male group, and $\beta_{3}$ is the offset in slope. Now our matrix formula looks like

$$\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{19}\\
y_{20}\\
\vdots\\
y_{47}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 1 & x_{1} & x_{1}\\
\vdots & \vdots & \vdots & \vdots\\
1 & 1 & x_{19} & x_{19}\\
1 & 0 & x_{20} & 0\\
\vdots & \vdots & \vdots & \vdots\\
1 & 0 & x_{47} & 0
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\vdots\\
\epsilon_{19}\\
\epsilon_{20}\\
\vdots\\
\epsilon_{47}
\end{array}\right]
$$
where the new fourth column is the what I would get if I multiplied the $\boldsymbol{x}$ column element-wise with the dummy-variable column. To fit this model in R we have
```{r}
library(ggplot2)
library(faraway)

# Forces R to recognize that 0, 1 are categorical, also
# relabels the levels to something I understand.
teengamb$sex <- factor(teengamb$sex, labels=c('Male','Female')) 

# Fit a linear model with the interaction of sex and income
# Interactions can be specified useing a colon :
m1 <- lm( gamble ~ sex + income + sex:income, data=teengamb )  

# R allows a shortcut for the prior definition
m1 <- lm( gamble ~ sex * income, data=teengamb )

# save the y.hat values for each observation
teengamb$gamble.hat <- predict(m1)

# Make a nice plot that includes the regreesion line.
ggplot(teengamb, aes(x=income, col=sex)) + 
      geom_point(aes(y=gamble)) +
      geom_line(aes(y=gamble.hat))

# print the model summary
summary(m1)
```

## Iris Example

For a second example, we will explore the relationship between sepal length and sepal width for three species of irises. This data set is available in R as `iris`. 

```{r}
data(iris)            # read in the iris dataset
levels(iris$Species)  # notice the order of levels of Species
```

The very first thing we should do when encountering a dataset is to do some sort of graphical summary to get an idea of what model seems appropriate.

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point()
```

Looking at this graph, it seems that I will likely have a model with different y-intercepts for each species, but it isn't clear to me if we need different slopes.

We consider the sequence of building successively more complex models:


```{r}
# make virginica the reference group 
iris$Species <- relevel(iris$Species, ref='virginica')

m1 <- lm( Sepal.Width ~ Sepal.Length, data=iris )            # One line
m2 <- lm( Sepal.Width ~ Sepal.Length + Species, data=iris )  # Parallel Lines
m3 <- lm( Sepal.Width ~ Sepal.Length * Species, data=iris )  # Non-parallel Lines
```

The three models we consider are the following:
```{r, fig.height=6, echo=FALSE}
yhat.1 <- predict(m1) 
yhat.2 <- predict(m2) 
yhat.3 <- predict(m3)
temp <- rbind( 
	cbind(iris, yhat=yhat.1, method='m1'),
	cbind(iris, yhat=yhat.2, method='m2'),
	cbind(iris, yhat=yhat.3, method='m3'))
ggplot(temp, aes(x=Sepal.Length, col=Species)) +
  geom_point(aes(y=Sepal.Width)) + 
  geom_line(aes(y=yhat)) + 
  facet_grid(method ~ .) 
```

Looking at these, it seems obvious that the simplest model where we ignore Species is horrible. The other two models seem decent, and I am not sure about the parallel lines model vs the differing slopes model. 

```{r}
summary(m1)$coefficients %>% round(digits=3)  
```

For the simplest model, there is so much unexplained noise that the slope variable isn't significant.

Moving onto the next most complicated model, where each species has their own y-intercept, but they share a slope, we have
```{r}
summary(m2)$coefficients %>% round(digits=3)  
```

The first two lines are the y-intercept and slope associated with the reference group and the last two lines are the y-intercept offsets from the reference group to *Setosa* and *Versicolor*, respectively. We have that the slope associated with increasing Sepal Length is significant and that *Setosa* has a statistically different y-intercept than the reference group *Virginica* and that *Versicolor* does not have a statstically different y-intercept than the reference group.

Finally we consider the most complicated model that includes two more slope parameters
```{r}
summary(m3)$coefficients %>% round(digits=3)  
```

These parameters are:

  Meaning                                |   R-label                      
-----------------------------------------|--------------------------------
 *Reference group y-intercept *          |  (Intercept)       
 *Reference group slope *                |  Sepal.Length                  
 *offset to y-intercept for Setosa*      |  Speciessetosa                 
 *offset to y-intercept for Versicolor*  |  Speciesversicolor              
 *offset to slope for Setosa*            |  Sepal.Length:Speciessetosa     
 *offset to slope for Versicolor*        |  Sepal.Length:Speciesversicolor 

It appears that slope for *Setosa* is different from the reference group *Virginica*. However because we've added $2$ parameters to the model, testing Model2 vs Model3 is not equivalent to just looking at the p-value for that one slope.  Instead we need to look at the F-test comparing the two models which will evaluate if the decrease in SSE is sufficient to justify the addition of two paramters.

```{r}
anova(m2, m3)
```

The F-test concludes that there is sufficent decrease in the SSE to justify adding two additional parameters to the model. 

## Exercises
1. We will analyze data from an experiment to study factors affecting the production of the plastic PVC. In the study, 3 `operators used` 8 different devices (called `resin` railcars?). For each operator-resin combination, 2 replicate observations were made.

    a) Load the faraway package and the dataset pvc via the following commands:
        ```{r}
        library(faraway)
        data(pvc)
        ```
    b) Graph the dataset using `psize` as the response variable and `resin` and `operator` as the response. Explore different ways of displaying the relationship (which covariate should be the color or perhaps use faceting) and describe the trend you see. Is there an obvious interaction between `resin` and `operator`?

    c) Fit the 2-way main effects model and the interaction model.

    d) Compare the two models using an F-test and explain which is a superior model and why.

    e) Reproduce your graph from part (b), but include the predicted value for each treatment combination. Summarize your findings.

2. We will analyze data from an experiment that investigated the growth of *Listeria monocytogenes* bacteria in the liver and spleen of 20 mice. Ten wildtype and ten RIP2 gene deficient mice were inoculated with *L. monocytogenes* and three days later, the spleen or liver was harvested. Researchers are believe the absence of the RIP2 might affect the two organs differently. 

    a) Download and install the package isdals (data sets from the text book Introduction to Statistical Data Analysis for the Life Sciences) and then load the dataset listeria.
        ```{r, eval=FALSE}
        install.packages('isdals')
        library(isdals)
        data(listeria)
        ?listeria
        ```

    b) The response variable `growth` is a measure of bacterial growth, but a log transformation should be performed. Create a new column in the dataset named `log.growth` that is the log transformed growth.

    c) Graph the dataset using `log.growth` as the response variable and `organ` and `type` as the response. Explore different ways of displaying the relationship (which covariate should be the color or perhaps use faceting) and describe the trend you see. Is there an obvious interaction?

    d) Fit the 2-way main effects model and the interaction model.

    e) Compare the two models using an F-test and explain which is a superior model and why.

    f) Reproduce your graph from part (b), but include the predicted value for each treatment combination. Summarize your findings.

    g) For the following cases, show the necessary calculation (and result) using the $\hat{\beta}$ values from the summary table of the interaction model. You don't need to make R do the addition or subtraction, but I want to make sure you know how to take the summary output and figure out the following:

        i. What is the estimated log.growth of the liver for a RIP2 deficient mouse? 
        ii. What is the estimated log.growth of the spleen for a RIP2 deficient mouse?
        iii. What is the estimated log.growth of the liver for a wild-type moue?
        iv. What is the estimated log.growth of the spleen for a wild-type mouse?