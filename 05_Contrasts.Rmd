# Contrasts

We often are interested in estimating a function of the parameters $\boldsymbol{\beta}$. For example in the offset representation of the ANOVA model with 3 groups we have
$$y_{ij}=\mu+\tau_{i}+\epsilon_{ij}$$
where 
$$
\boldsymbol{\beta}=\left[\mu\;\tau_{2}\;\tau_{3}\right]^{T}
$$
and $\mu$ is the mean of the control group, group one is the control group and thus $\tau_{1}=0$, and $\tau_{2}$ and $\tau_{3}$ are the offsets of group two and three from the control group. In this representation, the mean of group two is $\mu+\tau_{2}$ and is estimated with $\hat{\mu} + \hat{\tau}_2$.

## Estimate and variance

A contrast is a linear combinations of elements of $\boldsymbol{\hat{\beta}}$, which is a fancy way of saying that it is a function of the elements of $\boldsymbol{\hat{\beta}}$
  where the elements can be added, subtracted, or multiplied by constants. In particular, the contrast can be represented by the vector $\boldsymbol{c}$ such that the function we are interested in is $\boldsymbol{c}^{T}\boldsymbol{\hat{\beta}}$.

In the ANOVA case with $k=3$ where we have the offset representation, I might be interested in the mean of group 2, which could be written as 
$$
\mu+\tau_{2}=\underset{\boldsymbol{c}^{T}}{\underbrace{\left[\begin{array}{ccc}
1 & 1 & 0\end{array}\right]}}\cdot\underset{\hat{\boldsymbol{\beta}}}{\underbrace{\left[\begin{array}{c}
\hat{\mu}\\
\hat{\tau_{2}}\\
\hat{\tau_{3}}
\end{array}\right]}}
$$

Similarly in the simple regression case, I will be interested in the height of the regression line at $x_0$. This height can be written as  
$$
\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}=\underset{\boldsymbol{c}^{T}}{\underbrace{\left[\begin{array}{cc}
1 & x_{0}\end{array}\right]}}\cdot\underset{\hat{\boldsymbol{\beta}}}{\underbrace{\left[\begin{array}{c}
\hat{\beta}_{0}\\
\hat{\beta}_{1}
\end{array}\right]}}
$$
In this manner, we could think of the predicted values $\hat{y}_i$ as just the result of the contrasts $\boldsymbol{X}^T \hat{\boldsymbol{\beta}}$ where our design matrix takes the role of the contrasts.

One of the properties of maximum likelihood estimator (MLEs), is that they are invariant under transformations. Meaning that since $\hat{\boldsymbol{\beta}}$ is the MLE of $\boldsymbol{\beta}$, then $\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}$ is the MLE of $\boldsymbol{c}^{T}\boldsymbol{\beta}$. The only thing we need to perform hypotheses tests and create confidence intervals is an estimate of the variance of $\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}$. 

Because we know the variance of $\hat{\boldsymbol{\beta}}$ is
$$
Var\left(\hat{\boldsymbol{\beta}}\right)=\sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}
$$
and because $\boldsymbol{c}$ is a constant, then 
$$
Var\left(\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\right)=\sigma^{2}\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}
$$
and the standard error is found by plugging in our estimate of $\sigma^{2}$ and taking the square root.
$$
StdErr\left(\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\right)	=	\sqrt{\hat{\sigma}^{2}\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}
	=	\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}
$$ 

As usual, we can now calculate confidence intervals for $\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}$ using the usual formula
$$Est	\pm	t_{n-p}^{1-\alpha/2}\;StdErr\left(\,Est\,\right)$$
$$
\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}	\pm	t_{n-p}^{1-\alpha/2}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}
$$ 


Recall the hostility example which was an ANOVA with three groups with the data

 Method  |	Test Scores
---------|-------------------------
  1	     | 96	79	91	85	83	91	82	87	
  2	     | 77	76	74	73	78	71	80		
  3	     | 66	73	69	66	77	73	71	70	74

We have analyzed this data using both the cell means model and the offset and we will demonstrate how to calculate the group means from the offset representation. Thus we are interested in estimating $\mu+\tau_{2}$ and $\mu+\tau_{3}$.  I am also interested in estimating the difference between treatment 2 and 3 and will therefore be interested in estimating $\tau_{2} - \tau_{3}$.

```{r}
y <- c(96,79,91,85,83,91,82,87,
       77,76,74,73,78,71,80,
       66,73,69,66,77,73,71,70,74)
groups <- factor(c( rep('Group1',8), rep('Group2',7),rep('Group3',9) ))
```

We can fit the offset model and obtain the design matrix and estimate of $\hat{\sigma}$ via the following code. 
```{r}
m <- lm(y ~ groups)            # Fit the ANOVA model (offset representation)
coef(m)                        # Show me beta.hat
X <- model.matrix(m)           # obtains the design matrix
sigma.hat <- summary(m)$sigma  # create the summary table and grab sigma.hat 
beta.hat <- coef(m)
XtX.inv <- solve( t(X) %*% X )
```

Now we calculate 
```{r}
contr <- c(1,1,0)  # define my contrast
ctb <- t(contr) %*% beta.hat
std.err <- sigma.hat * sqrt( t(contr) %*% XtX.inv %*% contr )
ctb
std.err
```


and notice this is the exact same estimate and standard error we got for group two when we fit the cell means model.
```{r}
CellMeansModel <- lm(y ~ groups - 1)
summary(CellMeansModel)$coefficients
```


## Estimating contrasts in R

Instead of us doing all the matrix calculations ourselves, all we really need is to is specify the row vector $\boldsymbol{c}^{T}$. The function that will do the rest of the calculations is the generalized linear hypothesis test function `glht()` that can be found in the multiple comparisons package `multcomp`. The p-values will be adjusted to correct for testing multiple hypothesis, so there may be slight differences compared to the p-value seen in just the regular summary table.

### 1-way ANOVA

We will again use the Hostility data set and demonstrate how to calculate the point estimates, standard errors and confidence intervals for the group means given a model fit using the offset representation. 

```{r}
y <- c(96,79,91,85,83,91,82,87,
       77,76,74,73,78,71,80,
       66,73,69,66,77,73,71,70,74)
groups <- factor(c( rep('Group1',8), rep('Group2',7),rep('Group3',9) ))
m <- lm(y ~ groups)
summary(m)$coefficients
```

We will now define a row vector (and it needs to be a matrix or else `glht()` will throw an error. First we note that the simple contrast $\boldsymbol{c}^{T}=\left[1\;0\;0\right]$ just grabs the first coefficient and gives us the same estimate and standard error as the summary did.
```{r, message=FALSE, warning=FALSE}
library(multcomp)
contr <- rbind("Intercept"=c(1,0,0)) # 1x3 matrix with row named "Intercept"
test <- glht(m, linfct=contr)        # the linear function to be tested is contr
summary(test)
```


Next we calculate the estimate of all the group means $\mu$, $\mu+\tau_{2}$ and $\mu+\tau_{3}$ and the difference between group 2 and 3. Notice I can specify more than one contrast at a time.
```{r}
contr <- rbind("Mean of Group 1"=c(1,0,0),
               "Mean of Group 2"=c(1,1,0),
               "Mean of Group 3"=c(1,0,1),
               "Diff G2-G3"     =c(0,1,-1)) 
test <- glht(m, linfct=contr)
summary(test)
```

Finally we calculate confidence intervals in the usual manner using the `confint()` function.
```{r}
confint(test, level=0.95)
```


### ANCOVA example

In the ANCOVA case, there are more interesting contrasts to be made. For this example we will use the `ToothGrowth` dataset from the faraway package. This data measuring the effects of different dose levels of vitamin C on tooth growth of guinea pigs. The two different delivery methods are encoded by the variable supp which has levels of orange juice (OJ) and ascorbic acid (VC). 

We first fit a ANCOVA model with an interaction between log(dose) level and delivery method and graph the result.

```{r, fig.height=4, warning=FALSE, message=FALSE}
library(ggplot2)
library(faraway)
data(ToothGrowth)
ToothGrowth$logDose <- log( ToothGrowth$dose )
m <- lm(len ~ logDose * supp, data=ToothGrowth)

# predict() gives me the yhat values and optional CI
# these are just the "contrasts" defined by X matrix! 
ToothGrowth$len.hat <- predict(m, interval='confidence')[,1]
ToothGrowth$len.lwr <- predict(m, interval='confidence')[,2]
ToothGrowth$len.upr <- predict(m, interval='confidence')[,3]

# Plot the results using ggplot2
ggplot( ToothGrowth, aes(x=logDose, col=supp, fill=supp)) +
    geom_point(aes(y=len)) +
    geom_line(aes(y=len.hat)) +
    geom_ribbon(aes(ymin=len.lwr, ymax=len.upr), alpha=.2)
```


R has fit this model using the offset representation. First we present the summary coefficients so we know which parameters are which.

```{r}
summary(m)$coefficient
```

For a warm-up, we will calculate the y-intercepts of both groups and the slopes of both. For the OJ group, this is just the 1st and 2nd coefficients, while for the VC group it is $\beta_{0}+\beta_{2}$ and $\beta_{1}+\beta_{3}$. 

```{r}
# Add a heading so that I know which parameter is which!
#                                Int  logDose  suppVC   logDose:suppVC
contr <- rbind("Intercept OJ" = c(1,     0,     0,          0          ),
               "Slope     OJ" = c(0,     1,     0,          0          ),
               "Intercept VC" = c(1,     0,     1,          0          ),
               "Slope     VC" = c(0,     1,     0,          1          ) ) 
test <- glht(m, linfct=contr)
summary(test)
```

In our original table of summary coefficients, the (intercept) term corresponds to the tooth growth of a guinea pig when the logDose level is 0 (and therefore dose=1). If I wanted to estimate the tooth growth of a guinea pig fed OJ but with only $1/2$ a dose (and therefore $logDose=\log\left(1/2\right)=-0.69$), then we want 

\begin{eqnarray*}
\hat{y}_{oj,dose=0.5}	& = &	\hat{\beta}_{0}+\hat{\beta}_{1}\log\left(0.5\right)\\
	                    & = &	20.6+9.25\log\left(0.5\right)\\
	                    & = &	20.6+9.25\left(-0.69\right)\\
	                    & = &	14.21
\end{eqnarray*}

which I can write as 
$$y_{oj,dose=0.5}	=	\left[\begin{array}{cccc}
1 & -0.69 & 0 & 0\end{array}\right]\left[\begin{array}{c}
\hat{\beta_{0}}\\
\hat{\beta}_{1}\\
\hat{\beta}_{2}\\
\hat{\beta}_{3}
\end{array}\right]
	=	\boldsymbol{c}_{1}^{T}\hat{\boldsymbol{\beta}}
$$ 

To calculate the same value for the VC group, we need the following contrast:

$$\begin{aligned}
\hat{y}_{vc,dose=0.5}	
    &=	16.9633+13.0997\,\log\left(0.5\right)\\
    &=	\left(20.66-3.7\right)+\left(9.25+3.8\right)\,\log\left(0.5\right)\\
	  &=	\left(\hat{\beta}_{0}+\hat{\beta}_{2}\right)+\left(\hat{\beta}_{1}+\hat{\beta}_{4}\right)\,\left(-0.69\right)\\
	  &=	\left[\begin{array}{cccc}1 & -0.69 & 1 & -0.69\end{array}\right]\left[\begin{array}{c}
                 \hat{\beta_{0}}\\
                 \hat{\beta}_{1}\\
                 \hat{\beta}_{2}\\
                 \hat{\beta}_{3}\end{array}\right]\\
	  &=	\boldsymbol{c}_{2}^{T}\hat{\boldsymbol{\beta}}
	  \end{aligned}$$


```{r}
#                                Int  logDose  suppVC   logDose:suppVC
contr <- rbind("OJ; 1/2 dose" = c(1,   -0.69,    0,          0         ),
               "VC; 1/2 dose" = c(1,   -0.69,    1,        -0.69       ))
test <- glht(m, linfct=contr)
summary(test)
```

Finally we might be interested in testing if there is a treatment difference between OJ and VC at a 1/2
  dose level. So to do this, we want to calculate the difference between these two previous contrasts, i.e.

$$\begin{aligned}
\boldsymbol{c}_{1}^{T}\hat{\boldsymbol{\beta}}-\boldsymbol{c}_{2}^{T}\hat{\boldsymbol{\beta}}	&=	\left(\boldsymbol{c}_{1}^{T}-\boldsymbol{c}_{2}^{T}\right) \hat{\boldsymbol{ \beta }} \\
	&=	\left(\left[\begin{array}{cccc}
1 & -0.69 & 0 & 0\end{array}\right]-\left[\begin{array}{cccc}
1 & -0.69 & 1 & -0.69\end{array}\right]\right)\hat{\boldsymbol{\beta}} \\
	&=	\left[\begin{array}{cccc}
0 & 0 & -1 & 0.69\end{array}\right]\hat{\boldsymbol{\beta}}
\end{aligned}$$

  
and we can calculate
```{r}
contr <- rbind("OJ - VC; 1/2 dose" = c(0, 0, -1, 0.69))
test <- glht(m, linfct=contr)
summary(test)
```


When we do the same test, but at dose level 2 (logDose =$\log2=0.69$) we see that the difference is not statistically significant.
```{r}
contr <- rbind("OJ - VC; dose 2" = c(0, 0, -1, -0.69))
test <- glht(m, linfct=contr)
summary(test)
```

## Exercises

1. We will examine a dataset that summarizes cars featured in the magazine Motor Trend during the year 1974. In particular we will examine the relationship between the vehicles gas milage (`mpg`) versus the weight (`wt`) and number of cylinders (`cyl`) of the vehicle.
    ```{r, fig.height=4}
    library(ggplot2)
    # treat cylinders as categorical
    data(mtcars)
    mtcars$cyl <- factor(mtcars$cyl)
    ggplot(mtcars, aes(x=wt, col=cyl)) +
      geom_point(aes(y=mpg, shape=cyl), size=4) +
      xlab('Weight (tons)') +
      ylab('Miles per Gallon') +
      labs(color='Cylinders', shape='Cylinders')
    ```
    a. Fit the model that predicts mpg using both weight and number of cylinders and their interaction using the command
        ```{r}
        model <- lm(mpg ~ wt*cyl, data=mtcars)
        ```
    b. Create a vector of fitted values, add it to the data frame mtcars and then create a plot that includes the regression lines.
    c. Denote the coefficients obtained from the summary of your `lm()` call as $\hat{\beta}_{1}$ to $\hat{\beta}_{6}$ (in the order given by R). Write your interaction model out using subscript notation and should be in the form
        $$y_{i}=\begin{cases}
        ??????? & \;\;\textrm{if 4 cylinder}\\
        ??????? & \;\;\textrm{if 6 cylinder}\\
        ??????? & \;\;\textrm{if 8 cylinder}
        \end{cases}$$ 
        and give an interpretation for each $\beta_{j}$ value.
        i. $\hat{\beta}_1$ = ???
        ii. $\hat{\beta}_2$ = ???
        iii. $\hat{\beta}_3$ = ???
        iv. $\hat{\beta}_4$ = ???
        v. $\hat{\beta}_5$ = ???
        vi. $\hat{\beta}_6$ = ???
        

    d. What is the estimated mpg of a 6-cylinder vehicle weighing 3.5 tons? Give an associated 95% confidence interval. Calculate this using the `predict()` function. *Warning: When making the new data frame, make sure that R interprets cyl as a factor by either coercing it to a factor after creation or inputing the cylinder as a string (i.e. as “6” instead of 6).*
    e. Answer the previous question but using the `glht()` function.
    f. What is the estimated mpg (and 95% confidence interval) of an 8-cylinder vehicle weighing 3.5 tons? Compute this using the `glht()` function.
    g. What is the estimated difference (and 95% confidence interval) in mpg between the 8 and 6 cylinder vehicles that weigh 3.5 tons? Is there a statistically significant difference in mpg at 3.5 tons?
    h. Are the slopes of the 8-cylinder and 6-cylinder vehicles statistically different?