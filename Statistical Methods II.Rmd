--- 
title: "Statistical Methods II"
author: "Derek L. Sonderegger"
date: "`r Sys.Date()`"
site: bookdown::bookdown_site
output: bookdown::gitbook
documentclass: book
bibliography: [book.bib, packages.bib]
biblio-style: apalike
link-citations: yes
github-repo: dereksonderegger/STA_571_Book
description: "The second semester of an Intro Stats course designed for graduate students in Biology, Forestry, Ecology, etc."
---

# Matrix Theory

Almost all of the calculations done in classical statistics require
formulas with large number of subscripts and many different sums.
In this chapter we will develop the mathematical machinery to write
these formulas in a simple compact formula using *matrices*.

## Types of Matrices
We will first introduce the idea behind a matrix and give several
special types of matrices that we will encounter.

### Scalars

To begin, we first define a *scalar*. A scalar is just a single
number, either real or complex. The key is that a scalar is just a
single number. For example, $6$ is a scalar, as is $-3$. By convention,
variable names for scalars will be lower case and not in bold typeface. 

Examples could be $a=5$, $b=\sqrt{3}$, or $\sigma=2$.


### Vectors

A vector is collection of scalars, arranged as a row or column. Our
convention will be that a vector will be a lower cased letter but
written in a bold type. In other branches of mathematics is common
to put a bar over the variable name to denote that it is a vector,
but in statistics, we have already used a bar to denote a mean. 

Examples of column vectors could be
\begin{eqnarray*}
\boldsymbol{a} & = & \left[\begin{array}{c}
2\\
-3\\
4
\end{array}\right]\;\;\;\;\;\boldsymbol{b}=\left[\begin{array}{c}
2\\
8\\
3\\
4\\
1
\end{array}\right]
\end{eqnarray*}

and examples of row vectors are

\[
\boldsymbol{c}=\left[\begin{array}{cccc}
8 & 10 & 43 & -22\end{array}\right]
\]

\[
\boldsymbol{d}=\left[\begin{array}{ccc}
-1 & 5 & 2\end{array}\right]
\]

To denote a specific entry in the vector, we will use a subscript.
For example, the second element of $\boldsymbol{d}$ is $d_{2}=5$.
Notice, that we do not bold this symbol because the second element
of the vector is the scalar value $5$.


### Matrix

Just as a vector is a collection of scalars, a matrix can be viewed
as a collection of vectors (all of the same length). We will denote
matrices with bold capitalized letters. In general, I try to use letters
at the end of the alphabet for matrices. Likewise, I try to use symmetric
letters to denote symmetric matrices.

For example, the following is a matrix with two rows and three columns
\[
\boldsymbol{W}=\left[\begin{array}{ccc}
1 & 2 & 3\\
4 & 5 & 6
\end{array}\right]
\]
and there is no requirement that the number of rows be equal, less
than, or greater than the number of columns. In denoting the size
of the matrix, we first refer to the number of rows and then the number
of columns. Thus $\boldsymbol{W}$ is a $2\times3$ matrix and it
sometimes is helpful to remind ourselves of this by writing $\boldsymbol{W}_{2\times3}$.

To pick out a particular element of a matrix, I will again use a subscripting
notation, always with the row number first and then column. Notice
the notational shift to lowercase, non-bold font.
\[
w_{1,2}=2\;\;\;\;\;\;\textrm{and }\;\;\;\;\;\;\;w_{2,3}=6
\]


There are times I will wish to refer to a particular row or column
of a matrix and we will use the following notation
\[
\boldsymbol{w}_{1,\cdot}=\left[\begin{array}{ccc}
1 & 2 & 3\end{array}\right]
\]
is the first row of the matrix $\boldsymbol{W}$. The second column
of matrix $\boldsymbol{W}$ is 
\[
\boldsymbol{w}_{\cdot,2}=\left[\begin{array}{c}
2\\
5
\end{array}\right]
\]

### Square Matrices

A square matrix is a matrix with the same number of rows as columns.
The following are square
\[
\boldsymbol{Z}=\left[\begin{array}{cc}
3 & 6\\
8 & 10
\end{array}\right]\;\;\;\;\;\;\boldsymbol{X}=\left[\begin{array}{ccc}
1 & 2 & 3\\
2 & 1 & 2\\
3 & 2 & 1
\end{array}\right]
\]



### Symmetric Matrices

In statistics we are often interested in square matrices where the
$i,j$ element is the same as the $j,i$ element. For example, $x_{1,2}=x_{2,1}$
in the above matrix $\boldsymbol{X}.$ 

Consider a matrix $\boldsymbol{D}$ that contains the distance
from four towns to each of the other four towns. Let $d_{i,j}$ be
the distance from town $i$ to town $j$. It only makes sense that
the distance doesn't matter which direction you are traveling, and
we should therefore require that $d_{i,j}=d_{j,i}$. 

In this example, it is the values $d_{i,i}$ represent the distance
from a town to itself, which should be zero. It turns out that we
are often interested in the terms $d_{i,i}$ and I will refer to those
terms as the *main diagonal* of matrix $\boldsymbol{D}$. 

Symmetric matrices play a large role in statistics because matrices
that represent the covariances between random variables must be symmetric
because $Cov\left(Y,Z\right)=Cov\left(Z,Y\right)$. 


### Diagonal Matrices

A square matrix that has zero entries in every location except the
main diagonal is called a diagonal matrix. Here are two examples:
\[
\boldsymbol{Q}=\left[\begin{array}{ccc}
4 & 0 & 0\\
0 & 5 & 0\\
0 & 0 & 6
\end{array}\right]\;\;\;\;\;\;\;\;\boldsymbol{R}=\left[\begin{array}{cccc}
1 & 0 & 0 & 0\\
0 & 2 & 0 & 0\\
0 & 0 & 2 & 0\\
0 & 0 & 0 & 3
\end{array}\right]
\]
Sometimes to make matrix more clear, I will replace the $0$ with
a dot to emphasize the non-zero components.
\[
\boldsymbol{R}=\left[\begin{array}{cccc}
1 & \cdot & \cdot & \cdot\\
\cdot & 2 & \cdot & \cdot\\
\cdot & \cdot & 2 & \cdot\\
\cdot & \cdot & \cdot & 3
\end{array}\right]
\]



### Identity Matrices

A diagonal matrix with main diagonal values exactly $1$ is called
the identity matrix. The $3\times3$ identity matrix is denoted $I_{3}$.
\[
\boldsymbol{I}_{3}=\left[\begin{array}{ccc}
1 & \cdot & \cdot\\
\cdot & 1 & \cdot\\
\cdot & \cdot & 1
\end{array}\right]
\]



## Operations on Matrices

### Transpose

The simplest operation on a square matrix matrix is called *transpose*.
It is defined as $\boldsymbol{M}=\boldsymbol{W}^{T}$ if and only
if $m_{i,j}=w_{j,i}.$

\[
\boldsymbol{Z}=\left[\begin{array}{cc}
1 & 6\\
8 & 3
\end{array}\right]\;\;\;\;\;\;\boldsymbol{Z}^{T}=\left[\begin{array}{cc}
1 & 8\\
6 & 3
\end{array}\right]
\]
\[
\boldsymbol{M}=\left[\begin{array}{ccc}
3 & 1 & 2\\
9 & 4 & 5\\
8 & 7 & 6
\end{array}\right]\;\;\;\;\;\boldsymbol{M}^{T}=\left[\begin{array}{ccc}
3 & 9 & 8\\
1 & 4 & 7\\
2 & 5 & 6
\end{array}\right]
\]
We can think of this as swapping all elements about the main diagonal. Alternatively we could think about the transpose as making the first row become the first column, the second row become the second column, etc. In this fashion we could define the transpose of a non-square matrix.

\[
\boldsymbol{W}=\left[\begin{array}{ccc}
1 & 2 & 3\\
4 & 5 & 6
\end{array}\right]
\]

\[
\boldsymbol{W}^T=\left[\begin{array}{cc}
1 & 4 \\
2 & 5 \\
3 & 6
\end{array}\right]
\]


### Addition and Subtraction

Addition and subtraction are performed *element-wise*. This means
that two matrices or vectors can only be added or subtracted if their
dimensions match.
\[
\left[\begin{array}{c}
1\\
2\\
3\\
4
\end{array}\right]+\left[\begin{array}{c}
5\\
6\\
7\\
8
\end{array}\right]=\left[\begin{array}{c}
6\\
8\\
10\\
12
\end{array}\right]
\]
\[
\left[\begin{array}{cc}
5 & 8\\
2 & 4\\
11 & 15
\end{array}\right]-\left[\begin{array}{cc}
1 & 2\\
3 & 4\\
5 & -6
\end{array}\right]=\left[\begin{array}{cc}
4 & 6\\
-1 & 0\\
6 & 21
\end{array}\right]
\]



### Multiplication

Multiplication is the operation that is vastly different for matrices
and vectors than it is for scalars. There is a great deal of mathematical
theory that suggests a useful way to define multiplication. What is
presented below is referred to as the *dot-product* of vectors
in calculus, and is referred to as the standard *inner-product*
in linear algebra. 


### Vector Multiplication

We first define multiplication for a row and column vector. For this
multiplication to be defined, both vectors must be the same length.
The product is the sum of the element-wise multiplications.
\[
\left[\begin{array}{cccc}
1 & 2 & 3 & 4\end{array}\right]\left[\begin{array}{c}
5\\
6\\
7\\
8
\end{array}\right]=\left(1\cdot5\right)+\left(2\cdot6\right)+\left(3\cdot7\right)+\left(4\cdot8\right)=5+12+21+32=70
\]



### Matrix Multiplication

Matrix multiplication is just a sequence of vector multiplications.
If $\boldsymbol{X}$ is a $m\times n$ matrix and $\boldsymbol{W}$
is $n\times p$ matrix then $\boldsymbol{Z}=\boldsymbol{XW}$ is a
$m\times p$ matrix where $z_{i,j}=\boldsymbol{x}_{i,\cdot}\boldsymbol{w}_{\cdot, j}$
where $\boldsymbol{x}_{i,\cdot}$ is the $i$th column of
$\boldsymbol{X}$ and $\boldsymbol{w}_{\cdot, j}$ is the $j$th column
of $\boldsymbol{W}$. For example, let
\[
\boldsymbol{X}=\left[\begin{array}{cccc}
1 & 2 & 3 & 4\\
5 & 6 & 7 & 8\\
9 & 10 & 11 & 12
\end{array}\right]\;\;\;\;\;\;\;\;\;\boldsymbol{W}=\left[\begin{array}{cc}
13 & 14\\
15 & 16\\
17 & 18\\
19 & 20
\end{array}\right]
\]
so $\boldsymbol{X}$ is $3\times4$ (which we remind ourselves by
adding a $3\times4$ subscript to $\boldsymbol{X}$ as $\boldsymbol{X}_{3\times4}$)
and $\boldsymbol{W}$ is $\boldsymbol{W}{}_{4\times2}$. Because the
*inner* dimensions match for this multiplication, then $\boldsymbol{Z}_{3\times2}=\boldsymbol{X}_{3\times4}\boldsymbol{W}_{4\times2}$
is defined where\textbf{ 
\begin{eqnarray*}
z_{1,1} & = & \boldsymbol{x}_{1,\cdot}\boldsymbol{w}_{\cdot,1}\\
 & = & \left(1\cdot13\right)+\left(2\cdot15\right)+\left(3\cdot17\right)+\left(4\cdot19\right)=170
\end{eqnarray*}
}and similarly
\begin{eqnarray*}
z_{2,1} & = & \boldsymbol{x}_{2,\cdot}\boldsymbol{w}_{\cdot,1}\\
 & = & \left(5\cdot13\right)+\left(6\cdot15\right)+\left(7\cdot17\right)+\left(8\cdot19\right)=426
\end{eqnarray*}
so that 
\[
\boldsymbol{Z}=\left[\begin{array}{cc}
170 & 180\\
426 & 452\\
682 & 724
\end{array}\right]
\]


For another example, we note that
\begin{eqnarray*}
\left[\begin{array}{ccc}
1 & 2 & 3\\
2 & 3 & 4
\end{array}\right]\left[\begin{array}{cc}
1 & 2\\
2 & 2\\
1 & 2
\end{array}\right] & = & \left[\begin{array}{cc}
1+4+3\;\;\; & 2+4+6\\
2+6+4\;\;\; & 4+6+8
\end{array}\right]\\
 & = & \left[\begin{array}{cc}
8 & 12\\
12 & 18
\end{array}\right]
\end{eqnarray*}


Notice that this definition of multiplication means that the order
matters. Above, we calculated $\boldsymbol{X}_{3\times4}\boldsymbol{W}_{4\times2}$
but we cannot reverse the order because the inner dimensions do not
match up.


### Scalar times a Matrix

Strictly speaking, we are not allowed to multiply a matrix by a scalar
because the dimensions do not match. However, it is often notationally
convenient. So we define $a\boldsymbol{X}$ to be the *element-wise*
multiplication of each element of $\boldsymbol{X}$ by the scalar
$a$. Because this is just a notational convenience, the mathematical
theory about inner-products does not apply to this operation.
\[
5\left[\begin{array}{cc}
4 & 5\\
7 & 6\\
9 & 10
\end{array}\right]=\left[\begin{array}{cc}
20 & 25\\
35 & 30\\
45 & 50
\end{array}\right]
\]


Because of this definition, it is clear that $a\boldsymbol{X}=\boldsymbol{X}a$
and the order does not matter. Thus when mixing scalar multiplication
with matrices, it is acceptable to reorder scalars, but not matrices.


### Determinant

The determinant is defined only for square matrices and can be thought
of as the matrix equivalent of the absolute value or magnitude (i.e.
$|-6|=6$). The determinant gives a measure of the multi-dimensional
size of a matrix (say the matrix $\boldsymbol{A}$) and as such is
denoted $\det\left(\boldsymbol{A}\right)$ or $\left|\boldsymbol{A}\right|$.
Generally this is a very tedious thing to calculate by hand and for
completeness sake, we will give a definition and small examples.

For a $2\times2$ matrix
\[
\left|\begin{array}{cc}
a & c\\
b & d
\end{array}\right|=ad-cb
\]
So a simple example of a determinant is

\[
\left|\begin{array}{cc}
5 & 2\\
3 & 10
\end{array}\right|=50-6=44
\]


The determinant can be thought of as the area of the parallelogram
created by the row or column vectors of the matrix.

```{r, fig.height=2.5, echo=FALSE, fig.align='center'}
knitr::include_graphics("External_Images/determinant.png")
```



### Inverse

In regular algebra, we are often interested in solving equations such
as 
$$ 5x=15 $$
for $x$. To do so, we multiply each side of the equation by the inverse
of 5, which is $1/5$.

\begin{eqnarray*}
5x & = & 15\\
\frac{1}{5}\cdot5\cdot x & = & \frac{1}{5}\cdot15\\
1\cdot x & = & 3\\
x & = & 3
\end{eqnarray*}

For scalars, we know that the inverse of scalar $a$ is the value
that when multiplied by $a$ is 1. That is we see to find $a^{-1}$
such that $aa^{-1}=1$. 

In the matrix case, I am interested in finding $\boldsymbol{A}^{-1}$
such that $\boldsymbol{A}^{-1}\boldsymbol{A}=\boldsymbol{I}$ and
$\boldsymbol{A}\boldsymbol{A}^{-1}=\boldsymbol{I}$. For both of these
multiplications to be defined, $\boldsymbol{A}$ must be a square
matrix and so the inverse is only defined for square matrices.

For a $2\times2$ matrix 
\[
\boldsymbol{W}=\left[\begin{array}{cc}
a & b\\
c & d
\end{array}\right]
\]
the inverse is given by: 
\[
\boldsymbol{W}^{-1}=\frac{1}{\det\boldsymbol{W}}\;\left[\begin{array}{cc}
d & -b\\
-c & a
\end{array}\right]
\]


For example, suppose 
\[
\boldsymbol{W}=\left[\begin{array}{cc}
1 & 2\\
5 & 3
\end{array}\right]
\]
 then $\det W=3-10=-7$ and 
\begin{eqnarray*}
\boldsymbol{W}^{-1} & = & \frac{1}{-7}\;\left[\begin{array}{cc}
3 & -2\\
-5 & 1
\end{array}\right]\\
 & = & \left[\begin{array}{cc}
-\frac{3}{7} & \frac{2}{7}\\
\frac{5}{7} & -\frac{1}{7}
\end{array}\right]
\end{eqnarray*}
and thus
\begin{eqnarray*}
\boldsymbol{W}\boldsymbol{W}^{-1} & = & \left[\begin{array}{cc}
1 & 2\\
5 & 3
\end{array}\right]\left[\begin{array}{cc}
-\frac{3}{7} & \frac{2}{7}\\
\frac{5}{7} & -\frac{1}{7}
\end{array}\right]\\
\\
 & = & \left[\begin{array}{cc}
-\frac{3}{7}+\frac{10}{7}\;\;\; & \frac{2}{7}-\frac{2}{7}\\
\\
-\frac{15}{7}+\frac{15}{7}\;\;\; & \frac{10}{7}-\frac{3}{7}
\end{array}\right]\\
\\
 & = & \left[\begin{array}{cc}
1 & 0\\
0 & 1
\end{array}\right]=\boldsymbol{I}_{2}
\end{eqnarray*}


Not every square matrix has an inverse. If the determinant of the
matrix (which we think of as some measure of the magnitude or *size*
of the matrix) is zero, then the formula would require us to divide
by zero. Just as we cannot find the inverse of zero (i.e. solve $0x=1$
for $x$), a matrix with zero determinate is said to have no inverse.


## Exercises
Consider the following matrices: 
\[
\mathbf{A}=\left[\begin{array}{ccc}
1 & 2 & 3\\
6 & 5 & 4
\end{array}\right]\;\;\;\;\;\;\;\mathbf{B}=\left[\begin{array}{ccc}
6 & 4 & 3\\
8 & 7 & 6
\end{array}\right]\;\;\;\;\;\;\;\mathbf{c}=\left[\begin{array}{c}
1\\
2\\
3
\end{array}\right]\;\;\;\;\;\;\;\mathbf{d}=\left[\begin{array}{c}
4\\
5\\
6
\end{array}\right]\;\;\;\;\;\;\;\mathbf{E}=\left[\begin{array}{cc}
1 & 2\\
2 & 6
\end{array}\right]
\]

1. Find $\mathbf{Bc}$
2. Find $\mathbf{AB}^{T}$
3. Find $\mathbf{c}^{T}\mathbf{d}$
4. Find $\mathbf{cd}^{T}$
5. Confirm that $\mathbf{E}^{-1}=\left[\begin{array}{cc}
      3 & -1\\
      -1 & 1/2 \end{array}\right]$ 
      is the inverse of $\mathbf{E}$ by calculating $\mathbf{E}\mathbf{E}^{-1}=\mathbf{I}$.



<!--chapter:end:index.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Parameter Estimation
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

We have previously looked at ANOVA and regression models and, in many
ways, they felt very similar. In this chapter we will introduce the
theory that allows us to understand both models as a particular flavor
of a larger class of models known as _linear models_.

First we clarify what a linear model is. A linear model is a model
where the data (which we will denote using roman letters as $\boldsymbol{x}$
and $\boldsymbol{y}$) and parameters of interest (which we denote
using Greek letters such as $\boldsymbol{\alpha}$ and $\boldsymbol{\beta}$)
interact only via addition and multiplication. The following are linear
models:

Model              |  Formula
------------------ | ---------------------------------------------
ANOVA              | $y_{ij}=\mu+\tau_{i}+\epsilon_{ij}$ 
Simple Regression  | $y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}$ 
Quadratic Term     | $y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\epsilon_{i}$ 
General Regression | $y_{i}=\beta_{0}+\beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\dots+\beta_{p}x_{i,p}+\epsilon_{i}$ 


Notice in the Quadratic model, the square is
not a parameter and we can consider $x_{i}^{2}$ as just another column
of data. This leads to the second example of multiple regression where
we just add more slopes for other covariates where the $p$th
covariate is denoted $\boldsymbol{x}_{\cdot,p}$ and might be some
transformation (such as $x^{2}$ or $\log x$) of another column of
data. The critical point is that the transformation to the data  $\boldsymbol{x}$ does not depend
on a parameter. Thus the following is _not_ a linear model
\[
y_{i}=\beta_{0}+\beta_{1}x_{i}^{\alpha}+\epsilon_{i}
\]



## Simple Regression

We would like to represent all linear models in a similar compact
matrix representation. This will allow us to make the transition between
simple and multiple regression (and ANCOVA) painlessly.

To begin, we think about how to write the simple regression model
using matrices and vectors that correspond the the data and the parameters.
Notice we have


$$\begin{aligned}
y_{1} & =  \beta_{0}+\beta_{1}x_{1}+\epsilon_{1}\\
y_{2} & =  \beta_{0}+\beta_{1}x_{2}+\epsilon_{2}\\
y_{3} & =  \beta_{0}+\beta_{1}x_{3}+\epsilon_{3}\\
 & \vdots\\
y_{n-1} & =  \beta_{0}+\beta_{1}x_{n-1}+\epsilon_{n-1}\\
y_{n} & =  \beta_{0}+\beta_{1}x_{n}+\epsilon_{n}
\end{aligned}$$

where, as usual, $\epsilon_{i}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$.
These equations can be written using matrices as
RR
$$
\underset{\boldsymbol{y}}{\underbrace{\left[\begin{array}{c}
y_{1}\\
y_{2}\\
y_{3}\\
\vdots\\
y_{n-1}\\
y_{n}
\end{array}\right]}}=\underset{\boldsymbol{X}}{\underbrace{\left[\begin{array}{cc}
1 & x_{1}\\
1 & x_{2}\\
1 & x_{3}\\
\vdots & \vdots\\
1 & x_{n-1}\\
1 & x_{n}
\end{array}\right]}}\underset{\boldsymbol{\beta}}{\underbrace{\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}
\end{array}\right]}}+\underset{\boldsymbol{\epsilon}}{\underbrace{\left[\begin{array}{c}
\epsilon_{1}\\
\epsilon_{2}\\
\epsilon_{3}\\
\vdots\\
\epsilon_{n-1}\\
\epsilon_{n}
\end{array}\right]}}
$$

and we compactly write the model as 

$$
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}
$$
where $\boldsymbol{X}$ is referred to as the *design matrix*
and $\boldsymbol{\beta}$ is the vector of *location parameters* we are interested
in estimating. 


### Estimation of Location Paramters 

Our next goal is to find the best estimate of $\boldsymbol{\beta}$
given the data. To justify the formula, consider the case where there
is no error terms (i.e. $\epsilon_{i}=0$ for all $i$). Thus we have
$$
\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}
$$
and our goal is to solve for $\boldsymbol{\beta}$. To do this, we
must use a matrix inverse, but since inverses only exist for square
matrices, we pre-multiple by $\boldsymbol{X}^{T}$ (notice that $\boldsymbol{X}^{T}\boldsymbol{X}$
is a symmetric $2\times2$ matrix).
$$
\boldsymbol{X}^{T}\boldsymbol{y}=\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{\beta}
$$
 and then pre-multiply by $\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}$.

$$\begin{eqnarray*}
\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y} & = & \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\boldsymbol{\beta}\\
\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y} & = & \boldsymbol{\beta}
\end{eqnarray*}$$

This exercise suggests that $\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}$
is a good place to start when looking for the maximum-likelihood estimator
for $\boldsymbol{\beta}$. It turns out that this quantity is in fact
the maximum-likelihood estimator (and equivalently minimizes the sum-of-squared
error). Therefore we will use it as our estimate of $\boldsymbol{\beta}$.
$$
\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}
$$



### Estimation of Variance Parameter

Recall our model is 
$$
y_{i}=\beta_{0}+\beta_{1}x_{i}+\epsilon_{i}
$$
where $\epsilon_{i}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$.

Using our estimates $\hat{\boldsymbol{\beta}}$ we can obtain predicted values for the regression line at any x-value. In particular we can find the predicted value for each $x_i$ value in our dataset.
$$
\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1 x_i
$$
Using matrix notation, I would write $\hat{\boldsymbol{y}}=\boldsymbol{X}\hat{\boldsymbol{\beta}}$.

As usual we will find estimates of the noise terms (which we will
call residuals or errors) via 
$$\begin{eqnarray*}
\hat{\epsilon}_{i} & = & y_{i}-\hat{y}_{i}\\
 & = & y_{i}-\left(\hat{\beta}_{0}+\hat{\beta}_{1}x_{i}\right)
\end{eqnarray*}$$


Writing $\hat{\boldsymbol{y}}$ in matrix terms we have
$$\begin{eqnarray*}
\hat{\boldsymbol{y}} & = & \boldsymbol{X}\hat{\boldsymbol{\beta}}\\
 & = & \boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\\
 & = & \boldsymbol{H}\boldsymbol{y}
\end{eqnarray*}$$
where $\boldsymbol{H}=\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}$
is often called the *hat-matrix* because it takes $y$ to $\hat{y}$
and has many interesting theoretical properties.\footnote{Mathematically, $\boldsymbol{H}$ is the projection matrix that takes
a vector in $n$-dimensional space and projects it onto a $p$-dimension
subspace spanned by the vectors in $\boldsymbol{X}$. Projection matrices
have many useful properties and much of the theory of linear models
utilizes $\boldsymbol{H}$. }

We can now estimate the error terms via

$$\begin{eqnarray*}
\hat{\boldsymbol{\epsilon}} & = & \boldsymbol{y}-\hat{\boldsymbol{y}}\\
 & = & \boldsymbol{y}-\boldsymbol{H}\boldsymbol{y}\\
 & = & \left(\boldsymbol{I}_{n}-\boldsymbol{H}\right)\boldsymbol{y}
\end{eqnarray*}$$

As usual we estimate $\sigma^{2}$ using the mean-squared error 
$$\begin{eqnarray*}
\hat{\sigma}^{2} & = & \frac{1}{n-2}\;\sum_{i=1}^{n}\hat{\epsilon}_{i}^{2}\\
\\
 & = & \frac{1}{n-2}\;\hat{\boldsymbol{\epsilon}}^{T}\hat{\boldsymbol{\epsilon}}
\end{eqnarray*}$$

In the general linear model case where $\boldsymbol{\beta}$ has $p$
elements (and thus we have $n-p$ degrees of freedom), the formula
is
$$\begin{eqnarray*}
\hat{\sigma}^{2} & = & \frac{1}{n-p}\;\hat{\boldsymbol{\epsilon}}^{T}\hat{\boldsymbol{\epsilon}}
\end{eqnarray*}$$
 

### Expectation and variance of a random vector
Just as we needed to derive the expected value and variance of $\bar{x}$
in the previous semester, we must now do the same for $\hat{\boldsymbol{\beta}}$.
But to do this, we need some properties of expectations and variances.

In the following, let $\boldsymbol{A}_{n\times p}$ and $\boldsymbol{b}_{n\times1}$
be constants and $\boldsymbol{\epsilon}_{n\times1}$ be a random vector.

Expectations are very similar to the scalar case where
\[
E\left[\boldsymbol{\epsilon}\right]=\left[\begin{array}{c}
E\left[\epsilon_{1}\right]\\
E\left[\epsilon_{2}\right]\\
\vdots\\
E\left[\epsilon_{n}\right]
\end{array}\right]
\]
and any constants are pulled through the expectation
\[
E\left[\boldsymbol{A}^{T}\boldsymbol{\epsilon}+\boldsymbol{b}\right]=\boldsymbol{A}^{T}\,E\left[\boldsymbol{\epsilon}\right]+\boldsymbol{b}
\]


Variances are a little different. The variance of the vector $\boldsymbol{\epsilon}$
is
\[
Var\left(\boldsymbol{\epsilon}\right)=\left[\begin{array}{cccc}
Var\left(\epsilon_{1}\right) & Cov\left(\epsilon_{1},\epsilon_{2}\right) & \dots & Cov\left(\epsilon_{1},\epsilon_{n}\right)\\
Cov\left(\epsilon_{2},\epsilon_{1}\right) & Var\left(\epsilon_{2}\right) & \dots & Cov\left(\epsilon_{2},\epsilon_{n}\right)\\
\vdots & \vdots & \ddots & \vdots\\
Cov\left(\epsilon_{n},\epsilon_{1}\right) & Cov\left(\epsilon_{n},\epsilon_{2}\right) & \dots & Var\left(\epsilon_{1}\right)
\end{array}\right]
\]
 and additive constants are ignored, but multiplicative constants
are pulled out as follows:
\[
Var\left(\boldsymbol{A}^{T}\boldsymbol{\epsilon}+\boldsymbol{b}\right)=Var\left(\boldsymbol{A}^{T}\boldsymbol{\epsilon}\right)=\boldsymbol{A}^{T}\,Var\left(\boldsymbol{\epsilon}\right)\,\boldsymbol{A}
\]



### Variance of Location Parameters

We next derive the sampling variance of our estimator $\hat{\boldsymbol{\beta}}$
by first noting that $\boldsymbol{X}$ and $\boldsymbol{\beta}$ are
constants and therefore 
\begin{eqnarray*}
Var\left(\boldsymbol{y}\right) & = & Var\left(\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}\right)\\
 & = & Var\left(\boldsymbol{\epsilon}\right)\\
 & = & \sigma^{2}\boldsymbol{I}_{n}
\end{eqnarray*}
because the error terms are independent and therefore $Cov\left(\epsilon_{i},\epsilon_{j}\right)=0$
when $i\ne j$ and $Var\left(\epsilon_{i}\right)=\sigma^{2}$. Recalling
that constants come out of the variance operator as the constant \emph{squared,
}
\begin{eqnarray*}
Var\left(\hat{\boldsymbol{\beta}}\right) & = & Var\left(\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\right)\\
 & = & \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\,Var\left(\boldsymbol{y}\right)\,\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\\
 & = & \left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\,\sigma^{2}\boldsymbol{I}_{n}\,\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\\
 & = & \sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\\
 & = & \sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}
\end{eqnarray*}


Using this, the standard error (i.e. the estimated standard deviation)
of $\hat{\beta}_{j}$ (for any $j$ in $1,\dots,p$) is 
\[
StdErr\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}
\]



### Confidence intervals and hypothesis tests

We can now state the general method of creating confidence intervals
and perform hypothesis tests for any element of $\boldsymbol{\beta}$. 

The confidence interval formula is (as usual)
\[
\hat{\beta}_{j}\pm t_{n-p}^{1-\alpha/2}\,StdErr\left(\hat{\beta}_{j}\right)
\]
 and a test statistic for testing $H_{0}:\,\beta_{j}=0$ versus $H_{a}:\,\beta_{j}\ne0$
is 
\[
t_{n-p}=\frac{\hat{\beta}_{j}-0}{StdErr\left(\hat{\beta}_{j}\right)}
\]
 


### Summary of pertinent results
* $\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}$ is the unbiased maximum-likelihood estimator of $\boldsymbol{\beta}$.
* The Central Limit Theorem applies to each element of $\boldsymbol{\beta}$. That is, as $n\to\infty$, the distribution of $\hat{\beta}_{j}\to N\left(\beta_{j},\left[\sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}\right)$.
* The error terms can be calculated via
\begin{eqnarray*}
\hat{\boldsymbol{y}} & = & \boldsymbol{X}\hat{\boldsymbol{\beta}}\\
\hat{\boldsymbol{\epsilon}} & = & \boldsymbol{y}-\hat{\boldsymbol{y}}
\end{eqnarray*}

* The estimate of $\sigma^{2}$ is 
\[
\hat{\sigma}^{2}=\frac{1}{n-p}\;\hat{\boldsymbol{\epsilon}}^{T}\hat{\boldsymbol{\epsilon}}
\]

* The standard error (i.e. the estimated standard deviation) of $\hat{\beta}_{j}$
(for any $j$ in $1,\dots,p$) is 
\[
StdErr\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}
\]

### An example in R

Here we will work an example in R and see the calculations. Consider
the following data:
```{r, fig.height=3, warning=FALSE}
library(ggplot2)
n <- 20
x <- seq(0,10, length=n)
y <- -3 + 2*x + rnorm(n, sd=2)
my.data <- data.frame(x=x, y=y)
ggplot(my.data) + geom_point(aes(x=x,y=y))
```


First we must create the design matrix $\boldsymbol{X}$. Recall 
\[
\boldsymbol{X}=\left[\begin{array}{cc}
1 & x_{1}\\
1 & x_{2}\\
1 & x_{3}\\
\vdots & \vdots\\
1 & x_{n-1}\\
1 & x_{n}
\end{array}\right]
\]
 and can be created in R via the following:

```{r}
X <- cbind( rep(1,n), x)
```


Given $\boldsymbol{X}$ and $\boldsymbol{y}$ we can calculate 
\[
\hat{\boldsymbol{\beta}}=\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}
\]
in R using the following code:
```{r}
XtXinv <- solve( t(X) %*% X )
beta.hat <- XtXinv %*% t(X) %*% y
beta.hat
```


Our next step is to calculate the predicted values $\hat{\boldsymbol{y}}$
and the residuals $\hat{\boldsymbol{\epsilon}}$ 
\begin{eqnarray*}
\hat{\boldsymbol{y}} & = & \boldsymbol{X}\hat{\boldsymbol{\beta}}\\
\hat{\boldsymbol{\epsilon}} & = & \boldsymbol{y}-\hat{\boldsymbol{y}}
\end{eqnarray*}

```{r}
y.hat <- X %*% beta.hat
residuals <- y - y.hat
```

Now that we have the residuals, we can calculate $\hat{\sigma}^{2}$
and the standard errors of $\hat{\beta}_{j}$
\[
\hat{\sigma}^{2}=\frac{1}{n-p}\,\hat{\boldsymbol{\epsilon}}^{T}\hat{\boldsymbol{\epsilon}}
\]
\[
StdErr\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left[\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\right]_{jj}}
\]

```{r}
sigma2.hat <- ( t(residuals) %*% residuals) / (n-2)
sigma.hat <- sqrt( sigma2.hat )
std.errs <- sqrt( sigma2.hat * diag(XtXinv) )
```


We now print out the important values and compare them to the summary
output given by the `lm()` function in R.

```{r}
beta.hat
sigma.hat
std.errs
```
```{r}
model <- lm(y~x)
summary(model)
```


We calculate $95\%$ confidence intervals via:
```{r}
lwr <- beta.hat - qt(.975, n-2) * std.errs
upr <- beta.hat + qt(.975, n-2) * std.errs
CI <- cbind(lwr,upr)
colnames(CI) <- c('lower','upper')
rownames(CI) <- c('Intercept', 'x')
CI
```


These intervals are the same as what we get when we use the `confint()`
function.

```{r}
confint(model)
```


## ANOVA model

The anova model is also a linear model and all we must do is create
a appropriate design matrix. Given the design matrix $\boldsymbol{X}$,
all the calculations are identical as in the simple regression case.


### Cell means representation

Recall the cell means representation is
\[
y_{i,j}=\mu_{i}+\epsilon_{i,j}
\]
where $y_{i,j}$ is the $j$th observation within the $i$th group.
To clearly show the creation of the $\boldsymbol{X}$ matrix, let
the number of groups be $p=3$ and the number of observations per
group be $n_{i}=4$. We now expand the formula to show all the data.
\begin{eqnarray*}
y_{1,1} & = & \mu_{1}+\epsilon_{1,1}\\
y_{1,2} & = & \mu_{1}+\epsilon_{1,2}\\
y_{1,3} & = & \mu_{1}+\epsilon_{1,3}\\
y_{1,4} & = & \mu_{1}+\epsilon_{1,4}\\
y_{2,1} & = & \mu_{2}+\epsilon_{2,1}\\
y_{2,2} & = & \mu_{2}+\epsilon_{2,2}\\
y_{2,3} & = & \mu_{2}+\epsilon_{2,3}\\
y_{2,4} & = & \mu_{2}+\epsilon_{2,4}\\
y_{3,1} & = & \mu_{3}+\epsilon_{3,1}\\
y_{3,2} & = & \mu_{3}+\epsilon_{3,2}\\
y_{3,3} & = & \mu_{3}+\epsilon_{3,3}\\
y_{3,4} & = & \mu_{3}+\epsilon_{3,4}
\end{eqnarray*}
In an effort to write the model as $\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon}$
we will write the above as
\begin{eqnarray*}
y_{1,1} & = & 1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,1}\\
y_{1,2} & = & 1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,2}\\
y_{1,3} & = & 1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,3}\\
y_{1,4} & = & 1\mu_{1}+0\mu_{2}+0\mu_{3}+\epsilon_{1,4}\\
y_{2,1} & = & 0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,1}\\
y_{2,2} & = & 0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,2}\\
y_{2,3} & = & 0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,3}\\
y_{2,4} & = & 0\mu+1\mu_{2}+0\mu_{3}+\epsilon_{2,4}\\
y_{3,1} & = & 0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,1}\\
y_{3,2} & = & 0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,2}\\
y_{3,3} & = & 0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,3}\\
y_{3,4} & = & 0\mu+0\mu_{2}+1\mu_{3}+\epsilon_{3,4}
\end{eqnarray*}
and we will finally be able to write the matrix version
\[
\underset{\boldsymbol{y}}{\underbrace{\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{1,4}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{2,4}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}\\
y_{3,4}
\end{array}\right]}}=\underset{\mathbf{X}}{\underbrace{\left[\begin{array}{ccc}
1 & 0 & 0\\
1 & 0 & 0\\
1 & 0 & 0\\
1 & 0 & 0\\
0 & 1 & 0\\
0 & 1 & 0\\
0 & 1 & 0\\
0 & 1 & 0\\
0 & 0 & 1\\
0 & 0 & 1\\
0 & 0 & 1\\
0 & 0 & 1
\end{array}\right]}}\underset{\boldsymbol{\beta}}{\underbrace{\left[\begin{array}{c}
\mu_{1}\\
\mu_{2}\\
\mu_{3}
\end{array}\right]}}+\underset{\boldsymbol{\epsilon}}{\underbrace{\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{1,4}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{2,4}\\
\epsilon_{3,1}\\
\epsilon_{3,2}\\
\epsilon_{3,3}\\
\epsilon_{3,4}
\end{array}\right]}}
\]
\[
\]
Notice that each column of the $\boldsymbol{X}$ matrix is acting
as an indicator if the observation is an element of the appropriate
group. As such, these are often called *indicator variables*.
Another term for these, which I find less helpful, is *dummy variables*.


### Offset from reference group

In this model representation of ANOVA, we have an overall mean and
then offsets from the control group (which will be group one). The
model is thus
\[
y_{i,j}=\mu+\tau_{i}+\epsilon_{i,j}
\]
where $\tau_{1}=0$. We can write this in matrix form as
\[
\underset{\boldsymbol{y}}{\underbrace{\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{1,4}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{2,4}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}\\
y_{3,4}
\end{array}\right]}}=\underset{\mathbf{X}}{\underbrace{\left[\begin{array}{ccc}
1 & 0 & 0\\
1 & 0 & 0\\
1 & 0 & 0\\
1 & 0 & 0\\
1 & 1 & 0\\
1 & 1 & 0\\
1 & 1 & 0\\
1 & 1 & 0\\
1 & 0 & 1\\
1 & 0 & 1\\
1 & 0 & 1\\
1 & 0 & 1
\end{array}\right]}}\underset{\boldsymbol{\beta}}{\underbrace{\left[\begin{array}{c}
\mu\\
\tau_{2}\\
\tau_{3}
\end{array}\right]}}+\underset{\boldsymbol{\epsilon}}{\underbrace{\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{1,4}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{2,4}\\
\epsilon_{3,1}\\
\epsilon_{3,2}\\
\epsilon_{3,3}\\
\epsilon_{3,4}
\end{array}\right]}}
\]

## Exercises

1. We will do a simple ANOVA analysis on example 8.2 from Ott \& Longnecker using the matrix representation of the model. A clinical psychologist wished to compare three methods for reducing hostility levels in university students, and used a certain test (HLT) to measure the degree of hostility. A high score on the test indicated great hostility. The psychologist used 24 students who obtained high and nearly equal scores in the experiment. eight were selected at random from among the 24 problem cases and were treated with method 1. Seven of the remaining 16 students were selected at random and treated with method 2. The remaining nine students were treated with method 3. All treatments were continued for a one-semester period. Each student was given the HLT test at the end of the semester, with the results show in the following table. (This analysis was done in section 8.3 of my STA 570 notes)

    Method   | Values
    -------- | ------------
    1        | 96, 79, 91, 85, 83, 91, 82, 87 
    2        | 77, 76, 74, 73, 78, 71, 80 
    3        | 66, 73, 69, 66, 77, 73, 71, 70, 74

    We will be using the cell means model of ANOVA 
    $$ y_{ij}=\beta_{i}+\epsilon_{ij} $$
    where $\beta_{i}$ is the mean of group $i$ and     $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$.

    a. Create one vector of all 24 hostility test scores `y`. (Use the `c()` function.)
    
    b. Create a design matrix `X` with dummy variables for columns that code for what group an observation belongs to. Notice that `X` will be a $24$ rows by $3$ column matrix. _Hint: An R function that might be handy is `cbind(a,b)` which will bind two vectors or matrices together along the columns. (There is also a corresponding `rbind()` function that binds vectors/matrices along rows.) Also you'll like to have the repeat command `rep()`._
    
    c) Find $\hat{\boldsymbol{\beta}}$ using the matrix formula given in class. _Hint: The R function `t(A)` computes the matrix transpose $\mathbf{A}^{T}$, `solve(A)` computes $\mathbf{A}^{-1}$, and the operator `%*%` does matrix multiplication (used as `A %*% B`)._
    
    d) Examine the matrix $\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\mathbf{X}^{T}$.
What do you notice about it? In particular, think about the result
when you right multiply by $\mathbf{y}$. How does this matrix calculate the appropriate group means and using the appropriate group sizes $n_i$?

2. We will calculate the y-intercept and slope estimates in a simple linear model using matrix notation. We will use a data set that gives the diameter at breast height (DBH) versus tree height for a randomly selected set of trees. In addition, for each tree, a ground measurement of crown closure (CC) was taken. Larger values of crown closure indicate more shading and is often associated with taller tree morphology (possibly). We will be interested in creating a regression model that predicts height based on DBH and CC. In the interest of reduced copying, we will only use 10 observations. *(Note: I made this data up and the DBH values might be unrealistic. Don't make fun of me.)*

    ```{r, echo=FALSE}
    data <- data.frame(DBH=c(30.5, 31.5, 31.7, 32.3, 33.3, 35.0, 35.4, 35.6, 36.3, 37.8),
                       CC =c(.74,  .69,   .65,  .72,  .58,  .50,  .60,  .70,  .52,  .60),
                    Height=c(58,    64,    65,   70,   68,   63,   78,   80,   74,   76))
    library(pander)
    pander(t(data))
    ```

    We are interested in fitting the regression model
    $$y_{i}=\beta_{0}+\beta_{1}x_{i,1}+\beta_{2}x_{i,2}+\epsilon_{i}$$ where $\beta_{0}$ is the y-intercept and $\beta_{1}$ is the slope parameter associated with DBH and $\beta_{2}$ is the slope parameter associated with Crown Closure.

    a) Create a vector of all 10 heights $\mathbf{y}$.
    b) Create the design matrix $\mathbf{X}$.
    c) Find $\hat{\boldsymbol{\beta}}$ using the matrix formula given in class.
    d) Compare your results to the estimated coefficients you get using the `lm()` function. To add the second predictor to the model, your call to `lm()` should look something like `lm(Height ~ DBH + CrownClosure)`. 





<!--chapter:end:02_Estimation.Rmd-->

# Inference
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

## F-tests

We wish to develop a rigorous way to compare nested models and decide if a complicated model explains enough more variability than a simple model to justify the additional intellectual effort of thinking about the data in the complicated fashion.

It is important to specify that we are developing a way of testing nested models. By nested, we mean that the simple model can be created from the full model just by setting one or more model parameters to zero.

### Theory

Recall that in the simple regression and ANOVA cases we were interested in comparing a simple model versus a more complex model. For each model we computed the residual sum of squares (RSS) and said that if the complicated model performed much better than the simple then $RSS_{simple}\gg RSS_{complex}$. To do this we needed to standardize by the number of parameters added to the model and the degrees of freedom remaining in the full model. We first defined $RSS_{diff}=RSS_{simple}-RSS_{complex}$ and let $df_{diff}$ be the number of parameters difference between the simple and complex models. Then we had $$F=\frac{RSS_{difference}/df_{diff}}{RSS_{complex}/df_{complex}}$$
and we claimed that if the null hypothesis was true (i.e. the complex model is an unnecessary obfuscation of the simple), then this ratio follows an F
 -distribution with degrees of freedom $df_{diff}$ and $df_{complex}$.

The critical assumption for the F-test to be appropriate is that the error terms are independent and normally distributed with constant variance.

We will consider a data set from Johnson and Raven (1973) which also appears in Weisberg (1985). This data set is concerned with the number of tortoise species on $n=30$ different islands in the Galapagos. The variables of interest in the data set are:

  Variable  |   Description
------------|-----------------
`Species`   |  Number of tortoise species found on the island
`Endimics`  |  Number of tortoise species endemic to the island
`Elevation` |  Elevation of the highest point on the island
`Area`      |  Area of the island (km$^2$)
`Nearest`   |  Distance to the nearest neighboring island (km)
`Scruz`     |  Distance to the Santa Cruz islands (km)
`Adjacent`  |  Area of the nearest adjacent island (km$^2$)

We will first read in the data set from the package `faraway`.

```{r}
library(faraway)    # load the library 
data(gala)          # import the data set
head(gala)          # show the first couple of rows
```


First we will create the full model that predicts the number of species as a function of elevation, area, nearest, scruz and adjacent. Notice that this model has $p=6$ $\beta_{i}$ values (one for each coefficient plus the intercept).

$$ y_i = \beta_0 + \beta_1 Area_i + \beta_2 Elevation_i + \beta_3 Nearest_i + \beta_4 Scruz_i + \beta_5 Adjacent_i + \epsilon_i$$

We can happily fit this model just by adding terms on the left hand side of the model formula.  Notice that R creates the design matrix $X$ for us.
```{r}
M.c <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
model.matrix(M.c)  # this is the design matrix X.
```

All the usual calculations from chapter two can be calculated and we can see the summary table for this regression as follows:
```{r}
summary(M.c)
```

### Testing All Covariates

The first test we might want to do is to test if any of the covariates are significant. That is to say that we want to test the full model versus the simple null hypothesis model
$$y_{i}=\beta_{0}+\epsilon_{i}$$
that has no covariates and only a y-intercept. So we will create a simple model

```{r}
M.s <- lm(Species ~ 1, data=gala)
```

and calculate the appropriate Residual Sums of Squares (RSS) for each model, along with the difference in degrees of freedom between the two models.

```{r}
RSS.c <- sum(resid(M.c)^2)
RSS.s <- sum(resid(M.s)^2)
df.diff <- 5               # complex model has 5 additional parameters
df.c <- 30 - 6             # complex model has 24 degrees of freedom left
```

The F-statistic for this test is therefore

```{r}
F.stat <-  ( (RSS.s - RSS.c) / df.diff ) / ( RSS.c / df.c )
F.stat
```

and should be compared against the F-distribution with $5$ and $24$ degrees of freedom. Because a large difference between RSS.s and RSS.c would be evidence for the alternative, larger model, the p-value for this test is $$p-value=P\left(F_{5,24}\ge\mathtt{F.stat}\right)$$
 
```{r}
p.value <-  1 - pf(15.699, 5, 24)
p.value
```


Both the F.stat and its p-value are given at the bottom of the summary table. However, I might be interested in creating an ANOVA table for this situation.

Source         |  df   |  Sum Sq  | 	Mean Sq                 |	F             |  p-value             |
---------------|-------|----------|---------------------------|---------------|----------------------|
Difference	   | $p-1$ | $RSS_d$  | $MSE_d = RSS_d / (p-1)$   | $MSE_d/MSE_c$ | $P(F > F_{p-1,n-p})$ |
Complex        | $n-p$ | $RSS_c$  | $MSE_c = RSS_c / (n-p)$   |               |                      |
Simple         | $n-1$ | $RSS_s$  |                           |               |                      |


This table can be obtained from R by using the `anova()` function on the two models of interest. As usual with R, it does not show the simple row, but rather concentrates on the difference row.

```{r}
anova(M.s, M.c)
```


### Testing a Single Covariate

For a particular covariate, $\beta_{j}$, we might wish to perform a test to see if it can be removed from the model. It can be shown that the F-statistic can be re-written as

$$\begin{aligned}
F	&=	\frac{\left[RSS_{s}-RSS_{c}\right]/1}{RSS_{c}/\left(n-p\right)}\\
	&=	\vdots\\
	&=	\left[\frac{\hat{\beta_{j}}}{SE\left(\hat{\beta}_{j}\right)}\right]^{2}\\
	&= t^{2}
\end{aligned}$$
where $t$ has a t-distribution with $n-p$ degrees of freedom under the null hypothesis that the simple model is sufficient.

We consider the case of removing the covariate `Area` from the model and will calculate our test statistic using both methods.

```{r}
M.c <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
M.s <- lm(Species ~        Elevation + Nearest + Scruz + Adjacent, data=gala)
RSS.c <- sum( resid(M.c)^2 )
RSS.s <- sum( resid(M.s)^2 )
df.d <- 1
df.c <- 30-6
F.stat <- ((RSS.s - RSS.c)/1) / (RSS.c / df.c)
F.stat
1 - pf(F.stat, 1, 24)
sqrt(F.stat)
```

To calculate it using the estimated coefficient and its standard error, we must grab those values from the summary table

```{r}
temp <- summary(M.c)
temp$coefficients
beta.area <- temp$coefficients[2,1]
SE.beta.area <- temp$coefficients[2,2]
t <- beta.area / SE.beta.area
t
2 * pt(t, 24)
```


All that hand calculation is tedious, so we can again use the anova() command to compare the two models.

```{r}
anova(M.s, M.c)
```

### Testing a Subset of Covariates

Often a researcher will want to remove a subset of covariates from the model. In the Galapagos example, Area, Nearest, and Scruz all have non-significant p-values and would be removed when comparing the full model to the model without that one covariate. While each of them might be non-significant, is the sum of all three significant? 

Because the individual $\hat{\beta}_{j}$ values are not independent, then we cannot claim that the subset is not statistically significant just because each variable in turn was insignificant. Instead we again create simple and complex models in the same fashion as we have previously done. 

```{r}
M.c <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
M.s <- lm(Species ~        Elevation +                   Adjacent, data=gala)
anova(M.s, M.c)
```

We find a large p-value associated with this test and can safely stay with the null hypothesis, that the simple model is sufficient to explain the observed variability in the number of species of tortoise.

## Confidence Intervals for location parameters

Recall that 
$$\hat{\boldsymbol{\beta}}\sim N\left(\boldsymbol{\beta},\,\sigma^{2}\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}\right)$$
and it is easy to calculate the estimate of $\sigma^{2}$. This estimate will be the “average” squared residual $$\hat{\sigma}^{2}=\frac{RSS}{df}$$
where $RSS$ is the residual sum of squares and $df$ is the degrees of freedom $n-p$ where $p$ is the number of $\beta_{j}$ parameters. Therefore the standard error of the $\hat{\beta}_{j}$ values is
$$SE\left(\hat{\beta}_{j}\right)=\sqrt{\hat{\sigma}^{2}\left(\mathbf{X}^{T}\mathbf{X}\right)_{jj}^{-1}}$$
 

We can see this calculation in the summary regression table. We again consider the Galapagos Island data set. First we must create the design matrix

```{r}
y <- gala$Species
X <- cbind( rep(1,30), gala$Elevation, gala$Adjacent )
```

And then create $\left(\mathbf{X}^{T}\mathbf{X}\right)^{-1}$
```{r}
XtXinv <- solve(  t(X) %*% X )
XtXinv
diag(XtXinv)
```


Eventually we will need $\hat{\boldsymbol{\beta}}$
 
```{r}
beta.hat <- XtXinv %*% t(X) %*% y
beta.hat
```

And now find the estimate $\hat{\sigma}$
 

```{r}
H <- X %*% XtXinv %*% t(X)
y.hat <- H %*% y
RSS <- sum( (y-y.hat)^2 )
sigma.hat <- sqrt(  RSS/(30-3) )
sigma.hat
```

The standard errors of $\hat{\beta}$ is thus

```{r}
sqrt( sigma.hat^2 * diag(XtXinv) )
```

We can double check that this is what R calculates in the summary table

```{r}
model <- lm(Species ~ Elevation + Adjacent, data=gala)
summary(model)
```

It is highly desirable to calculate confidence intervals for the regression parameters. Recall that the general form of a confidence interval is
$$Estimate\;\pm Critical\,Value\;\cdot\;StandardError\left(Estimate\right)$$
For any specific $\beta_{j}$ we will have 
$$\hat{\beta}_{j}\pm t_{n-p}^{1-\alpha/2}\,\hat{\sigma}\sqrt{\left(\mathbf{X}^{T}\mathbf{X}\right)_{jj}^{-1}}$$
where $\hat{\sigma}^{2}\left(\mathbf{X}^{T}\mathbf{X}\right)_{jj}^{-1}$ is the $[j,j]$ element of the variance/covariance of $\hat{\boldsymbol{\beta}}$. 

To demonstrate this, we return to the Galapagos Island data set.

Finally we can calculate confidence intervals for our three $\beta_{j}$ values

```{r}
lower <- beta.hat - qt(.975, 27) * sigma.hat * sqrt( diag(XtXinv) )
upper <- beta.hat + qt(.975, 27) * sigma.hat * sqrt( diag(XtXinv) )
cbind(lower, upper)
```

That is certainly a lot of work to do by hand (even with R doing all the matrix multiplication) but we can get these from R by using the confint() command.

```{r}
confint(model)
```



## Prediction and Confidence Intervals for a response

Given a vector of predictor covariates $\boldsymbol{x}_{0}$ (think of $\boldsymbol{x}_{0}^{T}$ as potentially one row in $\boldsymbol{X}$. Because we might want to predict some other values than what we observe, we do not restrict ourselves to *only* rows in $\boldsymbol{X}$), we want to make inference on the expected value $\hat{y}_{0}$. We can calculate the value by 
$$\hat{y}_{0}=\boldsymbol{x}_{0}^{T}\hat{\boldsymbol{\beta}}$$
and we are interested in two different types of predictions. 

1. We might be interested in the uncertainty of a new data point. This uncertainty has two components: the uncertainty of the regression model and uncertainty of a new data point from its expected value.

2. Second, we might be interested in only the uncertainty about the regression model.

We note that because $\boldsymbol{x}_{0}^{T}$ is just a constant, we can calculate the variance of this value as
\[ \begin{aligned}
Var\left(\boldsymbol{x}_{0}^{T}\hat{\boldsymbol{\beta}}\right)	
  &= \boldsymbol{x}_{0}^{T}\,Var\left(\hat{\boldsymbol{\beta}}\right)\,\boldsymbol{x}_{0} \\
	&=	\boldsymbol{x}_{0}^{T}\,\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\sigma^{2}\,\boldsymbol{x}_{0} \\
	&=	\boldsymbol{x}_{0}^{T}\,\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\,\boldsymbol{x}_{0}\,\sigma^{2}
\end{aligned}\]
and use this to calculate two types of intervals. First, a prediction interval for a new observation is $$\hat{y}_{0}\pm t_{n-p}^{1-\alpha/2}\,\hat{\sigma}\sqrt{1+\boldsymbol{x}_{0}^{T}\,\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\,\boldsymbol{x}_{0}}$$
and a confidence interval for the mean response for the given $\boldsymbol{x}_{0}$ is 
$$\hat{y}_{0}\pm t_{n-p}^{1-\alpha/2}\,\hat{\sigma}\sqrt{\boldsymbol{x}_{0}^{T}\,\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\,\boldsymbol{x}_{0}}$$

Again using the Galapagos Island data set as an example, we might be interested in predicting the number of tortoise species of an island with highest point $400$ meters and nearest adjacent island with area $200 km^{2}$. We then have $$\boldsymbol{x}_{0}^{T} = \left[\begin{array}{ccc}1  &  400  &  200\end{array}\right]$$
and we can calculate

```{r}
x0 <- c(1, 400, 200)
y0 <- t(x0) %*% beta.hat
y0
```

and then calculate $\boldsymbol{x}_{0}^{T}\,\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\,\boldsymbol{x}_{0}$
 
```{r}
xt.XtXinv.x <- t(x0) %*% solve( t(X) %*% X ) %*% x0
```

Thus the prediction interval will be 

```{r}
c(y0 - qt(.975, 27) * sigma.hat * sqrt(1 + xt.XtXinv.x),
  y0 + qt(.975, 27) * sigma.hat * sqrt(1 + xt.XtXinv.x))
```

while a confidence interval for the expectation is

```{r}
c(y0 - qt(.975, 27) * sigma.hat * sqrt(xt.XtXinv.x),
  y0 + qt(.975, 27) * sigma.hat * sqrt(xt.XtXinv.x))
```


These prediction and confidence intervals can be calculated in R using the predict() function

```{r}
x0 <- data.frame(Elevation=400, Adjacent=200)
predict(model, newdata=x0, interval='prediction')
predict(model, newdata=x0, interval='confidence')
```

## Interpretation with Correlated Covariates

The standard interpretation of the slope parameter is that $\beta_{j}$ is the amount of increase in $y$ for a one unit increase in the $j$th covariate, provided that all other covariates stayed the same.

The difficulty with this interpretation is that covariates are often related, and the phrase “all other covariates stayed the same” is often not reasonable. For example, if we have a dataset that models the mean annual temperature of a location as a function of latitude, longitude, and elevation, then it is not physically possible to hold latitude, and longitude constant while changing elevation. 

One common issue that make interpretation difficult is that covariates can be highly correlated. 

Perch Example: We might be interested in estimating the weight of a fish based off of its length and width. The dataset we will consider is from fishes are caught from the same lake (Laengelmavesi) near Tampere in Finland. The following variables were observed:

  Variable    |   Interpretation
--------------|----------------------
 `Weight`     |  Weight (g)
 `Length.1`   |  Length from nose to beginning of Tail (cm)
 `Length.2`   |  Length from nose to notch of Tail (cm)
 `Length.3`   |  Length from nose to tip of tail (cm)
 `Height`     |  Maximal height as a percentage of `Length.3`
 `Width`      |  Maximal width as a percentage of `Length.3`
 `Sex`        |  0=Female, 1=Male
 `Species`    |  Which species of perch (1-7)
 
 
 
We first look at the data and observe the expected relationship between length and weight.

```{r}
file <- 'https://raw.githubusercontent.com/dereksonderegger/STA_571_Book/master/data-raw/Fish.csv'
fish <- read.table(file, header=TRUE, skip=111, sep=',')
pairs(fish[,c('Weight','Length.1','Length.2','Length.3','Height','Width')])
```

Naively, we might consider the linear model with all the length effects present.

```{r}
model <- lm(Weight ~ Length.1 + Length.2 + Length.3 + Height + Width, data=fish)
summary(model)
```

This is crazy. There is a negative relationship between `Length.2` and `Weight`. That does not make any sense unless you realize that this is the effect of `Length.2` assuming the other covariates are in the model and can be held constant while changing the value of `Length.2`, which is obviously ridiculous. 

If we remove the highly correlated covariates then we see a much better behaved model

```{r}
model <- lm(Weight ~ Length.2 + Height + Width, data=fish)
summary(model)
```

When you have two variables in a model that are highly positively correlated, you often find that one will have a positive coefficient and the other will be negative. Likewise, if two variables are highly negatively correlated, the two regression coefficients will often be the same sign. 

In this case the sum of the three length covariate estimates was approximately $31$ in both cases, but with three length variables, the second could be negative the third be positive with approximately the same magnitude and we get approximately the same model as with both the second and third length variables missing from the model.

In general, you should be very careful with the interpretation of the regression coefficients when the covariates are highly correlated. We will talk about how to recognize these situations and what to do about them later in the course.

## Exercises
1. The dataset prostate in package `faraway` has information about a study of 97 men with prostate cancer. We import the data and examine the first four observations using the following commands.
    ```{r, eval=FALSE}
    library(faraway)
    data(prostate)
    head(prostate)
    ```
    It is possible to get information about the data set using the command `help(prostate)`. Fit a model with `lpsa` as the response and all the other variables as predictors.
    
    a) Compute $90\%$ and $95\%$ confidence intervals for the parameter associated with `age`. Using just these intervals, what could we deduced about the p-value for age in the regression summary. *Hint: look at the help for the function `confint()`. You'll find the `level` option to be helpful.*
    
    b) Remove all the predictors that are not significant at the $5\%$ level. Test this model against the original model. Which is preferred?

2. Thirty samples of cheddar cheese were analyzed for their content of acetic acid, hydrogen sulfide and lactic acid. Each sample was tasted and scored by a panel of judges and the average taste score produces. Used the `cheddar` dataset from the `faraway` package (import it the same way you did in problem one, but now use `cheddar`) to answer the following:
    
    a) Fit a regression model with taste as the response and the three chemical contents as predictors. Identify the predictors that are statistically significant at the $5\%$ level.
    
    b) `Acetic` and `H2S` are measured on a log$_{10}$ scale. Create two new columns in the `cheddar` data frame that contain the values on their original scale. Fit a linear model that uses the three covariates on their non-log scale. Identify the predictors that are statistically significant at the 5% level for this model.
    
    c) Can we use an $F$-test to compare these two models? Explain why or why not. Which model provides a better fit to the data? Explain your reasoning.
    
    d) For the model in part (a), if a sample of cheese were to have `H2S` increased by 2 (where H2S is on the log scale and we increase this value by 2 using some method), what change in taste would be expected? What caveates must be made in this interpretation? _Hint: I don't want to get into interpreting parameters on the log scale just yet. So just interpret this as adding 2 to the covariate value and predicting the change in taste._

3. The `sat` data set in the `faraway` package gives data collected to study the relationship between expenditures on public education and test results.
    
    a) Fit a model that with `total` SAT score as the response and only the intercept as a covariate.
    
    b) Fit a model with `total` SAT score as the response and `expend`, `ratio`, and `salary` as predictors (along with the intercept). 
    
    c) Compare the models in parts (a) and (b) using an F-test. Is the larger model superior?
    
    d) Examine the summary table of the larger model? Does this contradict your results in part (c)? What might be causing this issue? Create a graph or summary diagnostics to support your guess.
    
    e) Fit the model with `salary` and `ratio` (along with the intercept) as predictor variables and examine the summary table. Which covariates are significant?
    
    f) Now add `takers` to the model (so the model now includes three predictor variables along with the intercept). Test the hypothesis that $\beta_{takers}=0$ using the summary table. 
    
    g) Discuss why `ratio` was not significant in the model in part (e) but was significant in part (f). *Hint: Look at the Residual Standard Error $\hat{\sigma}$ in each model and argue that each t-statistic is some variant of a "signal-to-noise" ratio and that the "noise" part is reduced in the second model.* 
    

<!--chapter:end:03_Inference.Rmd-->

# Analysis of Covariance (ANCOVA)

```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

```{r, message=FALSE, warning=FALSE}
library(faraway)   # for the data 
library(ggplot2)   # my favorite graphing package
library(dplyr)     # for the %>% operator
```


One way that we could extend the ANOVA and regression models is to have both categorical and continuous predictor variables. This model is commonly called ANCOVA which stands for *Analysis of Covariance*.

The dataset `teengamb` in the package `faraway` has data regarding the rates of gambling among teenagers in Britain and their gender and socioeconomic status. One question we might be interested in is how gender and income relate to how much a person gambles. But what should be the effect of gender be?

There are two possible ways that gender could enter the model. Either:

1. We could fit two lines to the data one for males and one for females but require that the lines be parallel (i.e. having the same slopes for income). This is accomplished by having a separate y-intercept for each gender. In effect, the line for the females would be offset by a constant amount from the male line. 

2. We could fit two lines but but allow the slopes to differ as well as the y-intercept. This is referred to as an “interaction” between income and gender.

```{r, fig.height=4, echo=FALSE, message=FALSE, warning=FALSE}
library(ggplot2)
library(faraway)
teengamb$sex <- factor(teengamb$sex, labels=c('Male','Female')) 
m1 <- lm( gamble ~ sex + income, data=teengamb )  
m2 <- lm( gamble ~ sex * income, data=teengamb )  
yhat.1 <- predict(m1) 
yhat.2 <- predict(m2) 
temp <- rbind( cbind(teengamb,yhat=yhat.1,method='Additive'),
               cbind(teengamb,yhat=yhat.2,method='Interaction') ) 
ggplot(temp, aes(x=income, col=sex)) + 
      geom_point(aes(y=gamble)) +
      geom_line(aes(y=yhat)) +
      facet_wrap(~ method) 
```

We will now see how to go about fitting these two models. As might be imagined, these can be fit in the same fashion we have been solving the linear models, but require a little finesse in defining the appropriate design matrix $\boldsymbol{X}$.

## Offset parallel Lines (aka additive models)

In order to get offset parallel lines, we want to write a model 
$$y_{i}=\begin{cases}
\beta_{0}+\beta_{1}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if female}\\
\beta_{0}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if male}
\end{cases}$$
where $\beta_{1}$ is the vertical offset of the female group regression line to the reference group, which is the males regression line. Because the first $19$ observations are female, we can this in in matrix form as 
$$\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{19}\\
y_{20}\\
\vdots\\
y_{47}
\end{array}\right]=\left[\begin{array}{ccc}
1 & 1 & x_{1}\\
\vdots & \vdots & \vdots\\
1 & 1 & x_{19}\\
1 & 0 & x_{20}\\
\vdots & \vdots & \vdots\\
1 & 0 & x_{47}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\vdots\\
\epsilon_{19}\\
\epsilon_{20}\\
\vdots\\
\epsilon_{47}
\end{array}\right]$$

I like this representation where $\beta_{1}$ is the offset from the male regression line because it makes it very convenient to test if the offset is equal to zero. The second column of the design matrix referred to as a “dummy variable” or “indicator variable” that codes for the female gender. Notice that even though I have two genders, I only had to add one additional variable to my model because we already had a y-intercept $\beta_{0}$ and we only added one indicator variable for females.

What if we had a third group? Then we would fit another column of indicator variable for the third group. The new beta coefficient in the model would be the offset of the new group to the reference group. For example we consider $n=9$ observations with $n_i=3$ observations per group where $y_{i,j}$is the $j$ th replication of the $i$th group.
$$\left[\begin{array}{c}
y_{1,1}\\
y_{1,2}\\
y_{1,3}\\
y_{2,1}\\
y_{2,2}\\
y_{2,3}\\
y_{3,1}\\
y_{3,2}\\
y_{3,3}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 0 & 0 & x_{1,1}\\
1 & 0 & 0 & x_{1,2}\\
1 & 0 & 0 & x_{1,3}\\
1 & 1 & 0 & x_{2,1}\\
1 & 1 & 0 & x_{2,2}\\
1 & 1 & 0 & x_{2,3}\\
1 & 0 & 1 & x_{3,1}\\
1 & 0 & 1 & x_{3,2}\\
1 & 0 & 1 & x_{3,3}
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1,1}\\
\epsilon_{1,2}\\
\epsilon_{1,3}\\
\epsilon_{2,1}\\
\epsilon_{2,2}\\
\epsilon_{2,3}\\
\epsilon_{3,3}\\
\epsilon_{3,2}\\
\epsilon_{3,3}
\end{array}\right]
$$ 

In this model, $\beta_0$ is the y-intercept for group $1$. The paramter $\beta_1$ is the vertical offset from the reference group (group $1$) for the second group.  Similarly $\beta_2$ is the vertical offset for group $3$. All groups will share the same slope, $\beta_4$.  

## Lines with different slopes (aka Interaction model)

We can now include a discrete random variable and create regression lines that are parallel, but often that is inappropriate, such as in the teenage gambling dataset. We want to be able to fit a model that has different slopes.
$$y_{i}=\begin{cases}
\left(\beta_{0}+\beta_{1}\right)+\left(\beta_{2}+\beta_{3}\right)x_{i}+\epsilon_{i} & \;\;\;\textrm{if female}\\
\beta_{0}+\beta_{2}x_{i}+\epsilon_{i} & \;\;\;\textrm{if male}
\end{cases}
$$
Where $\beta_{1}$ is the offset in y-intercept of the female group from the male group, and $\beta_{3}$ is the offset in slope. Now our matrix formula looks like

$$\left[\begin{array}{c}
y_{1}\\
\vdots\\
y_{19}\\
y_{20}\\
\vdots\\
y_{47}
\end{array}\right]=\left[\begin{array}{cccc}
1 & 1 & x_{1} & x_{1}\\
\vdots & \vdots & \vdots & \vdots\\
1 & 1 & x_{19} & x_{19}\\
1 & 0 & x_{20} & 0\\
\vdots & \vdots & \vdots & \vdots\\
1 & 0 & x_{47} & 0
\end{array}\right]\left[\begin{array}{c}
\beta_{0}\\
\beta_{1}\\
\beta_{2}\\
\beta_{3}
\end{array}\right]+\left[\begin{array}{c}
\epsilon_{1}\\
\vdots\\
\epsilon_{19}\\
\epsilon_{20}\\
\vdots\\
\epsilon_{47}
\end{array}\right]
$$
where the new fourth column is the what I would get if I multiplied the $\boldsymbol{x}$ column element-wise with the dummy-variable column. To fit this model in R we have
```{r}
library(ggplot2)
library(faraway)

# Forces R to recognize that 0, 1 are categorical, also
# relabels the levels to something I understand.
teengamb$sex <- factor(teengamb$sex, labels=c('Male','Female')) 

# Fit a linear model with the interaction of sex and income
# Interactions can be specified useing a colon :
m1 <- lm( gamble ~ sex + income + sex:income, data=teengamb )  

# R allows a shortcut for the prior definition
m1 <- lm( gamble ~ sex * income, data=teengamb )

# save the fit, lwr, upr values for each observation
# these are the yhat and CI 
teengamb <- cbind(teengamb, predict(m1, interval='conf'))

# Make a nice plot that includes the regreesion line.
ggplot(teengamb, aes(x=income, col=sex, fill=sex)) + 
      geom_ribbon(aes(ymin=lwr, ymax=upr),
                  alpha=.3) +   # how solid the layer is
      geom_point(aes(y=gamble)) +
      geom_line(aes(y=fit)) 

# print the model summary
summary(m1)
```

## Iris Example

For a second example, we will explore the relationship between sepal length and sepal width for three species of irises. This data set is available in R as `iris`. 

```{r}
data(iris)            # read in the iris dataset
levels(iris$Species)  # notice the order of levels of Species
```

The very first thing we should do when encountering a dataset is to do some sort of graphical summary to get an idea of what model seems appropriate.

```{r}
ggplot(iris, aes(x=Sepal.Length, y=Sepal.Width, color=Species)) +
  geom_point()
```

Looking at this graph, it seems that I will likely have a model with different y-intercepts for each species, but it isn't clear to me if we need different slopes.

We consider the sequence of building successively more complex models:


```{r}
# make virginica the reference group 
iris$Species <- relevel(iris$Species, ref='virginica')

m1 <- lm( Sepal.Width ~ Sepal.Length, data=iris )            # One line
m2 <- lm( Sepal.Width ~ Sepal.Length + Species, data=iris )  # Parallel Lines
m3 <- lm( Sepal.Width ~ Sepal.Length * Species, data=iris )  # Non-parallel Lines
```

The three models we consider are the following:

```{r, fig.height=6, echo=FALSE}
yhat.1 <- predict(m1) 
yhat.2 <- predict(m2) 
yhat.3 <- predict(m3)
temp <- rbind( 
	cbind(iris, yhat=yhat.1, method='m1'),
	cbind(iris, yhat=yhat.2, method='m2'),
	cbind(iris, yhat=yhat.3, method='m3'))
ggplot(temp, aes(x=Sepal.Length, col=Species)) +
  geom_point(aes(y=Sepal.Width)) + 
  geom_line(aes(y=yhat)) + 
  facet_grid(method ~ .) 
```

Looking at these, it seems obvious that the simplest model where we ignore Species is horrible. The other two models seem decent, and I am not sure about the parallel lines model vs the differing slopes model. 

```{r}
summary(m1)$coefficients %>% round(digits=3)  
```

For the simplest model, there is so much unexplained noise that the slope variable isn't significant.

Moving onto the next most complicated model, where each species has their own y-intercept, but they share a slope, we have
```{r}
summary(m2)$coefficients %>% round(digits=3)  
```

The first two lines are the y-intercept and slope associated with the reference group and the last two lines are the y-intercept offsets from the reference group to *Setosa* and *Versicolor*, respectively. We have that the slope associated with increasing Sepal Length is significant and that *Setosa* has a statistically different y-intercept than the reference group *Virginica* and that *Versicolor* does not have a statstically different y-intercept than the reference group.

Finally we consider the most complicated model that includes two more slope parameters
```{r}
summary(m3)$coefficients %>% round(digits=3)  
```

These parameters are:

  Meaning                                |   R-label                      
-----------------------------------------|--------------------------------
 *Reference group y-intercept *          |  (Intercept)       
 *Reference group slope *                |  Sepal.Length                  
 *offset to y-intercept for Setosa*      |  Speciessetosa                 
 *offset to y-intercept for Versicolor*  |  Speciesversicolor              
 *offset to slope for Setosa*            |  Sepal.Length:Speciessetosa     
 *offset to slope for Versicolor*        |  Sepal.Length:Speciesversicolor 

It appears that slope for *Setosa* is different from the reference group *Virginica*. However because we've added $2$ parameters to the model, testing Model2 vs Model3 is not equivalent to just looking at the p-value for that one slope.  Instead we need to look at the F-test comparing the two models which will evaluate if the decrease in SSE is sufficient to justify the addition of two paramters.

```{r}
anova(m2, m3)
```

The F-test concludes that there is sufficent decrease in the SSE to justify adding two additional parameters to the model. 

## Exercises

1. The in the `faraway` package, there is a dataset named `phbirths` that gives babies birth weights along with their gestational time in utero along with the mother's smoking status. 
    a. Load and inspect the dataset using
        ```{r, eval=FALSE}
        library(faraway)    # load the package
        data(phbirths)      # load the data within the package
        ?phbirths
        ```
    b. Create a plot of the birthweight vs the gestational age. Color code the points based on the mother's smoking status. Does it appear that smoking matters?
    c. Fit the simple model (one regression line) along with both the main effects (parallel lines) and interaction (non-parallel lines) ANCOVA model to these data.  Which model is preferred?
    d. Using whichever model you selected in the previous section, create a graph of the data along with the confidence region for the regression line(s).
    e. Now consider only the "full term babies" which are babies with gestational age at birth $\ge 36$ weeks. With this reduced dataset, repeat parts c,d.
    f. Interpret the relationship between gestational length and mother's smoking status on birthweight.


2. The in the `faraway` package, there is a dataset named `clot` that gives information about the time for blood to clot verses the blood dilution concentration when the blood was diluted with prothrombin-free plasma.  Unfortunately the researchers had to order the plasma in two different lots (could think of this as two different sources) and need to assertain if the lot number makes any difference in clotting time. 
    a. Log transform the `time` and `conc` variable and plot the log-transformed data with color of the data point indicating the lot number. 
    b. Ignoring the slight remaining curvature in the data, perform the appropriate analysis using transformed variables. Does `lot` matter?


3. In the `faraway` package, there is a data set `ToothGrowth` which is data from an experiment giving Vitamin C to guinea pigs. Guinea pigs were give vitamin C doses either via orange juice or ascorbic acid and the response of interest was a measure of tooth growth. 
    a. Log transform the `dose` and use that throughout this problem. Use $e$ as the base, which R does by default when you use the `log()` function. 
    b. Graph the data, fit appropriate ANCOVA models, and describe the relationship between the delivery method, log(dose) level, and tooth growth. Produce a graph with the data and the regression line(s) along with the confidence region for the line(s).
    c. Just using your graphs and visual inspection, at low dose levels, say $\log(dose)=-0.7$, is there a difference in delivery method? What about at high dose levels, say $\log(dose)=0.7$? At this point we don't know how to answer this question using appropriate statistical inference, but we will address this in the chapter on contrasts.


<!--chapter:end:04_ANCOVA.Rmd-->

# Contrasts
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

```{r, warning=FALSE, message=FALSE}
library(ggplot2)
library(dplyr)
library(lsmeans)    # for lsmeans()
library(multcomp)   # for glht() 
```


We often are interested in estimating a function of the parameters $\boldsymbol{\beta}$. For example in the offset representation of the ANOVA model with 3 groups we have
$$y_{ij}=\mu+\tau_{i}+\epsilon_{ij}$$
where 
$$
\boldsymbol{\beta}=\left[\mu\;\tau_{2}\;\tau_{3}\right]^{T}
$$
and $\mu$ is the mean of the control group, group one is the control group and thus $\tau_{1}=0$, and $\tau_{2}$ and $\tau_{3}$ are the offsets of group two and three from the control group. In this representation, the mean of group two is $\mu+\tau_{2}$ and is estimated with $\hat{\mu} + \hat{\tau}_2$.

## Estimate and variance

A contrast is a linear combinations of elements of $\boldsymbol{\hat{\beta}}$, which is a fancy way of saying that it is a function of the elements of $\boldsymbol{\hat{\beta}}$
  where the elements can be added, subtracted, or multiplied by constants. In particular, the contrast can be represented by the vector $\boldsymbol{c}$ such that the function we are interested in is $\boldsymbol{c}^{T}\boldsymbol{\hat{\beta}}$.

In the ANOVA case with $k=3$ where we have the offset representation, I might be interested in the mean of group 2, which could be written as 
$$
\mu+\tau_{2}=\underset{\boldsymbol{c}^{T}}{\underbrace{\left[\begin{array}{ccc}
1 & 1 & 0\end{array}\right]}}\cdot\underset{\hat{\boldsymbol{\beta}}}{\underbrace{\left[\begin{array}{c}
\hat{\mu}\\
\hat{\tau_{2}}\\
\hat{\tau_{3}}
\end{array}\right]}}
$$

Similarly in the simple regression case, I will be interested in the height of the regression line at $x_0$. This height can be written as  
$$
\hat{\beta}_{0}+\hat{\beta}_{1}x_{0}=\underset{\boldsymbol{c}^{T}}{\underbrace{\left[\begin{array}{cc}
1 & x_{0}\end{array}\right]}}\cdot\underset{\hat{\boldsymbol{\beta}}}{\underbrace{\left[\begin{array}{c}
\hat{\beta}_{0}\\
\hat{\beta}_{1}
\end{array}\right]}}
$$
In this manner, we could think of the predicted values $\hat{y}_i$ as just the result of the contrasts $\boldsymbol{X} \hat{\boldsymbol{\beta}}$ where our design matrix takes the role of the contrasts.

One of the properties of maximum likelihood estimator (MLEs), is that they are invariant under transformations. Meaning that since $\hat{\boldsymbol{\beta}}$ is the MLE of $\boldsymbol{\beta}$, then $\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}$ is the MLE of $\boldsymbol{c}^{T}\boldsymbol{\beta}$. The only thing we need to perform hypotheses tests and create confidence intervals is an estimate of the variance of $\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}$. 

Because we know the variance of $\hat{\boldsymbol{\beta}}$ is
$$
Var\left(\hat{\boldsymbol{\beta}}\right)=\sigma^{2}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}
$$
and because $\boldsymbol{c}$ is a constant, then 
$$
Var\left(\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\right)=\sigma^{2}\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}
$$
and the standard error is found by plugging in our estimate of $\sigma^{2}$ and taking the square root.
$$
StdErr\left(\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\right)	=	\sqrt{\hat{\sigma}^{2}\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}
	=	\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}
$$ 

As usual, we can now calculate confidence intervals for $\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}$ using the usual formula
$$Est	\pm	t_{n-p}^{1-\alpha/2}\;StdErr\left(\,Est\,\right)$$
$$
\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}	\pm	t_{n-p}^{1-\alpha/2}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}
$$ 


Recall the hostility example which was an ANOVA with three groups with the data

 Method  |	Test Scores
---------|-------------------------
  1	     | 96	79	91	85	83	91	82	87	
  2	     | 77	76	74	73	78	71	80		
  3	     | 66	73	69	66	77	73	71	70	74

We have analyzed this data using both the cell means model and the offset and we will demonstrate how to calculate the group means from the offset representation. Thus we are interested in estimating $\mu+\tau_{2}$ and $\mu+\tau_{3}$.  I am also interested in estimating the difference between treatment 2 and 3 and will therefore be interested in estimating $\tau_{2} - \tau_{3}$.

```{r}
y <- c(96,79,91,85,83,91,82,87,
       77,76,74,73,78,71,80,
       66,73,69,66,77,73,71,70,74)
groups <- factor(c( rep('Group1',8), rep('Group2',7),rep('Group3',9) ))
```

We can fit the offset model and obtain the design matrix and estimate of $\hat{\sigma}$ via the following code. 
```{r}
m <- lm(y ~ groups)            # Fit the ANOVA model (offset representation)
coef(m)                        # Show me beta.hat
X <- model.matrix(m)           # obtains the design matrix
sigma.hat <- summary(m)$sigma  # create the summary table and grab sigma.hat 
beta.hat <- coef(m)
XtX.inv <- solve( t(X) %*% X )
```

Now we calculate 
```{r}
contr <- c(1,1,0)  # define my contrast
ctb <- t(contr) %*% beta.hat
std.err <- sigma.hat * sqrt( t(contr) %*% XtX.inv %*% contr )
ctb
std.err
```


and notice this is the exact same estimate and standard error we got for group two when we fit the cell means model.
```{r}
CellMeansModel <- lm(y ~ groups - 1)
summary(CellMeansModel)$coefficients
```


## Estimating contrasts using `glht()`

Instead of us doing all the matrix calculations ourselves, all we really need is to is specify the row vector $\boldsymbol{c}^{T}$. The function that will do the rest of the calculations is the generalized linear hypothesis test function `glht()` that can be found in the multiple comparisons package `multcomp`. The p-values will be adjusted to correct for testing multiple hypothesis, so there may be slight differences compared to the p-value seen in just the regular summary table.

### 1-way ANOVA

We will again use the Hostility data set and demonstrate how to calculate the point estimates, standard errors and confidence intervals for the group means given a model fit using the offset representation. 

```{r}
y <- c(96,79,91,85,83,91,82,87,
       77,76,74,73,78,71,80,
       66,73,69,66,77,73,71,70,74)
groups <- factor(c( rep('Group1',8), rep('Group2',7),rep('Group3',9) ))
m <- lm(y ~ groups)
summary(m)$coefficients
```

We will now define a row vector (and it needs to be a matrix or else `glht()` will throw an error. First we note that the simple contrast $\boldsymbol{c}^{T}=\left[1\;0\;0\right]$ just grabs the first coefficient and gives us the same estimate and standard error as the summary did.
```{r, message=FALSE, warning=FALSE}
library(multcomp)
contr <- rbind("Intercept"=c(1,0,0)) # 1x3 matrix with row named "Intercept"
test <- glht(m, linfct=contr)        # the linear function to be tested is contr
summary(test)
```


Next we calculate the estimate of all the group means $\mu$, $\mu+\tau_{2}$ and $\mu+\tau_{3}$ and the difference between group 2 and 3. Notice I can specify more than one contrast at a time.
```{r}
contr <- rbind("Mean of Group 1"=c(1,0,0),
               "Mean of Group 2"=c(1,1,0),
               "Mean of Group 3"=c(1,0,1),
               "Diff G2-G3"     =c(0,1,-1)) 
test <- glht(m, linfct=contr)
summary(test)
```

Finally we calculate confidence intervals in the usual manner using the `confint()` function.
```{r}
confint(test, level=0.95)
```


### ANCOVA example

In the ANCOVA case, there are more interesting contrasts to be made. For this example we will use the `ToothGrowth` dataset. This data measuring the effects of different dose levels of vitamin C on tooth growth of guinea pigs. The two different delivery methods are encoded by the variable supp which has levels of orange juice (OJ) and ascorbic acid (VC). 

We first fit a ANCOVA model with an interaction between log(dose) level and delivery method and graph the result.

```{r, fig.height=4, warning=FALSE, message=FALSE}
data('ToothGrowth')
ToothGrowth$logDose <- log( ToothGrowth$dose )
m <- lm(len ~ logDose * supp, data=ToothGrowth)

# predict() gives me the yhat values and optional CI
# these are just the "contrasts" defined by X matrix! 
ToothGrowth$len.hat <- predict(m, interval='confidence')[,1]
ToothGrowth$len.lwr <- predict(m, interval='confidence')[,2]
ToothGrowth$len.upr <- predict(m, interval='confidence')[,3]

# Plot the results using ggplot2
ggplot( ToothGrowth, aes(x=logDose, col=supp, fill=supp)) +
    geom_point(aes(y=len)) +
    geom_line(aes(y=len.hat)) +
    geom_ribbon(aes(ymin=len.lwr, ymax=len.upr), alpha=.2)
```


R has fit this model using the offset representation. First we present the summary coefficients so we know which parameters are which.

```{r}
summary(m)$coefficient
```

For a warm-up, we will calculate the y-intercepts of both groups and the slopes of both. For the OJ group, this is just the 1st and 2nd coefficients, while for the VC group it is $\beta_{0}+\beta_{2}$ and $\beta_{1}+\beta_{3}$. 

```{r}
# Add a heading so that I know which parameter is which!
#                                Int  logDose  suppVC   logDose:suppVC
contr <- rbind("Intercept OJ" = c(1,     0,     0,          0          ),
               "Slope     OJ" = c(0,     1,     0,          0          ),
               "Intercept VC" = c(1,     0,     1,          0          ),
               "Slope     VC" = c(0,     1,     0,          1          ) ) 
test <- glht(m, linfct=contr)
summary(test)
```

In our original table of summary coefficients, the (intercept) term corresponds to the tooth growth of a guinea pig when the logDose level is 0 (and therefore dose=1). If I wanted to estimate the tooth growth of a guinea pig fed OJ but with only $1/2$ a dose (and therefore $logDose=\log\left(1/2\right)=-0.69$), then we want 

$$\begin{aligned}
\hat{y}_{oj,dose=0.5}	& = 	\hat{\beta}_{0}+\hat{\beta}_{1}\log\left(0.5\right)\\
	                    & = 	20.6+9.25\log\left(0.5\right)\\
	                    & = 	20.6+9.25\left(-0.69\right)\\
	                    & = 	14.21
\end{aligned}$$

which I can write as 
$$y_{oj,dose=0.5}	=	\left[\begin{array}{cccc}
1 & -0.69 & 0 & 0\end{array}\right]\left[\begin{array}{c}
\hat{\beta_{0}}\\
\hat{\beta}_{1}\\
\hat{\beta}_{2}\\
\hat{\beta}_{3}
\end{array}\right]
	=	\boldsymbol{c}_{1}^{T}\hat{\boldsymbol{\beta}}$$ 

To calculate the same value for the VC group, we need the following contrast:

$$\begin{aligned}
\hat{y}_{vc,dose=0.5}	
    &=	16.9633+13.0997\,\log\left(0.5\right)\\
    &=	\left(20.66-3.7\right)+\left(9.25+3.8\right)\,\log\left(0.5\right)\\
	  &=	\left(\hat{\beta}_{0}+\hat{\beta}_{2}\right)+\left(\hat{\beta}_{1}+\hat{\beta}_{3}\right)\,\left(-0.69\right)\\
	  &=	\left[\begin{array}{cccc}1 & -0.69 & 1 & -0.69\end{array}\right]\left[\begin{array}{c}
                 \hat{\beta_{0}}\\
                 \hat{\beta}_{1}\\
                 \hat{\beta}_{2}\\
                 \hat{\beta}_{3}\end{array}\right]\\
	  &=	\boldsymbol{c}_{2}^{T}\hat{\boldsymbol{\beta}}
	  \end{aligned}$$




```{r}
#                                Int  logDose  suppVC   logDose:suppVC
contr <- rbind("OJ; 1/2 dose" = c(1,   -0.69,    0,          0         ),
               "VC; 1/2 dose" = c(1,   -0.69,    1,        -0.69       ))
test <- glht(m, linfct=contr)
summary(test)
``` 

Finally we might be interested in testing if there is a treatment difference between OJ and VC at a 1/2
  dose level. So to do this, we want to calculate the difference between these two previous contrasts, i.e.

$$\begin{aligned}
\boldsymbol{c}_{1}^{T}\hat{\boldsymbol{\beta}}-\boldsymbol{c}_{2}^{T}\hat{\boldsymbol{\beta}}	&=	\left(\boldsymbol{c}_{1}^{T}-\boldsymbol{c}_{2}^{T}\right) \hat{\boldsymbol{ \beta }} \\
	&=	\left(\left[\begin{array}{cccc}
1 & -0.69 & 0 & 0\end{array}\right]-\left[\begin{array}{cccc}
1 & -0.69 & 1 & -0.69\end{array}\right]\right)\hat{\boldsymbol{\beta}} \\
	&=	\left[\begin{array}{cccc}
0 & 0 & -1 & 0.69\end{array}\right]\hat{\boldsymbol{\beta}}
\end{aligned}$$

  
and we can calculate
```{r}
contr <- rbind("OJ - VC; 1/2 dose" = c(0, 0, -1, 0.69))
test <- glht(m, linfct=contr)
summary(test)
```


When we do the same test, but at dose level 2 (logDose =$\log2=0.69$) we see that the difference is not statistically significant.
```{r}
contr <- rbind("OJ - VC; dose 2" = c(0, 0, -1, -0.69))
test <- glht(m, linfct=contr)
summary(test)
```

## Using `lsmeans` Package
Specifying the contrasts by hand is extremely difficult to do correctly and instead we would prefer to specify the contrasts using language like "create all possible pairwise contrasts" where each pair is just a subtraction.  The R-package `lsmeans` tries to simply the creation of common contrasts. 

To show how to use the `lsmeans` package, we'll consider a bunch of common models and show how to address common statistical questions for each.

### Simple Regression
There is a dataset built into R named `trees` which describes a set of $n=31$ cherry trees and the goal is to predict the volume of timber produced by each tree just using the tree girth a 4.5 feet above the ground.

```{r}
data(trees)
model <- lm( Volume ~ Girth, data=trees )
trees <- cbind( trees, predict(model, interval='conf'))
ggplot(trees, aes(x=Girth, y=Volume)) +
  geom_point() +
  geom_ribbon( aes(ymin=lwr, ymax=upr), alpha=.3 ) +
  geom_line( aes(y=fit) )
```

Using the `summary()` function, we can test hypotheses about if
the y-intercept or slope could be equal to zero, but we might be interested in confidence intervals for the regression line at girth values of 10 and 12.

```{r}
# We could find the regression line heights and CI using 
# either predict() or lsmeans()
predict(model, newdata=data.frame(Girth=c(10,12)), interval='conf' )
lsmeans(model, specs = ~Girth, at=list(Girth=c(10,12)) )  
```

The `lsmeans()` function requires us to specify the grid of reference points we are interested as well as which variable or variables we wish to separate out.  In the simple regression case, the `specs` argument is just the single covariate.

We might next ask if the difference in volume between a tree with 10 inch girth is statistically different than a tree with 12 inch girth?  In other words, we want to test
$$ H_0:\; (\beta_0 + \beta_1\cdot10 ) - (\beta_0 + \beta_1 \cdot 12) = 0$$ 
$$ H_a:\; (\beta_0 + \beta_1\cdot10 ) - (\beta_0 + \beta_1 \cdot 12) \ne 0$$ 

In this case, we want to look at all possible pairwise differences between the predicted values at $10$ and $12$.

```{r}
lsmeans(model, specs = pairwise~Girth,
        at=list(Girth=c(10,12)) ) 
```

Notice that if I was interested in 3 points, we would get all of the differences.

```{r}
lsmeans(model, specs = pairwise~Girth,
        at=list(Girth=c(10,11,12)) ) 
```

### 1-way ANOVA
To consider the pairwise contrasts between different levels we will consider the college student hostility data again. A clinical psychologist wished to compare three methods for reducing hostility levels in university students, and used a certain test (HLT) to measure the degree of hostility. A high score on the test indicated great hostility. The psychologist used $24$ students who obtained high and nearly equal scores in the experiment. Eight subjects were selected at random from among the $24$ problem cases and were treated with method 1, seven of the remaining $16$ students were selected at random and treated with method 2 while the remaining nine students were treated with method 3. All treatments were continued for a one-semester period. Each student was given the HLT test at the end of the semester, with the results show in the following table.

```{r}
Hostility <- data.frame(
  HLT = c(96,79,91,85,83,91,82,87,
          77,76,74,73,78,71,80,
          66,73,69,66,77,73,71,70,74),
  Method = c( rep('M1',8), rep('M2',7), rep('M3',9) ) )
```

```{r}
ggplot(Hostility, aes(x=Method, y=HLT)) +
  geom_boxplot()
```

To use the `lsmeans()`, we again will use the pairwise command where we specify that we want all the pairwise contrasts between Method levels.

```{r}
model <- lm( HLT ~ Method, data=Hostility )
lsmeans(model, pairwise~Method)
```

### ANCOVA
We will return to the `ToothGrowth` dataset and calculate all the interesting contrasts we previously built by hand.  

```{r}
data('ToothGrowth')
ToothGrowth$logDose <- log( ToothGrowth$dose )
m <- lm(len ~ logDose * supp, data=ToothGrowth)
ToothGrowth <- cbind(ToothGrowth, predict(m, interval='conf'))
ggplot(ToothGrowth, aes(x=logDose, color=supp, fill=supp)) +
  geom_ribbon( aes(ymin=lwr, ymax=upr), alpha=.3 ) +
  geom_line( aes(y=fit) ) +
  geom_point( aes(y=len) )
```

First, lets get the y-intercept for each group. In the following code, the formula with the `| supp` part says that we want to do all the calculations for each level of the supplement types. The call to `lsmeans()` calculates the response at logDose level 0. By passing the result of the `lsmeans()` function call to the `summary()` function, we can tell the summary function to print out both the confidence intervals and t-tests.


```{r}
lsmeans(m, ~logDose | supp, at=list(logDose=0)) %>% 
  summary(infer=c(TRUE,TRUE))
```

Next we estimated the tooth growth for a guinea pig with a logdose = -.69 (i.e. dose=1/2) for both the OJ and VC supplements. We also wanted to calculate the contrast between the two supplement predictions.

```{r}
lsmeans(m, pairwise~logDose*supp, at=list(logDose=-.69)) %>% 
  summary(infer=c(TRUE,TRUE))
```

We next make the same comparison for logdose = 0.69 (i.e. dose=2).
```{r}
lsmeans(m, pairwise~logDose*supp, at=list(logDose=.69)) %>% 
  summary(infer=c(TRUE,TRUE))
```

While `lsmeans()` makes it very easy to calculate the $\hat{y}$ value for some covariate combination or some difference of $\hat{y}$ values, it doesn't handle slope parameters particularly well.  The companion function `lstrends()` is intended to make similar calculations among slopes easy to create. We will now assess the difference in slopes between the two groups.

```{r}
lstrends(m, ~logDose*supp, var = 'logDose')
lstrends(m, pairwise~logDose*supp, var = 'logDose')
```

#### Common `lsmeans()` mistake
It is very common to make a mistake using `lsmeans()` or `lstrends()` if you don't understand what the specification formula means. In particular what happens if you were to use `~logDose` instead of the correct `~logDose*supp`.

Consider the case where we are interested in making predictions for each suppliment type at $1/2$ dose (logDose=-0.69).
```{r}
lsmeans(m, ~logDose*supp, at=list(logDose=-.69))
```

If I were to accidently forget the `supp` part of this, then `lsmeans()` would figure that I wanted to _average_ over the two levels and give me the average of $14.28$ and $7.92$, which is $11.1$. With the interaction term in the model, it is unlikely that I would want to do this, but `lsmeans()` complains a little but lets me do it.

```{r}
lsmeans(m, ~logDose, at=list(logDose=-.69))
```

What would happen if we only use `supp` in the formula specification? In this case we are lucky because the `logDose=-0.69` specified what dose level to look at.  If we had forgotten to specify this, then we would have done this calculation at the mean `logDose` value, which happens to be zero.

```{r}
lsmeans(m, ~supp*logDose, at=list(logDose=0))
lsmeans(m, ~supp)
```

Because the averaging over factor levels isn't what I typically want to do, I start the analysis making sure the model formula I used in the `lm()` command is the same as the one I pass into `lsmeans()`. 


## Exercises

1. We will examine a dataset that summarizes cars featured in the magazine Motor Trend during the year 1974. In particular we will examine the relationship between the vehicles gas mileage (`mpg`) versus the weight (`wt`) and number of cylinders (`cyl`) of the vehicle.
    ```{r, fig.height=4}
    library(ggplot2)
    library(dplyr)
    data(mtcars)
    mtcars$cyl <- factor(mtcars$cyl)     # treat cylinders as categorical
    ggplot(mtcars, aes(x=wt, col=cyl)) +
      geom_point(aes(y=mpg, shape=cyl), size=4) +
      xlab('Weight (tons)') +
      ylab('Miles per Gallon') +
      labs(color='Cylinders', shape='Cylinders')
    ```
    a. Create a smaller set of data with just 9 observations (3 from each cylinder group) and create the design matrix $\boldsymbol{X}$ for fitting the the model for estimating mpg using both weight and number of cylinders and their interaction using the following code:
        ```{r}
        mtcars.small <- mtcars %>%      
          group_by(cyl) %>%             # For each cylinder group
          arrange(cyl, wt) %>%          # Reorder my dataset
          slice(1:3)                    # grab first three rows of each group
        model.matrix(  lm(mpg ~ wt*cyl, data=mtcars.small) )
        ```
        _Hint: the purpose of this step is to make sure everyone knows the column order of the design matrix so that you interprete the $\beta$ terms in the correct order._
        
    b. Denote the coefficients obtained from the summary of your `lm()` call as $\hat{\beta}_{0}$ to $\hat{\beta}_{5}$ (in the order given by R). Write your interaction model out using subscript notation and should be in the form
        $$y_{i}=\begin{cases}
        ??????? & \;\;\textrm{if 4 cylinder}\\
        ??????? & \;\;\textrm{if 6 cylinder}\\
        ??????? & \;\;\textrm{if 8 cylinder}
        \end{cases}$$ 
        and give an interpretation for each $\beta_{j}$ value.
        i. $\hat{\beta}_0$ = ???
        ii. $\hat{\beta}_1$ = ???
        iii. $\hat{\beta}_2$ = ???
        iv. $\hat{\beta}_3$ = ???
        v. $\hat{\beta}_4$ = ???
        vi. $\hat{\beta}_5$ = ???
        
    c. Using the full dataset, fit the model that predicts mpg using both weight and number of cylinders and their interaction using the command
        ```{r}
        model <- lm(mpg ~ wt*cyl, data=mtcars)
        ```
        Create a vector of fitted values, add it to the data frame `mtcars` and then create a plot that includes the regression lines.
    
    d. What is the estimated mpg of a 6-cylinder vehicle weighing 3.5 tons? What is the estimated mpg of a 8-cylinder vehicle weight 3.5 tons? Give the associated 95% confidence intervals. Calculate these using the `predict()` function. *Warning: When making the new data frame, make sure that R interprets cyl as a factor by either coercing it to a factor after creation or inputing the cylinder as a string (i.e. as “6” instead of 6).*
    
    e. Recalculate your answer to part (d) using the `glht()` function. Also give the estimate and confidence interval for the difference in mpg betwen the 8 and 6 cylinder vehicles that weight 3.5 tones? Is there a statistically significant difference in mpg at 3.5 tons?
    
    f. Recalculate your answer to part (e) using the `lsmeans()` function.
    
    g. Are the slopes of the 8-cylinder and 6-cylinder vehicles statistically different? Use `glht()` to produce this result. 
    
    h. Recalculate your answer to part (g) using the `lstrends()` function.

<!--chapter:end:05_Contrasts.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Diagnostics and Transformations
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

We will be interested in analyzing whether or not our linear model is a good model and whether or not the data violate any of the assumptions that are required. In general we will be interested in three classes of assumption violations and our diagnostic measures might be able detect one or more of the following issues:

  1. Unusual observations that contribute too much influence to the analysis. These few observations might drastically change the outcome of the model.

  2. Model misspecification. Our assumption that $E\left[\boldsymbol{y}\right]=\boldsymbol{X}\boldsymbol{\beta}$ might be wrong and we might need to include different covariates in the model to get a satisfactory result.

  3. Error distribution. We have assumed that $\boldsymbol{\epsilon}\sim MVN\left(\boldsymbol{0},\sigma^{2}\boldsymbol{I}\right)$ but autocorrelation, heteroscedasticity, and non-normality might be present.

Often problems with one of these can be corrected by tranforming either the explanatory or response variables.

## Detecting Assumption Violations
Throughout this chapter I will use data created by Francis Anscombe that show how simple linear regression can be misused. In particular, these data sets will show how our diagnostic measures will detect various departures from the model assumptions.

The data are available in R as a data frame `anscombe` and is loaded by default. The data consists of four datasets, each having the same linear regression $\hat{y}=3+0.5\,x$ but the data are drastically different.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)

# The anscombe dataset has 8 columns - x1,x2,x3,x4,y1,y2,y3,y4
# and I want it to have 3 columns - Set, X, Y
Anscombe <- rbind( 
  data.frame(x=anscombe$x1, y=anscombe$y1, set='Set 1'),
  data.frame(x=anscombe$x2, y=anscombe$y2, set='Set 2'),
  data.frame(x=anscombe$x3, y=anscombe$y3, set='Set 3'),
  data.frame(x=anscombe$x4, y=anscombe$y4, set='Set 4')) 

# order them by their x values, and add an index column
Anscombe <- Anscombe %>% 
  group_by(set) %>%       # Every subsequent action happens by dataset
  arrange(x,y) %>%        # sort them on the x-values and if tied, by y-value
  mutate( index = 1:n() ) # give each observation within a set, an ID number

# Make a nice graph
ggplot(Anscombe, aes(x=x, y=y)) +
 geom_point() +
 facet_wrap(~set, scales='free') +
 stat_smooth(method="lm", formula=y~x, se=FALSE)
```




### Measures of Influence

#### Standardized Residuals (aka Studentized )

Recall that we have 

$$\begin{aligned}\hat{\boldsymbol{y}}	
  &=	\boldsymbol{X}\hat{\boldsymbol{\beta}}\\
	&=	\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}\boldsymbol{y}\\
	&=	\boldsymbol{H}\boldsymbol{y}\\
\end{aligned}$$

where the “Hat Matrix” is $\boldsymbol{H}=\boldsymbol{X}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{X}^{T}$ because we have $\hat{\boldsymbol{y}}=\boldsymbol{H}\boldsymbol{y}$. The elements of $\boldsymbol{H}$ can be quite useful in diagnostics. It can be shown that the variance of the $i$the residual is
$$Var\left(\hat{\epsilon}_{i}\right)=\sigma^{2}\left(1-\boldsymbol{H}_{ii}\right)$$
where $\boldsymbol{H}_{ii}$ is the $i$th element of the main diagonal of $\boldsymbol{H}$. This suggests that I could rescale my residuals to 
$$\hat{\epsilon}_{i}^{*}=\frac{\hat{\epsilon}_{i}}{\hat{\sigma}\sqrt{1-\boldsymbol{H}_{ii}}}$$ which, if the normality and homoscedasticity assumptions hold, should behave as a $N\left(0,1\right)$ sample. 

These rescaled residuals are called “studentized residuals”, though R typically refers to them as “standardized”. Since we have a good intuition about the scale of a standard normal distribution, the scale of standardized residuals will give a good indicator if normality is violated.

There are actually two types of studentized residuals, typically called *internal* and *external* among statisticians. The version presented above is the *internal* version which can be obtained using the R function `rstandard()` while the *external* version is available using `rstudent()`. Whenever you see R present standardized residuals, they are talking about internally studentized residuals. For sake of clarity, I will use the term *standardized* as well. 

##### Example - Anscombe's set 3
For the third dataset, the outlier is the ninth observation with $x_{9}=13$ and $y_{9}=12.74$. We calculate the standardized residuals using the function `rstandard()` and plot them

```{r}
Set3 <- Anscombe %>% filter(set == 'Set 3') # Just set 3
model <- lm(y ~ x, data=Set3)               # Fit the regression line
Set3$stdresid <- rstandard(model)           # rstandard() returns the standardized residuals

ggplot(Set3, aes(x=index, y=stdresid)) +    # make a plot
  geom_point() +
  labs(x='Observation Index', 
       y='Standardized Residuals', 
       title='Standardized Residuals vs Observation Index')
```


and we notice that the outlier residual is really big. If the model assumptions were true, then the standardized residuals should follow a standard normal distribution, and I would need to have hundreds of observations before I wouldn't be surprised to see a residual more than 3 standard deviations from 0.

#### Leverage

The extremely large standardized residual suggests that this data point is important, but we would like to quantify how important this observation actually is.

One way to quantify this is to look at the elements of $\boldsymbol{H}$. Because $$\hat{y}_{i}=\sum_{j=1}^{n}\boldsymbol{H}_{ij}y_{j}$$
then the $i$th row of $\boldsymbol{H}$ is a vector of weights that tell us how influential a point $y_{j}$ is for calculating the predicted value $\hat{y}_{i}$. If I look at just the main diagonal of $\boldsymbol{H}$, these are how much weight a point has on its predicted value. As such, I can think of the $\boldsymbol{H}_{ii}$ as the amount of leverage a particular data point has on the regression line. 

Fortunately there is already a function `hatvalues()` to compute these $\boldsymbol{H}_{ii}$ values for me. We will compare the leverages from Anscombe's set 3 versus set 4.

```{r}
Set3 <- Anscombe %>% filter( set == 'Set 3')
Set4 <- Anscombe %>% filter( set == 'Set 4')

model3 <- lm(y ~ x, data = Set3 )
model4 <- lm(y ~ x, data = Set4 )

Set3 <- Set3 %>% mutate(leverage = hatvalues(model3))  # add leverage columns
Set4 <- Set4 %>% mutate(leverage = hatvalues(model4))

ggplot( rbind(Set3,Set4), aes(x=index, y=leverage) ) +
  geom_point() +
  facet_grid( . ~ set )
```

This leverage idea only picks out the *potential* for a specific value of $x$ to be influential, but does not actually measure influence. It has picked out the issue with the fourth data set, but does not adequately address the outlier in set 3. 

#### Cook's Distance

To attempt to measure the actual influence of an observation $\left\{ y_{i},\boldsymbol{x}_{i}^{T}\right\}$ on the linear model, we consider the effect on the regression if we removed the observation and fit the same model. Let $$\hat{\boldsymbol{y}}=\boldsymbol{X}\hat{\boldsymbol{\beta}}$$
be the vector of predicted values, where $\hat{\boldsymbol{\beta}}$ is created using all of the data, and $\hat{\boldsymbol{y}}_{(i)}=\boldsymbol{X}\hat{\boldsymbol{\beta}}_{(i)}$ be the vector of predicted values where $\hat{\boldsymbol{\beta}}_{(i)}$ was estimated using all of the data except the $i$th observation. Letting $p$ be the number of $\beta_{j}$ parameters as usual we define Cook's distance of the $i$th observation as 
$$ D_{i} = \frac{\left(\hat{\boldsymbol{y}}-\hat{\boldsymbol{y}}_{(i)}\right)^{T}\left(\hat{\boldsymbol{y}}-\hat{\boldsymbol{y}}_{(i)}\right)}{p\hat{\sigma}^{2}}$$
which boils down to saying if the predicted values have large changes when the $i$th element is removed, then the distance is big. It can be shown that this formula can be simplified to
$$D_{i}=\frac{\hat{\epsilon}_{i}^{*}\boldsymbol{H}_{ii}}{p\left(1-H_{ii}\right)}$$
which expresses Cook's distance in terms of the $i$th studentized residual and the $i$th leverage.

Nicely, the R function `cooks.distance()` will calculate Cook's distance.

```{r, fig.height=3}
Set3 <- Set3 %>% mutate(cooksd = cooks.distance(model3))  
Set4 <- Set4 %>% mutate(cooksd = cooks.distance(model4))

# Note: The high leverage point in set 4 has a Cook's distance of Infinity.
ggplot(rbind(Set3,Set4), aes(x=index, y=cooksd)) + 
  geom_point() +
  facet_grid(. ~ set) +
  labs(y="Cook's Distance")
```

Some texts will give a rule of thumb that points with Cook's distances greater than 1 should be considered influential, while other books claim a reasonable rule of thumb is $4/\left(n-p-1\right)$ where $n$ is the sample size, and $p$ is the number of parameters in $\boldsymbol{\beta}$. My take on this, is that you should look for values that are highly different from the rest of your data.



### Diagnostic Plots

After fitting a linear model in R, you have the option of looking at diagnostic plots that help to decide if any assumptions are being violated. We will step through each of the plots that are generated by the function `plot(model)` or using `ggplot2` using the package `ggfortify`.

In the package `ggfortify` there is a function that will calculate the diagnostics measures and add them to your dataset.  This will simplify our graphing process.

```{r}
Set1 <- Anscombe %>% filter(set == 'Set 1')
model <- lm( y ~ x, data=Set1)
Set1 <- fortify(model)  # add dignostic measures to the dataset
Set1 %>% round(digits=3) # show the dataset nicely
```


#### Residuals vs Fitted

In the simple linear regression the most useful plot to look at was the residuals versus the $x$-covariate, but we also saw that this was similar to looking at the residuals versus the fitted values. In the general linear model, we will look at the residuals versus the fitted values or possibly the studentized residuals versus the fitted values.

##### Polynomial relationships

To explore how this plot can detect non-linear relationships between $y$ and $x$, we will examine a data set from Ashton et al. (2007) that relates the length of a tortoise's carapace to the number of eggs laid in a clutch. The data are

```{r, fig.height=3}
Eggs <- data.frame(
  carapace    = c(284,290,290,290,298,299,302,306,306,
                  309,310,311,317,317,320,323,334,334),
  clutch.size = c(3,2,7,7,11,12,10,8,8,
                  9,10,13,7,9,6,13,2,8)) 
ggplot(Eggs, aes(x=carapace, y=clutch.size)) + 
  geom_point()
```


Looking at the data, it seems that we are violating the assumption that a linear model is appropriate, but we will fit the model anyway and look at the residual graph.

```{r, fig.height=3}
model <- lm( clutch.size ~ carapace, data=Eggs )
plot(model, which=1)      # which=1 tells R to only make the first plot
```

```{r, fig.height=3, message=FALSE, warning=FALSE}
library(ggfortify)        # need the ggfortify library for autoplot.lm() to work
autoplot(model, which=1)  # same plot using ggplot2
```

The red/blue lines going through the plot is a smoother of the residuals. Ideally this should be a flat line and I should see no trend in this plot. Clearly there is a quadratic trend as larger tortoises have larger clutch sizes until some point where the extremely large tortoises start laying fewer (perhaps the extremely large tortoises are extremely old as well). To correct for this, we should fit a model that is quadratic in `carapace length`. We will create a new covariate, `carapace.2`, which is the square of the carapace length and add it to the model.

In general I could write the quadratic model as
$$y_{i}=\beta_{0}+\beta_{1}x_{i}+\beta_{2}x_{i}^{2}+\epsilon_{i}$$
and note that my model is still a linear model with respect to covariates $\boldsymbol{x}$ and $\boldsymbol{x}^{2}$ because I can still write the model as 
$$\begin{aligned} \boldsymbol{y}	
  &=	\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{\epsilon} \\
	&=	\left[\begin{array}{ccc}
      1 & x_{1} & x_{1}^{2}\\
      1 & x_{2} & x_{2}^{2}\\
      1 & x_{3} & x_{3}^{2}\\
      \vdots & \vdots & \vdots\\
      1 & x_{n} & x_{n}^{2}
  \end{array}\right]\left[\begin{array}{c}
        \beta_{0}\\
        \beta_{1}\\
        \beta_{2}
\end{array}\right]
+
\left[\begin{array}{c}
\epsilon_{1}\\
\epsilon_{2}\\
\epsilon_{3}\\
\vdots\\
\epsilon_{n}
\end{array}\right]\end{aligned}$$
 

```{r}
# add a new column that is carapace^2
Eggs2 <- Eggs %>% mutate( carapace.2 = carapace^2 )
model <- lm( clutch.size ~ carapace + carapace.2,    data=Eggs2 )

# make R do it inside the formula... convenient
model <- lm( clutch.size ~ carapace + I(carapace^2), data=Eggs )

# Fit an arbitrary degree polynomial
model <- lm( clutch.size ~ poly(carapace, 2),        data=Eggs )

# If you use poly() in the formula, you must use 'data=' here, 
# otherwise you can skip it and R will do the right thing.
autoplot(model, which=1, data=Eggs)  

```


Now our residual plot versus fitted values does not show any trend, suggesting that the quadratic model is fitting the data well. Graphing the original data along with the predicted values confirms this.

```{r}
# add the fitted and CI lwr/upr columns to my dataset 
Eggs <- Eggs %>% cbind( predict(model, interval='confidence') )

ggplot(Eggs, aes(x=carapace)) +
  geom_ribbon( aes(ymin=lwr, ymax=upr), fill='red', alpha=.3) +
  geom_line(aes(y=fit), color='red') +
  geom_point(aes(y=clutch.size)) 
```


##### Heteroskedasticity

The plot of residuals versus fitted values can detect heteroskedasticity (non-constant variance) in the error terms. 

To illustrate this, we turn to another dataset in the Faraway book. The dataset airquality uses data taken from an environmental study that measured four variables, ozone, solar radiation, temperature and windspeed for 153 consecutive days in New York. The goal is to predict the level of ozone using the weather variables.

We first graph all pairs of variables in the dataset.

```{r}
data(airquality)
pairs(~ Ozone + Solar.R + Wind + Temp, data=airquality)
```


and notice that ozone levels are positively correlated with solar radiation and temperature, and negatively correlated with wind speed. A linear relationship with wind might be suspect as is the increasing variability in the response to high temperature. However, we don't know if those trends will remain after fitting the model, because there is some covariance amongst the predictors.

```{r, fig.height=3}
model <- lm(Ozone ~ Solar.R + Wind + Temp, data=airquality)
autoplot(model, which=1) 
```


As we feared, we have both a non-constant variance and a non-linear relationship. A transformation of the $y$ variable might be able to fix our problem. 

#### QQplots

If we are taking a sample of size $n=10$ from a standard normal distribution, then I should expect that the smallest observation will be negative. Intuitively, you would expect the smallest observation to be near the $10$th percentile of the standard normal, and likewise the second smallest should be near the $20$th percentile. 

This idea needs a little modification because the largest observation cannot be near the $100$th percentile (because that is $\infty$). So we'll adjust the estimates to still be spaced at $(1/n)$ quantile increments, but starting at the $0.5/n$ quantile instead of the $1/n$ quantile. So the smallest observation should be near the $0.05$ quantile, the second smallest should be near the $0.15$ quantile, and the largest observation should be near the $0.95$ quantile. I will refer to these as the *theoretical quantiles*.

```{r, echo=FALSE}
x <- seq(-3,3,length=1000) 
labels <- NULL 
labels[1] <- expression(paste(z[.05])) 
labels[2] <- expression(paste(z[.15])) 
labels[3] <- expression(paste(z[.25])) 
labels[4] <- expression(paste(z[.35])) 
labels[5] <- expression(paste(z[.45])) 
labels[6] <- expression(paste(z[.55])) 
labels[7] <- expression(paste(z[.65])) 
labels[8] <- expression(paste(z[.75])) 
labels[9] <- expression(paste(z[.85])) 
labels[10]<- expression(paste(z[.95]))
ggplot(data.frame(x=x, y=dnorm(x)), aes(x=x, y=y)) +
  scale_x_continuous(breaks=qnorm(seq(.05,.95,by=.1)), labels=labels) +
  geom_line() +
  labs(y='Density', title='Std Normal distribution')
```

I can then graph the theoretical quantiles vs my observed values and if they lie on the 1-to-1 line, then my data comes from a standard normal distribution. 

```{r}
set.seed(93516)  # make random sample in the next code chunk consistant run-to-run

n <- 10
data <- data.frame( observed = rnorm(n, mean=0, sd=1) ) %>%
  arrange(observed) %>%
  mutate( theoretical = qnorm( (1:n -.5)/n ) )

ggplot(data, aes(x=theoretical, y=observed) ) + 
  geom_point() +
  geom_abline( intercept=0, slope=1,  linetype=2, alpha=.7) + 
  labs(main='Q-Q Plot: Observed vs Normal Distribution')
```


In the context of a regression model, we wish to look at the residuals and see if there are obvious departures from normality. Returning to the airquality example, R will calculate the qqplot for us.

```{r}
model <- lm(Ozone ~ Solar.R + Wind + Temp, data=airquality)
autoplot(model, which=2) 
```


In this case, we have a large number of residuals that are bigger than I would expect them to be based on them being from a normal distribution. We could further test this using the Shapiro-Wilks test and compare the standardized residuals against a $N\left(0,1\right)$ distribution.

```{r}
shapiro.test( rstandard(model) )
```

The tail of the distribution of observed residuals is *far* from what we expect to see.

#### Scale-Location Plot

This plot is a variation on the fitted vs residuals plot, but the y-axis uses the square root of the absolute value of the standardized residuals. Supposedly this makes detecting increasing variance easier to detect, but I'm not convinced.

#### Residuals vs Leverage (plus Cook's Distance)

This plot lets the user examine the which observations have a high potential for being influential (i.e. high leverage) versus how large the residual is. Because Cook's distance is a function of those two traits, we can also divide the graph up into regions by the value of Cook's Distance.

Returning to Anscombe's third set of data, we see

<<>>=
```{r}
model3 <- lm(y ~ x, data=Set3)
autoplot(model3, which=5)
```

that one data point (observation 10) has an extremely large standardized residual. This is one plot where I prefer what the base graphics in R does compared to the ggfortify version. The base version of R adds some contour lines that mark where the contours of where Cook's distance is 1/2 and 1. 

```{r}
plot(model3, which=5)
```

## Transformations

Transformations of the response variable and/or the predictor variables can drastically improve the model fit and can correct violations of the model assumptions. We might also create new predictor variables that are functions of existing variables. These include quadratic and higher order polynomial terms and interaction terms. 

Often we are presented with data and we would like to fit a linear model to the data. Unfortunately the data might not satisfy all of the assumptions of a linear model. For the simple linear model
$$y=\beta_{0}+\beta_{1}x+\epsilon$$
 where $\epsilon\sim N\left(0,\sigma^{2}\right)$, the necessary assumptions are:

1. Independent errors
2. Errors have constant variance, no matter what the x-value (or equivalently the fitted value)
3. Errors are normally distributed
4. The model contains all the appropriate covariates and no more.

In general, a transformation of the response variable can be used to address the 2nd and 3rd assumptions, and adding new covariates to the model will be how to address deficiencies of assumption 4.

### Transforming the Response

When the normality or constant variance assumption is violated, sometimes it is possible to transform the response to satisfy the assumption. Often times count data is analyzed as `log(count)` and weights are analyzed after taking a square root or cube root transform. Statistics involving income or other monetary values are usually analyzed on the log scale so as to reduce the leverage of high income observations.

While we may want to transform the response in order to satisfy the statistical assumptions, it is often necessary to back-transform to the original scale. For example if we fit a linear model for income ($y$) based on the amount of schooling the individual has received ($x$) 
$$\log y=\beta_{0}+\beta_{1}x+\epsilon$$
then we might want to give a prediction interval for an $x_{0}$ value. The predicted $log(income)$ value is 
$$\log\left(\hat{y}_{0}\right)=\hat{\beta}_{0}+\hat{\beta}_{x}x_{0}$$
and we could calculate the appropriate predicted income as $\hat{y}_{0}=e^{log\left(\hat{y}_{0}\right)}$. Likewise if we had a confidence interval or prediction interval for $\log\left(\hat{y}_{0}\right)$ of the form $\left(l,u\right)$ then the appropriate interval for $\hat{y}_{0}$ is $\left(e^{l},e^{u}\right)$. Notice that while $\left(l,u\right)$ might be symmetric about $\log\left(\hat{y}_{0}\right)$, the back-transformed interval is not symmetric about $\hat{y}_{0}$.

Unfortunately the interpretation of the regression coefficients $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ on the untransformed scale becomes more complicated. This is a very serious difficulty and might sway a researcher from transforming their data.

#### Box-Cox Family of Transformations

The Box-Cox method is a popular way of determining what transformation to make. It is intended for responses that are strictly positive (because $\log0=-\infty$ and the square root of a number gives complex numbers, which we don't know how to address in regression). The transformation is defined as $$g\left(y\right)=\begin{cases}
\frac{y^{\lambda}-1}{\lambda} & \lambda\ne0\\
\log y & \lambda=0
\end{cases}$$
This transformation is a smooth family of transformations because $$\lim_{\lambda\to0}\frac{y^{\lambda}-1}{\lambda}=\log y$$
In the case that $\lambda\ne 0$, then a researcher will usually use the simpler transformation $y^{\lambda}$ because the subtraction and division does not change anything in a non-linear fashion. Thus for purposes of addressing the assumption violations, all we care about is the $y^{\lambda}$ and prefer the simpler (i.e. more interpretable) transformation.

Finding the best transformation can be done by adding the $\lambda$ parameter to the model and finding the value that maximizes the log-likelihood function. Fortunately, we don't have to do this by hand, as the function `boxcox()` in the `MASS` library will do all the heavy calculation for us.

```{r, warning=FALSE, message=FALSE}
library(faraway)
library(MASS)
data(gala)
g <- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
boxcox(g)
```


The optimal transformation for these data would be $y^{1/4}=\sqrt[4]{y}$ but that is an extremely uncommon transformation. Instead we should pick the nearest “standard” transformation which would suggest that we should use either the $\log y$ or $\sqrt{y}$ transformation.

Thoughts on the Box-Cox transformation:

1. In general, I prefer to using a larger-than-optimal model when picking a transformation and then go about the model building process. After a suitable model has been chosen, I'll double check the my transformation was appropriate given the model that I ended up with.
2. Outliers can have a profound effect on this method. If the “optimal” transformation is extreme ($\lambda=5$ or something silly) then you might have to remove the outliers and refit the transformation.
3. If the range of the response $y$ is small, then the method is not as sensitive.
4. These are not the only possible transformations. For example, for binary data, the `logit` and `probit` transformations are common.

### Transforming the predictors

#### Polynomials of a predictor

Perhaps the most common transformation to make is to make a quadratic function in $x$. Often the relationship between $x$ and $y$ follows a curve and we want to fit a quadratic model
$$\hat{y}=\hat{\beta}_{0}+\hat{\beta}_{1}x+\hat{\beta}_{2}x^{2}$$
and we should note that this is still a linear model because $\hat{y}$ is a linear function of $x$ and $x^{2}$. As we have already seen, it is easy to fit the model. Adding the column of $x^{2}$ values to the design matrix does the trick.

The difficult part comes in the interpretation of the parameter values. No longer is $\hat{\beta}_{1}$ the increase in $y$ for every one unit increase in $x$. Instead the three parameters in my model interact in a complicated fashion. For example, the peak of the parabola is at $-\hat{\beta}_{1}/2\hat{\beta}_{2}$ and whether the parabola is cup shaped vs dome shaped and its steepness is controlled by $\hat{\beta}_{2}$. Because my geometric understanding of degree $q$ polynomials relies on have all factors of degree $q$ or lower, whenever I include a covariate raised to a power, I should include all the lower powers as well.

#### Log and Square Root of a predictor

Often the effect of a covariate is not linearly related to response, but rather some function of the covariate. For example the area of a circle is not linearly related to its radius, but it is linearly related to the radius squared.
$$Area=\pi r^{2}$$
Similar situations might arise in biological settings, such as the volume of conducting tissue being related to the square of the diameter. Or perhaps an animals metabolic requirements are related to some power of body length. In sociology, it is often seen that the utility of, say, $1000 drops off in a logarithmic fashion according to the person's income. To a graduate student, $1K is a big deal, but to a corporate CEO, $1K is just another weekend at the track. Making a log transformation on any monetary covariate, might account for the non-linear nature of “utility”.

Picking a good transformation for a covariate is quite difficult, but most fields of study have spent plenty of time thinking about these issues. When in doubt, look at scatter plots of the covariate vs the response and ask what transformation would make the data fall onto a line?

#### Examples of transformation of predictors

To illustrate how to add a transformation of a predictor to a linear model in R, we will consider the Galapagos data in `faraway`.

```{r, fig.height=4}
library(faraway)
data(gala)
# look at all the scatterplots
pairs(log(Species) ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala)
```


Looking at these graphs, I think I should transform `elevation`, `Adjacent`, and `Area` and perhaps a log transformation is a good idea. 

```{r}
pairs( log(Species) ~ log(Elevation) + log(Area) + 
                       Nearest + Scruz + log(Adjacent), data=gala)
```


Looking at these graphs, it is clear that `log(Elevation)` and `log(Area)` are highly correlated and we should probably have one or the other, but not both in the model.

```{r}
m.c <- lm(log(Species) ~  log(Area) + Nearest + Scruz + log(Adjacent), data=gala)
summary(m.c)$coefficients %>% round(digits=3) # more readable...
```


We will remove all the parameters that appear to be superfluous, and perorm an F-test to confirm that the simple model is sufficient. 

```{r}
m.s <- lm(log(Species) ~ log(Area), data=gala)
anova(m.s, m.c)
```


Next we will look at the coefficients.
```{r}
summary(m.s)
```

The slope coefficient (0.3886) is the increase in log(Species) for every 1 unit increase in log(Area). Unfortunately that is not particularly convenient to interpretation and we will address this in the next section of this chapter. 

Finally, we might be interested in creating a confidence interval for the expected number of tortoise species for an island with `Area=50`. 

```{r}
x0 <- data.frame(Area=50)
log.Species.CI <- predict(m.s, newdata=x0, interval='confidence')
log.Species.CI       # Log(Species) scale
exp(log.Species.CI)  # Species scale
```


Notice that on the species-scale, we see that the fitted value is not in the center of the confidence interval.

To help us understand what the log transformations are doing, we can produce a plot with the island Area on the x-axis and the expected number of Species on the y-axis and hopefully that will help us understand the relationship between the two.

```{r}
library(ggplot2)
pred.data <- data.frame(Area=1:50)
pred.data <- pred.data %>% cbind( predict(m.s, newdata=pred.data, interval='conf'))
ggplot(pred.data, aes(x=Area)) +
  geom_line(aes(y=exp(fit))) +
  geom_ribbon(aes(ymin=exp(lwr), ymax=exp(upr)), alpha=.2) +
  ylab('Number of Species')
```


### Interpretation of log transformed variables

One of the most difficult issues surrounding transformed variables is that the interpretation is difficult. Here we look at the interpretation of log transformed variables.

To begin with, we need to remind ourselves of what the functions $\log x$ and $e^{x}$ look like.

```{r, echo=FALSE, fig.height=3}
library(ggplot2)
x <- seq(-4,5, length=1000) 
data <- data.frame(x=x, y=exp(x), funct='exp(x)') 
x <- seq(0,5, length=1000)[-1] 
data <- rbind(data, data.frame(x=x, y=log(x), funct='log(x)')) 
ggplot(data, aes(x=x, y=y, color=funct)) +
  geom_line() + 
  coord_cartesian(ylim = c(-4, 4), xlim=c(-3,4)) + 
	labs(color="Function")
```

  
In particular we notice that 
$$e^{0}=1$$ 
and 
$$\log\left(1\right)=0$$ 
and the functions $e^{x}$ and $\log x$ are inverse functions of each other. 
  
$$e^{\log x}=\log\left(e^{x}\right)=x$$
 
Also it is important to note that the $\log$ function has some interesting properties in that it makes operations “1-operation easier”.
$$\begin{aligned} 
\log\left(a^{b}\right)	      &=	b\log a  \\
\log\left(\frac{a}{b}\right)	&=	\log a-\log b \\
\log\left(ab\right)	          &=	\log a+\log b
\end{aligned}$$
 
To investigate the effects of a log transformation, we'll examine a dataset that predicts the writing scores of $n=200$ students using the gender, reading and math scores. This example was taken from the UCLA Statistical Consulting Group. 

```{r}
scores <- read.table(
  file='https://stats.idre.ucla.edu/wp-content/uploads/2016/02/lgtrans.csv',
  header=TRUE, sep=',')
scores <- scores %>% mutate(gender = female)
pairs(write~read+math+gender, data=scores)
```


#### Log-transformed response, untransformed covariates

We consider the model where we have transformed the response variable and just an intercept term.
$$\log y=\beta_{0}+\epsilon$$

```{r}
m <- lm(log(write) ~ 1, data=scores)
summary(m)$coef %>% round(digits=3)
```


We interpret the intercept as the mean of the log-transformed response values. We could back transform this to the original scale $\hat{y} = e^{\hat{\beta}_{0}} = e^{3.948} = 51.83$ as a typical value of write. To distinguish this from the usually defined mean of the write values, we will call this as the *geometric mean*.

Next we examine how to interpret the model when a categorical variable is added to the model.
$$\log y=\begin{cases}
\beta_{0}+\epsilon & \;\;\textrm{if female}\\
\beta_{0}+\beta_{1}+\epsilon & \;\;\textrm{if male}
\end{cases}$$

```{r}
m <- lm(log(write) ~ gender, data=scores)
summary(m)$coef %>% round(digits=3)
```

The intercept is now the mean of the log-transformed `write` responses for the females and thus $e^{\hat{\beta}_0} = \hat{y}_{f}$ and the offset for males is the change in `log(write)` from the female group. Notice that for the males, we have
$$\begin{aligned}
\log\hat{y}_m	&=	\hat{\beta}_{0}+\hat{\beta}_{1} \\
    \hat{y}_m	&=	e^{\hat{\beta}_{0}+\hat{\beta}_{1}} \\
	          &=	\underset{\hat{y}_{f}}{\underbrace{e^{\hat{\beta}_{0}}}}\;\;\;\;\;\underset{\textrm{percent change for males}}{*\;\;\underbrace{e^{\hat{\beta}_{1}}}} 
\end{aligned}$$

and therefore we see that males tend to have writing scores $e^{-0.103}=0.90=90\%$ of the females. Typically this sort of result would be reported as the males have a 10% lower writing score than the females.

The model with a continuous covariate has a similar interpretation.
$$\log y=\begin{cases}
\beta_{0}+\beta_{2}x+\epsilon & \;\;\textrm{if female}\\
\beta_{0}+\beta_{1}+\beta_{2}x+\epsilon & \;\;\textrm{if male}
\end{cases}$$

We will use the reading score read to predict the writing score. Then $\hat{\beta}_{2}$ is the predicted increase in `log(write)` for every 1-unit increase in read score. The interpretation of $\hat{\beta}_{0}$ is now $\log\hat{y}$ when $x=0$ and therefore $\hat{y}=e^{\hat{\beta}_{0}}$ when $x=0$.

```{r}
m <- lm(log(write) ~ gender + read, data=scores)  # main effects model
summary(m)$coefficients %>% round(digits=3)
```

For females, we consider the difference in $\log\hat{y}$ for a 1-unit increase in $x$ and will interpret this on the original write scale.
$$\begin{aligned}
\log\hat{y}_f	&=	\hat{\beta}_{0}+\hat{\beta}_{2}x \\
\hat{y}_f	    &=	e^{\hat{\beta}_{0}+\hat{\beta}_{2}x}
\end{aligned}$$
therefore we consider $e^{\hat{\beta}_{2}}$ as the *percent* increase in write score for a 1-unit increase in $x$ because of the following. Consider $x_1$ and $x_2 = x_1 +1$. Then we consider the ratio of predicted values:
$$
\frac{\hat{y}_2}{\hat{y}_1} 
  = \frac{e^{\hat{\beta}_{0}+\hat{\beta}_{2}\,\left(x+1\right)}}{e^{\hat{\beta}_{0}+\hat{\beta}_{2}\,x}} 
  = \frac{e^{\hat{\beta}_{0}}e^{\hat{\beta}_{2}\,x}e^{\hat{\beta}_{2}}}{e^{\hat{\beta}_{0}}e^{\hat{\beta}_{2}\,x}} 
  = e^{\hat{\beta}_{2}}$$


For our writing scores example we have that $e^{\hat{\beta}_{2}}=e^{0.011}=1.01$
meaning there is an estimated $1\%$ increase in `write` score for every 1-point increase in `read` score. 

If we are interested in, say, a 20-unit increase in $x$, then that would result in an increase of 

$$\frac{e^{\hat{\beta}_{0} + \hat{\beta}_{2} \, \left(x+20\right)}} {e^{\hat{\beta}_{0}+\hat{\beta}_{2} \, x}}
 =\frac{e^{\hat{\beta}_{0}} e^{\hat{\beta}_{2}\,x} e^{20\hat{\beta}_{2}}}{e^{\hat{\beta}_{0}} e^{\hat{\beta}_{2} \, x}}
 = e^{20\hat{\beta}_{2}} = \left( e^{\hat{\beta}_{2}} \right)^{20}$$
 
and for the writing scores we have $$e^{20\hat{\beta}_{2}} = \left( e^{\hat{\beta}_{2}} \right)^{20}=1.01^{20} = 1.22$$ or a 22% increase in writing score for a 20-point increase in reading score.

In short, we can interpret $e^{\hat{\beta}_{i}}$ as the percent increase/decrease in the non-transformed response variable.
Some students get confused by what is meant by a $\%$ increase or decrease in $x$.

  * A $75\%$ decrease in $x$ has a resulting value of $\left(1-0.75\right)x=\left(0.25\right) x$
  * A $75\%$ increase in $x$ has a resulting value of $\left(1+0.75\right)x=\left(1.75\right) x$
  * A $100\%$ increase in $x$ has a resulting value of $\left(1+1.00\right)x=2x$ and is a doubling of $x$.
  * A $50\%$ decrease in $x$ has a resulting value of $\left(1-0.5\right)x=\left(0.5\right) x$ and is a halving of $x$.


#### Untransformed response, log-transformed covariate

We consider the model
$$y=\beta_{0}+\beta_{2}\log x+\epsilon$$
and consider two different values of $x$ (which we'll call $x_{1}$ and $x_{2}$ and we are considering the effect of moving from $x_{1}$ to $x_{2}$) and look at the differences between the predicted values $\hat{y}_2 - \hat{y}_1$.

$$\begin{aligned}
\hat{y}_{2}-\hat{y}_{1}	
  & =	\left[\hat{\beta}_{0}+\hat{\beta}_{2}\log x_{2}\right]-\left[\hat{\beta}_{0}+\hat{\beta}_{2}\log x_{1}\right] \\
	& =	\hat{\beta}_{2}\left[\log x_{2}-\log x_{1}\right] \\
	& =	\hat{\beta}_{2}\log\left[\frac{x_{2}}{x_{1}}\right]
	\end{aligned}$$
	
This means that so long as the ratio between the two x-values is constant, then the change in $\hat{y}$ is the same. So doubling the value of $x$ from 1 to 2 has the same effect on $\hat{y}$ as changing x from 50 to 100. 

```{r}
m <- lm( write ~ gender + log(read), data=scores)
summary(m)$coefficients %>% round(digits=3)
```

```{r}
# predict writing scores for three females, 
# each with a reading score 50% larger than the other previous
predict(m, newdata=data.frame(gender=rep('female',3),
                              read=c(40, 60, 90)))
```

We should see a 
$$29.045 \; \log \left( 1.5 \right) = 11.78$$  
difference in $\hat{y}$ values for the first and second students and the second and third.

#### Log-transformed response, log-transformed covariate

This combines the interpretation of the in the previous two section. We consider
$$\log y=\beta_{0}+\beta_{2}\log x+\epsilon$$ 
and we again consider two $x$ values (again $x_{1}$ and $x_{2}$). We then examine the difference in the $\log\hat{y}$ values as 
$$\begin{aligned}
\log\hat{y}_{2}-\log\hat{y}_{1}	&= \left[\hat{\beta}_{0}+\hat{\beta}_{2}\log x_{2}\right]-\left[\hat{\beta}_{0}+\hat{\beta}_{2}\log x_{1}\right] \\
\log\left[\frac{\hat{y}_{2}}{\hat{y}_{1}}\right]	&=	\hat{\beta}_{2}\log\left[\frac{x_{2}}{x_{1}}\right] \\
\log\left[\frac{\hat{y}_{2}}{\hat{y}_{1}}\right]	&=	\log\left[\left(\frac{x_{2}}{x_{1}}\right)^{\hat{\beta}_{2}}\right] \\
\frac{\hat{y}_{2}}{\hat{y}_{1}}	&=	\left(\frac{x_{2}}{x_{1}}\right)^{\hat{\beta}_{2}}
\end{aligned}$$
 
This allows us to examine the effect of some arbitrary percentage increase in $x$ value as a percentage increase in $y$ value. 

```{r}
m <- lm(log(write) ~ gender + log(read), data=scores)
summary(m)$coefficients %>% round(digits=3)
```

which implies for a $10$% increase in `read` score, we should see a $1.10^{0.581}=1.05$ multiplier in `write` score. That is to say, a $10\%$ increase in reading score results in a $5\%$ increase in writing score. 

For the Galapagos islands, we had
```{r}
m.s <- lm(log(Species) ~ log(Area), data=gala)
summary(m.s)$coefficients %>% round(digits=3)
```

and therefore doubling of Area (i.e. the ratio of the $Area_{2} / Area_{1} = 2$) results in a $2^{0.389}=1.31$ multiplier of the `Species` value. That is to say doubling the island area increases the number of species by $31\%$.

## Exercises

1. The dataset `infmort` in package faraway has information about infant mortality from countries around the world. Be aware that this is a old data set and does not necessarily reflect current conditions. More information about the dataset can be found using `help(infmort)`. We will be interested in understanding how infant mortality is predicted by per capita income, world region, and oil export status.

    a. Plot the relationship between income and mortality. This can be done using the command
        ```{r, eval=FALSE}
        library(faraway)
        pairs(mortality ~., data=infmort)
        ```
        What do you notice about the relationship between mortality and income?
    
    b. Fit a linear model without any interaction terms with all three covariates as predictors of infant mortality. Examine the diagnostic plots. What stands out?
    
    c. Use the `boxcox()` function in the library MASS to determine a what a good transformation to the mortality response variable.
    
    d. Make a log transformation to the mortality variable and refit the model without interactions. Use the log transformed mortality for all further questions.
    
    e. Examine the pairs plot with log(mortality), income, and log(income). Which should be used in our model, `income` or `log(income)`? 
    
    f. Examine models that have a `region:log(income)` interaction and `oil:log(income)` interaction along with the main effect of the third variable. Are either interaction significant vs the model without interactions? What about a model that contains both interactions?
    
    g. Interpret the effects of income, world region, and oil exports on log infant mortality based on these data. *Hint: graph the data and the predicted values.*

2. Using the `pressure` data available in the `faraway` package, fit a model with pressure as the response and temperature as the predictor using transformations to obtain a good fit. Feel free to experiment with what might be considered a ridiculously complicated model. 

    a. Document your process of building your final model. Do not show graphs or computer output that is not relevant to your decision or that you do not wish to comment on. 
    
    b. Comment on the interpretability of your (possibly ridiculously complicated) model.

3. Use transformations to find a good model for `volume` in terms of `girth` and `height` using the `trees` dataset in the `faraway` package.

    a. Document your process of building your final model. Again, only include output or graphs that are relevant to your decisions and include discussion about anything you include.

    b. Create a prediction interval for the volume of a tree with girth=16 and height=70. Notice that if you have transformed your response variable in you model, you'll have to back-transform to the original y-scale.

4. For this problem, we will look at a manufacturing problem. We will investigate the relationship predicting the time taken polishing a newly manufactured dish versus the dish diameter (in inches), type, and price.

    a. The data live in a package I have on GitHub. The following code will download the data package:
        ```{r, warning=FALSE, message=FALSE, eval=FALSE}
        library(devtools)   # You might need to load this package the usual way...
        install_github('dereksonderegger/dsData') # load an R package that lives on GitHub.
        ```

    b. Load the data and examine it using the commands
        ```{r, eval=FALSE}
        library(dsData)
        str(Dishes)
        pairs(Time ~ ., data=Dishes)
        ```
      Comment on the relationships and possible transformations to be made.

    c. Fit a linear model predicting Time as a function the main of Diameter, Price, and Type, but with no interaction.

    d. Examine the diagnostic plots. What stands out to you?

    e. While most of the diagnostics look fine, there is weak evidence that there might be some heteroskedasticity. To address this (and provide an interesting model to interpret), refit your linear model but with a log-transformed Time and Price variables.

    f. What is your interpretation of the parameter associated with the Diameter variable on the original scale of the Time variable? Does this make sense to you considering the `pairs()` plot?  

    g. How does a 20% increase in Price affect the polishing time? 



<!--chapter:end:06_Diagnostics.Rmd-->

# Variable Selection

```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

Given a set of data, we are interested in selecting the best subset of predictors for the following reasons:

1. Occam's Razor tells us that from a list of plausible model or explanations, the simplest is usually the best. In the statistics sense, I want the smallest model that adequately explains the observed data patterns.

2. Unnecessary predictors add noise to the estimates of other quantities and will waste degrees of freedom, possibly increasing the estimate of $\hat{\sigma}^{2}$.

3. We might have variables that are collinear.

The problems that arise in the diagnostics of a model will often lead a researcher to consider other models, for example to include a quadratic term to account for curvature. The model building process is often an iterative procedure where we build a model, examine the diagnostic plots and consider what could be added or modified to correct issues observed.

## Nested Models 

Often one model is just a simplification of another and can be obtained by setting some subset of $\beta_{i}$ values equal to zero. Those models can be adequately compared by the F-test, which we have already made great use of.

We should be careful to note that we typically do not want to remove the main covariate from the model if the model uses the covariate in a more complicated fashion. For example, if my model is $$y=\beta_{0}+\beta_{1}x+\beta_{2}x^{2}+\epsilon$$
where $\epsilon\sim N\left(0,\sigma^{2}\right)$, then considering the simplification $\beta_{1}=0$ and removing the effect of $x$ is not desirable because that forces the parabola to be symmetric about $x=0$. Similarly, if the model contains an interaction effect, then the removal of the main effect drastically alters the interpretation of the interaction coefficients and should be avoided. Often times removing a lower complexity term while keeping a higher complexity term results in unintended consequences and is typically not recommended.

## Testing-Based Model Selection

Starting with a model that is likely too complex, consider a list of possible terms to remove and remove each in turn while evaluating the resulting model to the starting model using an F-test. Whichever term has the highest p-value is removed and the process is repeated until no more terms have non-significant p-values. This is often referred to as *backward selection*.

It should be noted that the cutoff value for significance here does not have to be $\alpha=0.05$. If prediction performance is the primary goal, then a more liberal $\alpha$ level is appropriate.


Starting with a model that is likely too small, consider adding terms until there are no more terms that when added to the model are significant. This is called *forward selection*.

This is a hybrid between forward selection and backward elimination. At every stage, a term is either added or removed. This is referred to as *stepwise selection*.

Stepwise, forward, and backward selection are commonly used but there are some issues.

1. Because of the “one-at-a-time” nature of the addition/deletion, the most optimal model might not be found.

2. p-values should not be treated literally. Because the multiple comparisons issue is completely ignored, the p-values are lower than they should be if multiple comparisons were accounted for. As such, it is possible to sort through a huge number of potential covariates and find one with a low p-value simply by random chance. This is “data dredging” and is a serious issue.

3. As a non-thinking algorithm, these methods ignore the science behind that data and might include two variables that are highly collinear or might ignore variables that are scientifically interesting.

### Example - U.S. Life Expectancy

Using data from the Census Bureau we can look at the life expectancy as a response to a number of predictors. One R function that is often convenient to use is the update() function that takes a `lm()` object and adds or removes things from the formula. The notation `. ~ .` means to leave the response and all the predictors alone, while `. ~ . + vnew` will add the main effect of vnew to the model.

```{r, message=FALSE, warning=FALSE}
library(faraway)
library(dplyr)   # for the %>% operator!

# Convert from a matrix to a data frame with state abbreviations
state.data <- data.frame(state.x77, row.names=state.abb)
str(state.data)
```


We should first look at the 
```{r}
pairs(Life.Exp ~ ., data=state.data)
```


I want to add a quadratic effect for HS.Grad rate and for Income. We'll add it to the data frame and then perform the backward elimination method starting with the model with all predictors as main effects.

```{r}
state.data$HS.Grad.2 <- state.data$HS.Grad ^ 2
state.data$Income.2  <- state.data$Income  ^ 2
# explicitly define my starting model
m1 <- lm(Life.Exp ~ Population + Income + Illiteracy + 
                	Murder + HS.Grad + Frost + HS.Grad.2 + Income.2, data=state.data)
#
# Define the same model, but using shorthand
# The '.' means everything else in the data frame
m1 <- lm( Life.Exp ~ ., data=state.data)
summary(m1)$coefficients %>% round(digits=3)
```


The signs make reasonable sense (higher murder rates decrease life expectancy) but covariates like `Income` are not significant, which is surprising. We will first remove the term with the highest p-value, which is the state's `HS.Grad`. However, I don't want to remove the lower-order graduation term and keep the squared-term. So instead I will remove both of them since they are the highest p-values. Notice that `HS.Grad` is correlated with `Income` and `Illiteracy`.

```{r}
# Remove Graduation Rate from the model from the model
m1 <- update(m1, .~. - HS.Grad - HS.Grad.2)
summary(m1)$coefficients %>% round(digits=3)
```

```{r}
# Next remove Illiteracy
m1 <- update(m1, .~. - Illiteracy)
summary(m1)$coefficients %>% round(digits=3)
```

```{r}
# And Population...
m1 <- update(m1, .~. - Population)
summary(m1)$coefficients %>% round(digits=3)
```

```{r}
# Remove Area from the model
m1 <- update(m1, .~. - Area)
summary(m1)$coefficients %>% round(digits=3)
```

The removal of `Income.2` is a tough decision because the p-value is very close to $\alpha=0.05$ and might be left in if it makes model interpretation easier or if the researcher feels a quadratic effect in income is appropriate (perhaps rich people are too stressed?).

```{r}
summary(m1)
```

We are left with a model that adequately explains `Life.Exp` but we should be careful to note that just because a covariate was removed from the model does not imply that it isn't related to the response. For example, being a high school graduate is highly correlated with not being illiterate as is `Income` and thus replacing `Illiteracy` shows that illiteracy is associated with lower life expectancy, but it is not as predictive as `Income`. 

```{r}
m2 <- lm(Life.Exp ~ Illiteracy+Murder+Frost, data=state.data)
summary(m2)
```

Notice that the $R^{2}$ values for both models are quite similar $0.7042$ vs $0.6739$ but the first model with the higher $R^{2}$ has one more predictor variable? Which model should I prefer? I can't do an F-test because these models are not nested.

## Criterion Based Procedures

### Information Criterions

It is often necessary to compare models that are not nested. For example, I might want to compare 
$$y=\beta_{0}+\beta_{1}x+\epsilon$$
vs
$$y=\beta_{0}+\beta_{2}w+\epsilon$$

This comparison comes about naturally when doing forward model selection and we are looking for the “best” covariate to add to the model first.

Akaike introduced his criterion (which he called “An Information Criterion”) as
$$AIC=\underset{\textrm{decreases if RSS decreases}}{\underbrace{-2\,\log L\left(\hat{\boldsymbol{\beta}},\hat{\sigma}|\,\textrm{data}\,\right)}}+\underset{\textrm{increases as p increases}}{\underbrace{2p}}$$
where 
$L\left(\hat{\boldsymbol{\beta}}|\,\textrm{data}\,\right)$ is the likelihood function and $p$ is the number of elements in the $\hat{\boldsymbol{\beta}}$
vector and we regard a lower AIC value as better. Notice the $2p$
term is essentially a penalty on adding addition covariates so to lower the AIC value, a new predictor must lower the negative log likelihood more than it increases the penalty.

To convince ourselves that the first summand decreases with decreasing RSS in the standard linear model, we examine the likelihood function
$$\begin{aligned}
f\left(\boldsymbol{y}\,|\,\boldsymbol{\beta},\sigma,\boldsymbol{X}\right)	&=	\frac{1}{\left(2\pi\sigma^{2}\right)^{n/2}}\exp\left[-\frac{1}{2\sigma^{2}}\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)^{T}\left(\boldsymbol{y}-\boldsymbol{X}\boldsymbol{\beta}\right)\right] \\
	&=	L\left(\boldsymbol{\beta},\sigma\,|\,\boldsymbol{y},\boldsymbol{X}\right)
\end{aligned}$$
and we could re-write this as
$$\begin{aligned}
\log L\left(\hat{\boldsymbol{\beta}},\hat{\sigma}\,|\,\textrm{data}\right)	&=	-\log\left(\left(2\pi\hat{\sigma}^{2}\right)^{n/2}\right)-\frac{1}{2\hat{\sigma}^{2}}\left(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}}\right)^{T}\left(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}}\right) \\
	&=	-\frac{n}{2}\log\left(2\pi\hat{\sigma}^{2}\right)-\frac{1}{2\hat{\sigma}^{2}}\left(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}}\right)^{T}\left(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}}\right) \\
	&=	-\frac{1}{2}\left[n\log\left(2\pi\hat{\sigma}^{2}\right)+\frac{1}{\hat{\sigma}^{2}}\left(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}}\right)^{T}\left(\boldsymbol{y}-\boldsymbol{X}\hat{\boldsymbol{\beta}}\right)\right] \\
	&=	-\frac{1}{2}\left[+n\log\left(2\pi\right)+n\log\hat{\sigma}^{2}+\frac{1}{\hat{\sigma}^{2}}RSS\right]
\end{aligned}$$
 

It isn't clear what we should do with the $n\log\left(2\pi\right)$ term in the $\log L()$ function. There are some compelling reasons to ignore it and just use the second, and there are reasons to use both terms. Unfortunately, statisticians have not settled on one convention or the other and different software packages might therefore report different values for AIC. 

As a general rule of thumb, if the difference in AIC values is less than two then the models are not significantly different, differences between 2 and 4 AIC units are marginally significant and any difference greater than 4 AIC units is highly significant.

Notice that while this allows us to compare models that are not nested, it does require that the same data are used to fit both models. Because I could start out with my data frame including both $x$ and $x^{2}$, (or more generally $x$ and $f\left(x\right)$ for some function $f()$) you can regard a transformation of a covariate as “the same data”. However, a transformation of a y-variable is not and therefore we cannot use AIC to compare a models `log(y) ~ x` versus the model `y ~ x`.

Another criterion that might be used is *Bayes Information Criterion* (BIC) which is

$$BIC=-2\,\log L\left(\hat{\boldsymbol{\beta}},\hat{\sigma}|\,\textrm{data}\,\right)+p\log n$$

and this criterion punishes large models more than AIC does (because $\log n>2$ for $n\ge8$)

The AIC value of a linear model can be found using the AIC() on a lm() object.

```{r}
AIC(m1)
AIC(m2)
```

Because the AIC value for the first model is lower, we would prefer the first model that includes both `Income` and `Income.2` compared to model 2, which was `Life.Exp ~ Illiteracy+Murder+Frost`.

### Adjusted `R-sq`
 

One of the problems with $R^{2}$ is that it makes no adjustment for how many parameters in the model. Recall that $R^{2}$ was defined as 
$$R^{2}=\frac{RSS_{S}-RSS_{C}}{RSS_{S}}=1-\frac{RSS_{C}}{RSS_{S}}$$
where the simple model was the intercept only model. We can create an $R_{adj}^{2}$ statistic that attempts to add a penalty for having too many parameters by defining
$$R_{adj}^{2}=1-\frac{RSS_{C}/\left(n-p\right)}{RSS_{S}/\left(n-1\right)}$$
With this adjusted definition, adding a variable to the model that has no predictive power will decrease $R_{adj}^{2}$.

### Example
Returning to the life expectancy data, we could start with a simple model add covariates to the model that have the lowest AIC values. R makes this easy with the function add1() which will take a linear model (which includes the data frame that originally defined it) and will sequentially add all of the possible terms that are not currently in the model and report the AIC values for each model.

```{r}
# Define the biggest model I wish to consider
biggest <- Life.Exp ~ Population + Income + Illiteracy + Murder + 
                      HS.Grad + Frost + Area + HS.Grad.2 + Income.2

# Define the model I wish to start with
m <- lm(Life.Exp ~ 1, data=state.data)

add1(m, scope=biggest)  # what is the best addition to make?
```

Clearly the additiona of `Murder` to the model results in the lowest AIC value, so we will add `Murder` to the model. Notice the `<none>` row corresponds to the model m which we started with and it has a `RSS=88.299`. For each model considered, R will calculate the `RSS_{C}` for the new model and will calculate the difference between the starting model and the more complicated model and display this in the Sum of Squares column.

```{r}
m <- update(m, . ~ . + Murder)  # add murder to the model
add1(m, scope=biggest)          # what should I add next?
```


There is a companion function to `add1()` that finds the best term to drop. It is conveniently named `drop1()` but here the `scope` parameter defines the smallest model to be considered.

It would be nice if all of this work was automated. Again, R makes our life easy and the function `step()` does exactly this. The set of models searched is determined by the scope argument which can be a *list* of two formulas with components upper and lower or it can be a single formula, or it can be blank. The right-hand-side of its lower component defines the smallest model to be considered and the right-hand-side of the upper component defines the largest model to be considered. If `scope` is a single formula, it specifies the upper component, and the lower model taken to be the intercept-only model. If scope is missing, the initial model is used as the upper model.

```{r}
smallest <- Life.Exp ~ 1
biggest <- Life.Exp ~ Population + Income + Illiteracy + 
                      Murder + HS.Grad + Frost + Area + HS.Grad.2 + Income.2
m <- lm(Life.Exp ~ Income, data=state.data)
step(m, scope=list(lower=smallest, upper=biggest))
```

Notice that our model selected by `step()` is not the same model we obtained when we started with the biggest model and removed things based on p-values. 

The log-likelihood is only defined up to an additive constant, and there are different conventional constants used. This is more annoying than anything because all we care about for model selection is the difference between AIC values of two models and the additive constant cancels. The only time it matters is when you have two different ways of extracting the AIC values. Recall the model we fit using the top-down approach was

```{r}
# m1 was
m1 <- lm(Life.Exp ~ Income + Murder + Frost + Income.2, data = state.data)
AIC(m1)
```

and the model selected by the stepwise algorithm was

```{r}
m3 <- lm(Life.Exp ~ Murder + Frost + HS.Grad + Population, data = state.data)
AIC(m3)
```

Because `step()` and `AIC()` are following different conventions the absolute value of the AICs are different, but the difference between the two is constant no matter which function we use.

First we calculate the difference using the AIC() function:

```{r}
AIC(m1) - AIC(m3)
```

and next we use `add1()` on both models to see what the AIC values for each.

```{r}
add1(m1, scope=biggest)
add1(m3, scope=biggest)
```


Using these results, we can calculate the difference in AIC values to be the same as we calculated before $$\begin{aligned}
-22.465--28.161	&=	-22.465+28.161 \\
	&=	5.696
	\end{aligned}$$
 
## Exercises
1. Consider the `prostate` data from the `faraway` package. The variable `lpsa` is a measurement of a prostate specific antigen which higher levels are indicative of prostate cancer. Use `lpsa` as the response and all the other variables as predictors (no interactions). Determine the “best” model using:

    a. Backward elimination using the analysis of variance F-statistic as the criteria.

    b. Forward selection using AIC as the criteria.

2. Again from the `faraway` package, use the `divusa` which has diviorce rates for each year from 1920-1996 along with other population information for each year. Use `divorce` as the response variable and all other variables as the predictors.

    a. Determine the best model using stepwise selection starting from the intercept only model and the most complex model being all main effects (no interactions). Use the F-statistic to determine significance. Note: add1(), drop1(), and step() allow an option of test='F' to use an F-test instead of AIC.

    b. Following the stepwise selection, comment on the relationship between p-values used and the AIC difference observed. Do the AIC rules of thumb match the p-value interpretation?

<!--chapter:end:07_Variable_Selection.Rmd-->

# One way ANOVA
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

```{r, warning=FALSE, message=FALSE}
# Load the libraries I'll use
library(faraway)
library(ggplot2)
library(dplyr)
library(multcompView)
```


Given a categorical covariate (which I will call a factor) with $I$ levels, we are interested in fitting the model
$$y_{ij}=\mu+\tau_{i}+\epsilon_{ij}$$
where $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$, $\mu$ is the overall mean, and $\tau_{i}$ are the offset of factor level $i$ from $\mu$. Unfortunately this model is not identifiable because I could add a constant (say $5$) to $\mu$ and subtract that same constant from each of the $\tau_{i}$ values and the group mean $\mu+\tau_{i}$ would not change. There are two easy restrictions we could make to make the model identifiable:

1. Set $\mu=0$. In this case, $\tau_{i}$ represents the expected value of an observation in group level $i$. We call this the “cell means” representation.

2. Set $\tau_{1}=0$. Then $\mu$ represents the expected value of treatment $1$, and the $\tau_{i}$ values will represent the offsets from group 1. The group or level that we set to be zero is then referred to as the reference group. We can call this the “offset from reference” model.

We will be interested in testing the null and alternative hypotheses
$$\begin{aligned}
H_{0}:\;\;y_{ij}	&=	\mu+\epsilon_{ij}              \\
H_{a}:\;\;y_{ij}	=	\mu+\alpha_{i}+\epsilon_{ij}
\end{aligned}$$
 

## An Example

We look at a dataset that comes from the study of blood coagulation times: 24 animals were randomly assigned to four different diets and the samples were taken in a random order. The diets are denoted as $A$,$B$,$C$,and $D$ and the response of interest is the amount of time it takes for the blood to coagulate. 

```{r, warning=FALSE, message=FALSE, fig.height=3}
data(coagulation)
ggplot(coagulation, aes(x=diet, y=coag)) + 
	geom_boxplot() +
	labs( x='Diet', y='Coagulation Time' )
```


Just by looking at the graph, we expect to see that diets $A$ and $D$ are similar while $B$ and $C$ are different from $A$ and $D$ and possibly from each other, too. We first fit the offset model.

```{r}
m <- lm(coag ~ diet, data=coagulation)
summary(m)
```

Notice that diet $A$ is the reference level and it has a mean of $61$. Diet $B$ has an offset from $A$ of $5$, etc. From the very small F-statistic, we conclude that simple model 
$$y_{ij}=\mu+\epsilon_{ij}$$
is not sufficient to describe the data.

## Degrees of Freedom

Throughout the previous example, the degrees of freedom that are reported keeps changed depending on what models we are comparing. The simple model we are considering is 
$$y_{ij}\sim\mu+\epsilon_{ij}$$
which has 1 parameter that defines the expected value versus
$$y_{ij}\sim\mu+\tau_{i}+\epsilon_{ij}$$
where there really are only $4$ parameters that define the expected value because $\tau_{1}=0$. In general, the larger model is only adding $I-1$ terms to the model where $I$ is the number of levels of the factor of interest.

## Diagnostics

It is still important to check the diagnostics plots, but certain diagnostic plots will be useless. In particular, we need to be concerned about constant variance among the groups and normality of the residuals.

```{r, fig.height=3}
library(ggfortify)
m <- lm(coag ~ diet, data=coagulation)
autoplot(m, which=2)  #  QQ plot
```


The residual plots however, might need a little bit of extra work, because there are only four possible predicted values (actually $3$ because group $A$ and $D$ have the same predicted values). Note that we actually have $n=24$ observations, but I can only see 16 of them.

```{r, fig.height=3}
plot(m, which=1)  #  Residals vs fitted
```


To remedy this, we will plot the residuals vs fitted by hand, and add a little bit of random noise to the fitted value, just so that we don't have points stack up on top of each other. Lets also add a different shape for each diet.

```{r}
coagulation$fitted <- predict(m)
coagulation$resid <- resid(m)
ggplot(coagulation, aes(x=fitted, y=resid, shape=diet, color=diet)) +
  geom_point(position=position_jitter(w=0.3, h=0))
```


## Pairwise Comparisons

After detecting differences in the factor levels, we are often interested in which factor levels are different from which. Often we are interested in comparing the mean of level $i$ with the mean of level $j$. As usual we let the vector of parameter estimates be $\hat{\boldsymbol{\beta}}$ then the contrast of interested can be written as
$$\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\pm t_{n-p}^{1-\alpha/2}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}$$ for some vector $\boldsymbol{c}$.

Unfortunately this interval does not take into account the multiple comparisons issue (i.e. we are making $I(I-1)/2$ contrasts if our factor has $I$ levels). To account for this, we will not use a quantile from a t-distribution, but from Tukey's studentized range distribution $q_{n,n-I}$ divided by $\sqrt{2}$. The intervals we will use are:
$$\boldsymbol{c}^{T}\hat{\boldsymbol{\beta}}\pm\frac{q_{n,n-I}^{1-\alpha/2}}{\sqrt{2}}\;\hat{\sigma}\sqrt{\boldsymbol{c}^{T}\left(\boldsymbol{X}^{T}\boldsymbol{X}\right)^{-1}\boldsymbol{c}}$$

There are several ways to make R calculate this interval (See the contrasts chapter for a more general treatment of this.), but the easiest is to use `TukeyHSD()`. This function will calculate all of these pairwise intervals using the above formula, which is commonly known as Tukey's Honestly Significant Differences. This function expects to receive output from the aov() function. The aov() is similar to lm() but does not accept continuous covariates in the model. Because it is possible to convert a `lm` output object to an `aov` object, I typically will do the following

```{r}
m <- lm(coag ~ diet, data=coagulation)   # use the lm() function as usual
Pvalues <- TukeyHSD( aov(m), level=.90 ) # convert to the format that TukeyHSD likes...
Pvalues
```

Here we see that diets $A$ and $D$ are similar to each other, but different than $B$ and $C$ and that $B$ and $C$ are not statistically different from each other at the $0.10$ level.

### Presentation of Results
The display of `TukeyHSD` is pretty annoying and often I want to turn the vector of adjusted p-values into a matrix of p-values. We want a matrix that is $p \times p$.

```{r}
# Convert to a matrix of adjusted p-values
Pmatrix <- vec2mat( Pvalues$diet[, 'p adj'] )
Pmatrix
```

From this matrix of adjusted p-values, there are many functions that will be helpful. One very common thing is to use a lettering scheme that indicates which groups are different. So if two groups share a letter, they are not significantly different.

```{r}
multcompLetters(Pmatrix)
```

To make our lives easier while making graphs, we *really* want to be able to add these back into our original data frame.  Unfortunately the `multcompView` package that does this doesn't really make it easy to do this.  So I wrote us a little function that will suffice:

```{r}
#' Create a data frame with significant groupings 
#'
#' This function runs TukeyHSD on the input model and then creates a data frame
#' with a column for the factor and a second for the Significance Group
#' 
#' @param model The output of a lm() or aov() call that can be coerced to an aov object.
#' @param variable The variable of interest.
#' @output A data frame with a column for factor and another for the signicance group.
make_TukeyHSD_letters <- function(model, variable){ 
  Tukey <- TukeyHSD(aov(model))[[variable]]
  temp <- Tukey[,'p adj'] %>%
    vec2mat() %>%
    multcompLetters()
  out <- data.frame(group = names(temp$Letters), SigGroup=temp$Letters)
  colnames(out)[1] <- variable
  out
}  
```

Now that the function is defined, we can happily use it.
```{r}
make_TukeyHSD_letters(m, 'diet') 
```

This is useful, but it will be more useful when I merge this small data frame
with the original data and then each observation will have an associated significance group.

```{r}
coagulation <- coagulation %>%
  left_join( make_TukeyHSD_letters(m, 'diet') )

ggplot(coagulation, aes(x=diet, y=coag, color=SigGroup)) +
  geom_boxplot() +
  geom_text( aes(label=SigGroup), y=71.5)
```



## Exercises
1. Use the dataset `chickwts` in the `faraway` package. This was an experiment to determine which feed types result in the largest chickens.  A set of 71 chicks were all randomly assigned one of six feed types and their weight in grams after six weeks was recorded.  Determine whether there are differences in the weights of chickens according to their feed. Perform all necessary model diagnostics and examine the contrasts between each pair of feed levels. Summarize these results. 

<!--chapter:end:08_OneWayAnova.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Two-way ANOVA
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

```{r, warning=FALSE, message=FALSE}
# Load my usual packages
library(MASS)     # for the boxcox function
library(faraway)
library(ggplot2)
library(dplyr)
library(ggfortify)
library(multcompView)
```


Given a response that is predicted by two different categorical variables. Suppose we denote the levels of the first factor as $\alpha_{i}$ and has $I$ levels. The second factor has levels $\beta_{j}$ and has $J$ levels. As usual we let $\epsilon_{ijk}\stackrel{iid}{\sim}N\left(0,\sigma^{2}\right)$, and we wish to fit the model

$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\epsilon_{ijk}$$

which has the main effects of each covariate or possibly the model with the interaction 
$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\left(\alpha\beta\right)_{ij}+\epsilon_{ijk}$$
 

To consider what an interaction term might mean consider the role of temperature and humidity on the amount of fungal growth. You might expect to see data similar to this (where the numbers represent some sort of measure of fungal growth):

+----------------+---------+-----+-----+-----+-----+ 
|                |         |  5% | 30% | 60% | 90% | 
+================+=========+=====+=====+=====+=====+ 
|                | **2C**  |  2  |  4  |  8  | 16  | 
+----------------+---------+-----+-----+-----+-----+  
|  Temperature   | **10C** |  3  |  9  |  27 |  81 | 
+----------------+---------+-----+-----+-----+-----+ 
|                | **30C** |  4  |  16 |  64 | 256 | 
+----------------+---------+-----+-----+-----+-----+ 


In this case we see that increased humidity increases the amount of fungal growth, but the amount of increase depends on the temperature. At 2 C, the increase is humidity increases are significant, but at 10 C the increases are larger, and at 30 C the increases are larger yet. The effect of changing from one humidity level to the next *depends on which temperature level we are at*. This change in effect of humidity is an interaction effect. A memorable example is that chocolate by itself is good. Strawberries by themselves are also good. But the combination of chocolate and strawberries is a delight greater than the sum of the individual treats.  

We can look at a graph of the Humidity and Temperature vs the Response and see the effect of increasing humidity changes based on the temperature level. Just as in the ANCOVA model, the interaction manifested itself in non-parallel slopes, the interaction manifests itself in non-parallel slopes when I connect the dots across the factor levels.

```{r, echo=FALSE}
Temp <- factor(1:3, labels=c('2C', '10C', '30C'))
Humidity <- factor(1:4, labels=c('5%','30%', '60%', '90%') ) 
X <- expand.grid(Temperature=Temp, Humidity=Humidity) 
Growth <- c(2,3,4, 4,9,16, 8,27,64, 16,81,256) 
my.data <- data.frame( cbind(X, Growth=Growth) )
ggplot(my.data, aes(x=Humidity, color=Temperature, shape=Temperature, y=Growth)) + 
	geom_point(size=5) + 
	geom_line(aes(x=as.integer(Humidity))) + 
	scale_color_manual(values=c('Blue','Black', 'Red')) 
```

Unfortunately the presence of a significant interaction term in the model makes interpretation difficult, but examining the interaction plots can be quite helpful in understanding the effects. Notice in this example, we 3 levels of temperature and 4 levels of humidity for a total of 12 different possible treatment combinations. In general I will refer to these combinations as cells.

## Orthogonality

When designing an experiment, I want to make sure than none of my covariates are confounded with each other and I'd also like for them to not be correlated. Consider the following three experimental designs, where the number in each bin is the number of subjects of that type. I am interested in testing 2 different drugs and studying its effect on heart disease within the gender groups.

+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Design 1    | Males  | Females |                       | Design 2    | Males | Females |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment A |   0    |   10    |                       | Treatment A |   1   |   9     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment B |   6    |   0     |                       | Treatment B |   5   |   1     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+

$\vspace{.2cm}$

+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Design 3    | Males  | Females |                       | Design 4    | Males | Females |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment A |   3    |   5     |                       | Treatment A |   4   |   4     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+
| Treatment B |   3    |   5     |                       | Treatment B |   4   |   4     |
+-------------+--------+---------+-----------------------+-------------+-------+---------+


1. This design is very bad. Because we have no males taking drug 1, and no females taking drug 2, we can't say if any observed differences are due to the effect of drug 1 versus 2, or gender. When this situation happens, we say that the gender effect is confounded with the drug effect.

2. This design is not much better. Because we only have one observation in the Male-Drug 1 group, any inference we make about the effect of drug 1 on males is based on one observation. In general that is a bad idea.

3. Design 3 is better than the previous 2 because it evenly distributes the males and females among the two drug categories. However, it seems wasteful to have more females than males because estimating average of the male groups, I only have 6 observations while I have 10 females.

4. This is the ideal design, with equal numbers of observations in each gender-drug group.

Designs 3 and 4 are good because the correlation among my predictors is 0. In design 1, the drug covariate is perfectly correlated to the gender covariate. The correlation is less in design 2, but is zero in designs 3 and 4.We could show this by calculating the design matrix for each design and calculating the correlation coefficients between each of pairs of columns.

Having an orthogonal design with equal numbers of observations in each group has many nice ramifications. Most importantly, with an orthogonal design, the interpretation of parameter is not dependent on what other factors are in the model. Balanced designs are also usually optimal in the sense that the variances of $\hat{\boldsymbol{\beta}}$ are as small as possible given the number of observations we have (barring any other *a priori* information). 

## Main Effects Model

In the one factor ANOVA case, the additional degrees of freedom used by adding a factor with $I$ levels was $I-1$. In the case that we consider two factors with the first factor having $I$ levels and the second factor having $J$ levels, then model
$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\epsilon_{ijk}$$ 
adds $(I-1)+(J-1)$ parameters to the model because both $\alpha_{1}=\beta_{1}=0$.

* The intercept term, $\mu$ is the reference point for all the other parameters. This is the expected value for an observation in the first level of factor 1 and the first level of factor two.

* $\alpha_{i}$ is the amount you expect the response to increase when changing from factor 1 level 1, to factor 1 level i
  (while the second factor is held constant).

* $\beta_{j}$ is the amount you expect the response to increase when changing from factor 2 level 1 to factor 2 level j
  (while the first factor is held constant).

Referring back to the fungus example, let the $\alpha_{i}$ values be associated with changes in humidity and $\beta_{j}$ values be associated with changes in temperature levels. Then the expected value of each treatment combination is

+---------+-----------------+--------------------------+-------------------------+-------------------------+ 
|         |     **5%**      |       **30%**            |      **60%**            |      **90%**            | 
+=========+=================+==========================+=========================+=========================+ 
| **2C**  |   $\mu+0+0$     |  $\mu+\alpha_2+0$        |  $\mu+\alpha_3+0$       |  $\mu+\alpha_4+0$       | 
+---------+-----------------+--------------------------+-------------------------+-------------------------+  
| **10C** | $\mu+0+\beta_2$ |  $\mu+\alpha_2+\beta_2$  |  $\mu+\alpha_3+\beta_2$ |  $\mu+\alpha_4+\beta_2$ | 
+---------+-----------------+--------------------------+-------------------------+-------------------------+  
| **30C** | $\mu+0+\beta_3$ |  $\mu+\alpha_2+\beta_3$  |  $\mu+\alpha_3+\beta_3$ |  $\mu+\alpha_4+\beta_3$ | 
+---------+-----------------+--------------------------+-------------------------+-------------------------+  


### Example - Fruit Trees

An experiment was conducted to determine the effects of four different pesticides on the yield of fruit from three different varieties of a citrus tree. Eight trees of each variety were randomly selected from an orchard. The four pesticides were randomly assigned to two trees of each variety and applications were made according to recommended levels. Yields of fruit (in bushels) were obtained after the test period.

Critically notice that we have equal number of observations for each treatment combination.

```{r}
# Typing the data in by hand because I got this example from a really old text book...
Pesticide <- factor(c('A','B','C','D')) 
Variety <- factor(c('1','2','3')) 
fruit <- data.frame( expand.grid(rep=1:2, Pest=Pesticide, Var=Variety) ) 
fruit$Yield <- c(49,39,50,55,43,38,53,48,55,41,67,58,53,42,85,73,66,68,85,92,69,62,85,99)
```

The first thing to do (as always) is to look at our data

```{r, fig.height=3}
ggplot(fruit, aes(x=Pest, color=Var, y=Yield, shape=Var)) + 
	geom_point(size=5) 
```

The first thing we notice is that pesticides B and D seem to be better than the others and that variety 3 seems to be the best producer. The effect of pesticide treatment seems consistent between varieties, so we don't expect that the interaction effect will be significant. We next fit a linear model and look at the diagnostic plots.

```{r, fig.height=3}
m3 <- lm(Yield ~ Var + Pest, data=fruit)
autoplot(m3, which=1:2)
```


There might be a little curvature in the fitted vs residuals, but because we can't fit a polynomial to a categorical variable, and the QQ-plot looks good, we'll ignore it for now and eventually consider an interaction term. Just for fun, we can examine the smaller models with just Variety or Pesticide.

```{r}
m1 <- lm(Yield ~ Var, data=fruit)
m2 <- lm(Yield ~ Pest, data=fruit)
m3 <- lm(Yield ~ Var + Pest, data=fruit)
summary(m1)$coef  %>% round(digits=3)
summary(m2)$coef  %>% round(digits=3)
summary(m3)$coef  %>% round(digits=3)
```

Notice that the affects for Variety and Pesticide are the same *whether or not the other is in the model*. This is due to the orthogonal design of the experiment and makes it much easier to interpret the main effects of Variety and Pesticide.

### ANOVA Table

Most statistical software will produce an analysis of variance table when fitting a two-way ANOVA. This table is very similar to the analysis of variance table we have seen in the one-way ANOVA, but has several rows which correspond to the additional factors added to the model. 

Consider the two-way ANOVA with factors $A$ and $B$ which have levels $I$ and $J$ discrete levels respectively. For convenience let $RSS_{1}$ is the residual sum of squares of the intercept-only model, and $RSS_{A}$ be the residual sum of squares for the model with just the main effect of factor $A$, and $RSS_{A+B}$ be the residual sum of squares of the model with both main effects. Finally assume that we have a total of $n$ observations. The ANOVA table for this model is as follows:

+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+
|  Source   |    df          |   Sum of Sq (SS)           |    Mean Sq               |    F         |    p-value                             |
+===========+================+============================+==========================+==============+========================================+
|   **A**   | $df_A=I-1$     | $SS_A = RSS_1 - RSS_A$     | $MS_A = SS_A / df_A$     | $MS_A / MSE$ | $P\left( F_{df_A, df_e} > F_A \right)$ |
+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+
|   **B**   | $df_B=J-1$     | $SS_B = RSS_A - RSS_{A+B}$ | $MS_B = SS_B / df_B$     | $MS_B / MSE$ | $P\left( F_{df_B, df_e} > F_B \right)$ |  
+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+
| **Error** | $df_e=n-I-J+1$ | $RSS_{A+B}$                | $MSE = RSS_{A+B} / df_e$ |              |                                        |
+-----------+----------------+----------------------------+--------------------------+--------------+----------------------------------------+

*Note, if the table is cut off, you can change decrease your font size and have it all show up...*

This arrangement of the ANOVA table is referred to as “Type I” sum of squares. 

We can examine this table in the fruit trees example using the anova() command but just passing a single model.

```{r}
m4 <- lm(Yield ~ Var + Pest, data=fruit)
anova( m4 )
```

We might think that this is the same as fitting three nested models and running an F-test on each successive pairs of models, but it isn't. While both will give the same Sums of Squares, the F statistics are different because the MSE of the complex model is different. In particular, the F-statistics are larger and thus the p-values are smaller for detecting significant effects.

```{r}
m1 <- lm(Yield ~ 1, data=fruit)
m2 <- lm(Yield ~ Var, data=fruit)
m3 <- lm(Yield ~ Var + Pest, data=fruit)
anova( m1, m2 )
anova( m2, m3 )
```

### Estimating Contrasts

As in the one-way ANOVA, we are interested in which factor levels differ. For example, we might suspect that it makes sense to group pesticides B and D together and claim that they are better than the group of A and C. 

Just as we did in the one-way ANOVA model, this is such a common thing to do that there is an easy way to do this, using `TukeyHSD`, which requires us coerce our model output to an `aov` object. We could specify all of the contrasts by hand and use `glht()` in the `multcomp` package to calculate all of the contrasts, but using `TukeyHSD` will be simpler. 

```{r}
m3 <- lm(Yield ~ Var + Pest, data=fruit)
TukeyHSD(aov(m3))
```

The output from the TukeyHSD is quite useful, but it would be nice to generate the data frame indicating group differences similar to what we did in the one-way ANOVA. We will use the exact same function we had before:

```{r}
#' Create a data frame with significant groupings 
#'
#' This function runs TukeyHSD on the input model and then creates a data frame
#' with a column for the factor and a second for the Significance Group
#' 
#' @param model The output of a lm() or aov() call that can be coerced to an aov object.
#' @param variable The variable of interest.
#' @output A data frame with a column for factor and another for the signicance group.
make_TukeyHSD_letters <- function(model, variable){ 
  Tukey <- TukeyHSD(aov(model))[[variable]]
  temp <- Tukey[,'p adj'] %>%
    vec2mat() %>%
    multcompLetters()
  out <- data.frame(group = names(temp$Letters), SigGroup=temp$Letters)
  colnames(out)[1] <- variable
  out
}  
```


```{r}
make_TukeyHSD_letters( m3, 'Var')
make_TukeyHSD_letters( m3, 'Pest')
```

So we see that each variety is significantly different from all the others and among the pesticides, $A$ and $C$ are indistigishable as are $B$ and $D$, but there is a difference between the $A,C$ and $B,D$ groups.

## Interaction Model

When the model contains the interaction of the two factors, our model is written as
$$y_{ijk}=\mu+\alpha_{i}+\beta_{j}+\left(\alpha\beta\right)_{ij}+\epsilon_{ijk}$$
 
Interpreting effects effects can be very tricky. Under the interaction, the effect of changing from factor 1 level 1 to factor 1 level $i$ depends on what level of factor 2 is. In essence, we are fitting a model that allows each of the $I\times J$ cells in my model to vary independently. As such, the model has a total of $I\times J$ parameters but because the model without interactions had $1+(I-1)+(J-1)$ terms in it, the interaction is adding $df_{AB}$ parameters. We can solve for this via: 
$$\begin{aligned}
I\times J	&=	1+(I-1)+(J-1)+df_{AB} \\
I\times J	&=	I+J-1+df_{AB} \\
IJ-I-J	  &=	-1+df_{AB} \\
I(J-1)-J	&=	-1+df_{AB} \\
I(J-1)-J+1	&=	df_{AB}  \\
I(J-1)-(J-1)	&=	df_{AB} \\
(I-1)(J-1)	&=	df_{AB} 
\end{aligned}$$

This makes sense because the first factor added $(I-1)$ columns to the design matrix and an interaction with a continuous covariate just multiplied the columns of the factor by the single column of the continuous covariate. Creating an interaction of two factors multiplies each column of the first factor by all the columns defined by the second factor. 

The expected value of the $ij$ combination is $\mu+\alpha_{i}+\beta_{j}+\left(\alpha\beta\right)_{ij}$. Returning to our fungus example, the expected means for each treatment under the model with main effects and the interaction is

+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+ 
|         |     **5%**        |                     **30%**                            |                      **60%**                          |                    **90%**                            | 
+=========+===================+========================================================+=======================================================+=======================================================+ 
| **2C**  |   $\mu+0+0+0$     |  $\mu+\alpha_2+0+0$                                    |  $\mu+\alpha_3+0+0$                                   |  $\mu+\alpha_4+0+0$                                   | 
+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+  
| **10C** | $\mu+0+\beta_2+0$ |  $\mu+\alpha_2+\beta_2+\left(\alpha\beta\right)_{22}$  |  $\mu+\alpha_3+\beta_2+\left(\alpha\beta\right)_{32}$ |  $\mu+\alpha_4+\beta_2+\left(\alpha\beta\right)_{42}$ | 
+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+  
| **30C** | $\mu+0+\beta_3+0$ |  $\mu+\alpha_2+\beta_3+\left(\alpha\beta\right)_{23}$  |  $\mu+\alpha_3+\beta_2+\left(\alpha\beta\right)_{33}$ |  $\mu+\alpha_4+\beta_2+\left(\alpha\beta\right)_{43}$ | 
+---------+-------------------+--------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+  


Notice that we have added $6=3\cdot2=\left(4-1\right)\left(3-1\right)=\left(I-1\right)\left(J-1\right)$ interaction parameters $\left(\alpha\beta\right)_{ij}$ to the main effects only model. The interaction model has $p=12$ parameters, one for each cell in my treatment array.

In general it is hard to interpret the meaning of $\alpha_{i}$, $\beta_{j}$, and $\left(\alpha\beta\right)_{ij}$ and the best way to make sense of them is to look at the interaction plots.

### ANOVA Table

Most statistical software will produce an analysis of variance table when fitting a two-way ANOVA. This table is very similar to the analysis of variance table we have seen in the one-way ANOVA, but has several rows which correspond to the additional factors added to the model. 

Consider the two-way ANOVA with factors $A$ and $B$ which have levels $I$ and $J$ discrete levels respectively. For convenience let $RSS_{1}$ be the residual sum of squares of the intercept-only model, and $RSS_{A}$ be the residual sum of squares for the model with just the main effect of factor $A$. Likewise $RSS_{A+B}$ and $RSS_{A*B}$ shall be the residual sum of squares of the model with just the main effects and the model with main effects and the interaction. Finally assume that we have a total of $n$ observations. The ANOVA table for this model is as follows:

+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
|           |     df               |  Sum Sq (SS)                     |    MS                              |  F             | $Pr(\ge F)$                               |
+===========+======================+==================================+====================================+================+===========================================+
|  **A**    | $df_A=I-1$           | $SS_A = RSS_1 - RSS_A$           | $MS_A = SS_A/df_A$                 | $MS_A / MSE$   | $Pr(F_{df_A,df_{\epsilon}} \ge F_A$       |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
|  **B**    | $df_B=J-1$           | $SS_B = RSS_A - RSS_{A+B}$       | $MS_B = SS_B/df_B$                 | $MS_B / MSE$   | $Pr(F_{df_B,df_{\epsilon}} \ge F_B$       |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
| **AB**    | $df_{AB}=(I-1)(J-1)$ | $SS_{A*B} = RSS_{A*B}-RSS_{A+B}$ | $MS_{AB} = SS_{AB} / df_{AB}$      | $MS_{AB}  MSE$ | $Pr(F_{df_{AB},df_{\epsilon}} \ge F_{AB}$ |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+
| **Error** | $df_{\epsilon}=n-IJ$ | $RSS_{A*B}$                      | $MSE = RSS_{A*B} / df_{\epsilon}$  |                |                                           |
+-----------+----------------------+----------------------------------+------------------------------------+----------------+-------------------------------------------+

This arrangement of the ANOVA table is referred to as “Type I” sum of squares. Type III sums of squares are the difference between the full interaction model and the model removing each parameter group, even when it doesn't make sense. For example in the Type III table, $SS_{A}=RSS_{B+A:B}-RSS_{A*B}$. There is an intermediate form of the sums of squares called Type II, that when removing a main effect also removes the higher order interaction. In the case of balanced (orthogonal) designs, there is no difference between the different types, but for non-balanced designs, the numbers will change. To access these other types of sums of squares, use the `Anova()` function in the package `car`. 

### Example - Fruit Trees (continued)

We next consider whether or not to include the interaction term to the fruit tree model. We fit the model with the interaction and then graph the results.

```{r, fig.height=3}
m4 <- lm(Yield ~ Var * Pest, data=fruit)
fruit$y.hat <- predict(m4)
ggplot(fruit, aes(x=Pest, color=Var, shape=Var, y=Yield)) + 
	geom_point(size=5) +
	geom_line(aes(y=y.hat, x=as.integer(Pest)))
```

All of the line segments are close to parallel so, we don't expect the interaction to be significant.

```{r}
anova( m4 )

```

Examining the ANOVA table, we see that the interaction effect is not significant and we will stay with simpler model `Yield~Var+Pest`. 

### Example - Warpbreaks

This data set looks at the number of breaks that occur in two different types of wool under three different levels of tension (low, medium, and high). The fewer number of breaks, the better. 

As always, the first thing we do is look at the data. In this case, it looks like the number of breaks decreases with increasing tension and perhaps wool B has fewer breaks than wool A.

```{r}
library(ggplot2)
library(faraway)
data(warpbreaks)
ggplot(warpbreaks, aes(x=tension, y=breaks, color=wool, shape=wool), size=2) + 
  geom_boxplot() +
  geom_point(position=position_dodge(width=.35)) # offset the wool groups 
```

We next fit our linear model and examine the diagnostic plots.

```{r}
model <- lm(breaks ~ tension + wool, data=warpbreaks)
autoplot(model, which=c(1,2))
```

The residuals vs fitted values plot is a little worrisome and appears to be an issue with non-constant variance, but the normality assumption looks good. We'll check for a Box-Cox transformation next.

```{r}
boxcox(model)
```


This suggests we should make a log transformation, though because the confidence interval is quite wide we might consider if the increased difficulty in interpretation makes sufficient progress towards making the data meet the model assumptions.. The diagnostic plots of the resulting model look better for the constant variance assumption, but the normality is now a worse off. Because the Central Limit Theorem helps deal with the normality question, I'd rather stabilize the variance at the cost of the normality.

```{r, fig.height=3}
model.1 <- lm(log(breaks) ~ tension + wool, data=warpbreaks)
autoplot(model.1, which=c(1,2))
```

Next we'll fit the interaction model and check the diagnostic plots. The diagnostic plots look good and this appears to be a legitimate model.

```{r}
model.2 <- lm(log(breaks) ~ tension * wool, data=warpbreaks)
autoplot(model.2, which=c(1,2))
```

Then we'll do an F-test to see if it is a better model than the main effects model. The p-value is marginally significant, so we'll keep the interaction in the model, but recognize that it is a weak interaction. 

```{r}
anova(model.1, model.2)
```


Next we look at the effect of the interaction and the easiest way to do this is to look at the interaction plot. This plot shows the raw data and connects lines to the cell mean of each factor combination.

```{r, fig.height=3}
warpbreaks$logy.hat <- predict(model.2)
ggplot(warpbreaks, aes(x=tension, y=log(breaks), color=wool, shape=wool)) +
  geom_point() +
  geom_line(aes(y=logy.hat, x=as.integer(tension)))
```


We can see that it appears that wool A has a decrease in breaks between low and medium tension, while wool B has a decrease in breaks between medium and high. It is actually quite difficult to see this interaction when we examine the model coefficients.

```{r}
summary(model.2)
```



To test if there is a statistically significant difference between medium and high tensions for wool type B, we really need to test the following hypothesis:
$$\begin{aligned}
H_{0}:\;\left(\mu+\alpha_{2}+\beta_{2}+\left(\alpha\beta\right)_{22}\right)-\left(\mu+\alpha_{3}+\beta_{2}+\left(\alpha\beta\right)_{32}\right)	& =	  0 \\
H_{a}:\;\left(\mu+\alpha_{2}+\beta_{2}+\left(\alpha\beta\right)_{22}\right)-\left(\mu+\alpha_{3}+\beta_{2}+\left(\alpha\beta\right)_{32}\right)	&\ne	0
\end{aligned}$$

This test reduces to testing if $\alpha_{2}-\alpha_{3}+\left(\alpha\beta\right)_{22}-\left(\alpha\beta\right)_{23}=0$. Calculating this difference from the estimated values of the summery table we have $-.6012+.6003+.6281-.2221=0.4051$, we don't know if that is significantly different than zero. 

In the main effects model, we were able to read off the necessary test using Tukey's HSD. Fortunately, we can do the same thing here. In this case, we'll look at the interactions piece of the TukeyHSD command. In this case, we find the test H:B - M:B in the last row of the interactions.

```{r}
TukeyHSD( aov(model.2) )
```


So TukeyHSD gives us all the tests comparing the cell means, but what are those main effects testing? In the case where our experiment is balanced with equal numbers of observations in each treatment cell, we can interpret these differences as follows. Knowing that each cell in our table has a different estimated mean, we could consider the average of all the type A cells as the typical wool A. Likewise we could average all the cell means for the wool B cells. Then we could look at the difference between those two averages. In the balanced design, this is equivalent to removing the tension term from the model and just looking at the difference between the average log number of breaks.

Using TukeyHSD, we can see the wool effect difference between types B and A is $-0.1522$. We can calculate the mean number of log breaks for each wool type and take the difference by the following:

```{r}
warpbreaks %>% 
  group_by(wool) %>% 
  summarise( wool.means = mean(log(breaks)) ) %>%
  summarise( diff(wool.means) )
```

In the unbalanced case taking the average of the cell means produces a different answer than taking the average of the data. It isn't clear which is preferred and TukeyHSD produces a result that is between those two methods.

## Exercises
1. In the `faraway` package, the data set `rats` has data on a gruesome experiment that examined the time till death of 48 rats when they were subjected to three different types of poison administered in four different manners (which they called treatments). We are interested in assessing which poison works the fastest as well as which administration method is most effective. 
    a. Consider the interaction model which allows for a different effect of treatment for each poison type. Fit this model and examine the diagnostic plots. What stands out?
    b. Perform a Box-Cox analysis and perform the recommended transformation (with the realm of "common" transformations). Call the transformed variable `speed`.
    c. Fit the interaction model using the transformed response. Create a graph of data and the predicted values. Visually assess if you think the interaction is significant.
    d. Perform an appropriate statistical test to see if the interaction is statistically significant.
    e. What do you conclude about the poisons and treatment (application) types?
    
2. In the `faraway` package, the dataset `butterfat` has information about the the percent of the milk was butterfat (more is better) taken from $n=100$ cows. There are $5$ different breeds of cows and $2$ different ages.  We are interested in asessing if `Age` and `Breed` affect the butterfat content
    a. Graph the data. Do you think an interaction model is justified?
    b. Perform an appropriate set of tests to select a model for predicting `Butterfat`.
    c. Discuss your findings.

3. In the `faraway` package, the dataset `alfalfa` has information from a study that examined the effect of seed innoculum, irrigation, and shade on alfalfa yield. This data has $n=24$ observations.
    a. Graph the data. 
    b. Consider the main effects model with all three predictor variables. Examine the diagnostic plots and comment. Consider a Box-Cox transformation to your response (you might need to use `lambda = seq(-2,6, by=.01)` to see the whole curve). Make any transformation you feel is justified.
    c. Consider the model with `shade` and `inoculum` and the interaction between the two. Examine the anova table.  Why does R complain that the fit is perfect? *Hint: Think about the degrees of freedom of the model compared to the sample size.*
    d. Discuss your findings and the limitations of your investigation based on data.
    


<!--chapter:end:09_TwoWayAnova.Rmd-->

# Block Designs

```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

```{r, message=FALSE, warning=FALSE}
# packages for this chapter
library(tidyverse)    # ggplot2, dplyr, etc...
library(multcompView) # TukeyLetters stuff
library(devtools)
#install_github('dereksonderegger/dsData')  # some datasets I've made up; only install once...
library(dsData)
library(faraway)
library(MASS)         # for the oats dataset
```
Often there are covariates in the experimental units that are known to affect the response variable and must be taken into account. Ideally an experimenter can group the experimental units into blocks where the within block variance is small, but the block to block variability is large.  For example, in testing a drug to prevent heart disease, we know that gender, age, and exercise levels play a large role. We should partition our study participants into gender, age, and exercise groups and then randomly assign the treatment (placebo vs drug) within the group. This will ensure that we do not have a gender, age, and exercise group that has all placebo observations.

Often blocking variables are not the variables that we are primarily interested in, but must nevertheless be considered. We call these nuisance variables.  We already know how to deal with these variables by adding them to the model, but there are experimental designs where we must be careful because the experimental treatments are *nested*.

Example 1. An agricultural field study has three fields in which the researchers will evaluate the quality of three different varieties of barley. Due to how they harvest the barley, we can only create a maximum of three plots in each field. In this example we will block on field since there might be differences in soil type, drainage, etc from field to field. In each field, we will plant all three varieties so that we can tell the difference between varieties without the block effect of field confounding our inference. In this example, the varieties are nested within the fields.  

+--------------+-------------+------------+-----------+
|              | Field 1     |  Field 2   |  Field 3  |
+==============+=============+============+===========+
| **Plot 1**   |  Variety A  |  Variety C | Variety B |
+--------------+-------------+------------+-----------+
| **Plot 2**   |  Variety B  |  Variety A | Variety C |
+--------------+-------------+------------+-----------+
| **Plot 3**   |  Variety C  |  Variety B | Variety A |
+--------------+-------------+------------+-----------+


Example 2. We are interested in how a mouse responds to five different materials inserted into subcutaneous tissue to evaluate the materials' use in medicine. Each mouse can have a maximum of 3 insertions. Here we will block on the individual mice because even lab mice have individual variation. We actually are not interested in estimating the effect of the mice because they aren't really of interest, but the mouse block effect should be accounted for before we make any inferences about the materials. Notice that if we only have one insertion per mouse, then the mouse effect will be confounded with materials.  

## Randomized Complete Block Design (RCBD)
The dataset oatvar in the faraway library contains information about an experiment on eight different varieties of oats. The area in which the experiment was done had some systematic variability and the researchers divided the area up into five different blocks in which they felt the area inside a block was uniform while acknowledging that some blocks are likely superior to others for growing crops. Within each block, the researchers created eight plots and randomly assigned a variety to a plot. This type of design is called a Randomized Complete Block Design (RCBD) because each block contains all possible levels of the factor of primary interest.

```{r, message=FALSE, warning=FALSE}
data(oatvar)
ggplot(oatvar, aes(y=yield, x=block, color=variety)) + 
	geom_point(size=5) +
	geom_line(aes(x=as.integer(block))) # connect the dots
```

While there is one unusual observation in block IV, there doesn't appear to be a blatant interaction. We will consider the interaction shortly. For the main effects model of yield ~ block + variety we have $p=12$ parameters and $28$ residual degrees of freedom because 
$$\begin{aligned}
  df_\epsilon	&=	n-p \\
	    &=	n-\left(1+\left[\left(I-1\right)+\left(J-1\right)\right]\right) \\
	    &=	n-\left(1+\left[\left(5-1\right)+\left(8-1\right)\right]\right) \\
	    &=	n-12 \\
	    &=	28
 \end{aligned}$$
 
```{r}
m1 <- lm( yield ~ block + variety, data=oatvar)
anova(m1)
# plot(m1)      # check diagnostic plots - they are fine...
```

Because this is an orthogonal design, the sums of squares doesn't change regardless of which order we add the factors, but if we remove one or two observations, they would.

In determining the significance of `variety` the above F-value and p-value is correct. We have 40 observations (5 per variety), and after accounting for the model structure (including the extraneous blocking variable), we have $28$ residual degrees of freedom.

But the F-value and p-value for testing if `block` is significant is wrong!  Imagine that variety didn't matter we just have 8 replicate samples per block, but these aren't true replicates, they are what is called *pseudoreplicates*. Imagine taking a sample of $n=3$ people and observing their height at 1000 different points in time during the day.  You don't have 3000 data points for estimating the mean height in the population, you have 3.  Unless we account for the this, the inference for the block variable is wrong.


```{r}
m3 <- aov( yield ~ block + variety + Error(block), data=oatvar)
summary(m3)
```


Fortunately in this case, we don't care about the blocking variable and including it in the model was simply guarding us in case there was a difference, but I wasn't interested in estimating it.  If the only covariate I care about is the most deeply nested effect, then I can do my usual analysis and recognize the p-values for the blocking variable is wrong, but I don't care about it.


```{r}
#' Create a data frame with significant groupings 
#'
#' This function runs TukeyHSD on the input model and then creates a data frame
#' with a column for the factor and a second for the Significance Group
#' 
#' @param model The output of a lm() or aov() call that can be coerced to an aov object.
#' @param variable The variable of interest.
#' @output A data frame with a column for factor and another for the signicance group.
make_TukeyHSD_letters <- function(model, variable){ 
  Tukey <- TukeyHSD(aov(model))[[variable]]
  temp <- Tukey[,'p adj'] %>%
    vec2mat() %>%
    multcompLetters()
  out <- data.frame(group = names(temp$Letters), SigGroup=temp$Letters)
  colnames(out)[1] <- variable
  out
}  
```

```{r}
# Ignore any p-values regarding block, but I'm happy with the analysis for variety
oatvar <- oatvar %>%
  left_join( make_TukeyHSD_letters( m1, 'variety') )

ggplot(oatvar, aes(x=variety, y=yield)) +
  geom_boxplot() +
  geom_text( aes(label=SigGroup), y=505) +
  scale_y_continuous(limits=c(200, 510))  # give me some room for letters
```


## Split-plot designs
There are plenty of experimental designs where we have levels of treatments nested within each other for practical reasons. The literature often gives the example of an agriculture experiment where we investigate the effect of irrigation and fertilizer on the yield of a crop. However because our irrigation system can't be fine-tuned, we have plots with different irrigation levels and within each plot we have perhaps four subplots that have the fertilizer treatment.

```{r, echo=FALSE}
data('AgData')
# The data is actually more complex than described. For this example we'll look
# at the simpler analysis where we model the mean yield in each subplot.
AgData <- AgData %>% 
  group_by(plot, subplot, Fertilizer, Irrigation) %>% 
  summarise( yield = mean(yield))
```

```{r, echo=FALSE}
AgData <- AgData %>% 
  mutate( row=ceiling(as.integer(subplot) / 2),
          col=as.integer(subplot) %% 2 + 1,
          vis.plot = as.integer(plot)) %>%
  group_by(plot) %>%
  mutate(Fertilizer = sample(Fertilizer)) 
ggplot(AgData ) +
   geom_tile(aes(x=col, y=row, fill=Fertilizer),  color='black', size=1) +
   facet_wrap(  ~ plot, labeller=label_both, ncol=4) +
   geom_text( aes(label=paste("Irrigation", Irrigation)), x=1.5, y=2.7) +
   ylim(c(0.5, 2.8))
```

So all together we have 8 plots, and 32 subplots. When I analyze the fertilizer, I have 32 experimental units (the thing I have applied my treatment to), but when analyzing the effect of irrigation, I only have 8 experimental units. 

I like to think of this set up as having some lurking variables that act at the plot level (changes in aspect, maybe something related to what was planted prior) and some lurking variables that act on a local subplot scale (maybe variation in clay/silt/sand ratios).  So even after I account for Irrigation and Fertilizer treatments, observations within a plot will be more similar to each other than observations in two different plots.


We can think about doing two separate analyses, one for the effect of irrigation, and another for the effect of the fertilizer.

```{r}
# To analyze Irrigation, average over the subplots first...
Irrigation.data <-   AgData %>% 
  group_by(plot, Irrigation) %>%
  summarise( yield = mean(yield)) 

# Now do a standard analysis. I use the aov() command instead of lm()
# because we will shortly do something very tricky that can only be
# done with aov(). For the most part, everything is 
# identical from what you are used to.
m <- aov( yield ~ Irrigation, data=data.frame(Irrigation.data) )
anova(m)
```

In this case we see that we have insufficient evidence to conclude that the observed difference between the Irrigation levels could not be due to random chance.

Next we can do the appropriate analysis for the fertilizer, recognizing that all the p-values for the plot effects are nonsense and should be ignored.

```{r}
m <- aov( yield ~ plot + Fertilizer, data=AgData )
summary(m)
```


Ideally I wouldn't have to do the averaging over the nested observations and we would like to not have the misleading p-values for the plots. To do this, we only have to specify the nesting of the error terms and R will figure out the appropriate degrees of freedom for the covariates.

```{r}
# To do this right, we have to abandon the general lm() command and use the more
# specialized aov() command.  The Error() part of the formula allows me to nest
# the error terms and allow us to do the correct analysis. The order of these is
# to start with the largest/highest level and then work down the nesting.
m2 <- aov( yield ~ Irrigation + Fertilizer + Error(plot/subplot), data=AgData )
summary(m2)
```

In the output, we see that the ANOVA table row for the Fertilizer is the same for both analyses, but the sums-of-squares for Irrigation are different between the two analyses (because of the averaging) while the F and p values are the same between the two analyses.

What would have happened if we had performed the analysis incorrectly and had too many degrees of freedom for the Irrigation test?

```{r}
bad.model <- aov( yield ~ Irrigation + Fertilizer, data=AgData)
anova(bad.model)
```

In this case we would have concluded that we had statistically significant evidence to conclude the Irrigation levels are different.  Notice that the sums-of-squares in this **wrong** analysis match up with the sums-of-squares in the correct design and the only difference is that when we figure out the sum-of-squares for the residuals we split that into different pools. 
$$\begin{aligned}
  RSS_{total} &= RSS_{Fertilizer} + RSS_{Irrigation} \\
        304.0 &= 129.6 + 174.4
  \end{aligned}$$

When we want to infer if the amount of noise explained by adding Irrigation or Fertilizer is sufficiently large to justify their inclusion into the model, we compare the sum-of-squares value to the RSS but now we have to use the appropriate pool. 

***

A second example of a slightly more complex split plot is given in the package `MASS` under the dataset `oats`. From the help file the data describes the following experiment:
    
    The yield of oats from a split-plot field trial using three varieties and 
    four levels of manurial treatment. The experiment was laid out in 6 blocks 
    of 3 main plots, each split into 4 sub-plots. The varieties were applied to 
    the main plots and the manurial treatments to the sub-plots.
    
This is a lot to digest so lets unpack it. First we have 6 blocks and we'll replicate the exact same experiment in each block.  Within a block, we'll split it into three sections, which we'll call plots (within the block).  Finally within each plot, we'll have 4 subplots.  

We have 3 varieties of oats, and 4 levels of fertilizer (manure). To each set of 3 plots, we'll randomly assign the 3 varieties, and to each set of subplots, we'll assign the fertilizers. 

One issue that makes this issue confusing for students is that most texts get lazy and don't define the blocks, plots, and sub-plots when there are no replicates in a particular level.  I prefer to be clear about defining those so.

```{r}
data(oats)
oats <- oats %>% mutate(
  Nf = ordered(N, levels = sort(levels(N))),  # make manure an ordered factor
  plot = as.integer(V),                       # plot
  subplot = as.integer(Nf))                   # sub-plot
```


As always we first create a graph to examine the data
```{r}
ggplot(oats, aes(x=Nf, y=Y, color=V)) +
  facet_wrap( ~ B, ncol=3) +
  geom_point() +
  geom_line(aes(x=as.integer(Nf)))
```

This graph also makes me think that variety doesn't matter and it is unlikely that there an interaction between oat variety and fertilizer level, but we should check. 

```{r}
#  What makes sense to me
# m.c <- aov( Y ~ V * Nf + Error(B/plot/subplot), data=oats)
```
Unfortunately the above model isn't correct because R isn't smart enough to understand that the levels of plot and subplot are exact matches to the Variety and Fertilizer levels. As a result if I defined the model above, the degrees of freedom will be all wrong because there is too much nesting.  So we have to be smart enough to recognize that plot and subplot are actually Variety and Fertilizer.

```{r}
m.c <- aov( Y ~ V * Nf + Error(B/V/Nf), data=oats)
summary(m.c)
```

Sure enough the interaction term is not significant. We next consider the Variety term.

```{r}
m.s <- aov( Y ~ V + Nf + Error(B/V/Nf), data=oats)
summary(m.s)
```

We conclude by noticing that the Variety does not matter, but that the fertilizer level is quite significant.

***

There are many other types of designs out there. For example you might have 5 levels of a factor, but when you split your block into plots, you can only create 3 plots.  So not every block will have every level of the factor. This is called *Randomized Incomplete Block Designs* (RIBD).

You might have a design where you apply even more levels of nesting.  Suppose you have a green house study where you have rooms where you can apply a temperature treatment, within the room you have four tables and can apply a light treatment to each table. Finally within each table you can have four trays where can apply a soil treatment to each tray. This is a continuation of the split-plot design and by extending the nesting we can develop *split-split-plot* and *split-split-split-plot* designs.

You might have 7 covariates each with two levels (High, Low) and you want to investigate how these influence your response but also allow for second and third order interactions.  If you looked at every treatment combination you'd have $2^7=128$ different treatment combinations and perhaps you only have the budget for a sample of $n=32$.  How should you design your experiment? This question is addressed by *fractional factorial* designs.

If your research interests involve designing experiments such as these, you should consider taking an Experimental design course.

## Exercises
1. ???
2. ???
3. ???

<!--chapter:end:10_BlockDesigns.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Mixed Effects Models
```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```

```{r, warning=FALSE, message=FALSE}
library(faraway)
library(MASS)
library(lme4)
library(tidyverse)
library(lsmeans)
library(multcompView)
library(car)
library(stringr)
# library(devtools)
# install_github('dereksonderegger/dsData')  # some datasets I've made up; only install once...
library(dsData)
library(pander)
```


The assumption of independent observations is often not supported and dependent data arises in a wide variety of situations. The dependency structure could be very simple such as rabbits within a litter being correlated and the litters being independent. More complex hierarchies of correlation are possible. For example we might expect voters in a particular part of town (called a precinct) to vote similarly, and particular districts in a state tend to vote similarly as well, which might result in a precinct / district / state hierarchy of correlation.

Many of the designs mentioned in the Block Designs section could be similarly modeled using Mixed Effects Models. In many respects, the random effects structure provides a more flexible framework to consider many of the traditional experimental designs as well as many non-traditional designs with the benefit of more easily assessing variability at each hierarchical level.

Mixed effects models combine what we call "fixed" and "random" effects. 

+---------------------+--------------------------------------------------------+
| **Fixed effects**   | Unknown constants that we wish to estimate from the    |
|                     | model and could be similarly estimated in subsequent   |
|                     | experimentation. The research is interested in these   |
|                     | particular levels.                                     |
+---------------------+--------------------------------------------------------+
| **Random effects**  | Random variables sampled from a population which       |
|                     | cannot be observed in subsequent experimentation. The  |
|                     | research is not interested in these particular levels, | 
|                     | but rather how the levels vary from sample to sample.  | 
+---------------------+--------------------------------------------------------+

For example, in a rabbit study that examined the effect of diet on the growth of domestic rabbits and we had 10 litters of rabbits and used the 3 most similar from each litter to test 6 different diets. Here, the 6 different diets are fixed effects because they are not randomly selected from a population, these exact same diets can be further studied, and these are the diets we are interested it. The litters of rabbits and the individual rabbits are randomly selected from populations, cannot be exactly replicated in future studies, and we are not interested in the individual litters but rather what the variability is between individuals and between litters. 

Often random effects are not of primary interest to the researcher, but must be considered. Often blocking variables are random effects because the arise from a random sample of possible blocks that are potentially available to the researcher.

Mixed effects models are models that have both fixed and random effects. We will first concentrate on understanding how to address a model with two sources error and then complicate the matter with fixed effects.


## Review of Maximum Likelihood Methods

Recall that the likelihood function is the function links the model parameters to the data and is found by taking the probability density function and interpreting it as a function of the parameters instead of the a function of the data. Loosely, the probability function tells us what outcomes are most probable, with the height of the function telling us which values (or regions of values) are most probable given a set of parameter values. The higher the probability function, the higher the probability of seeing that value (or data in that region). The likelihood function turns that relationship around and tells us what parameter values are most likely to have generated the data we have, again with the parameter values with a higher likelihood value being more “likely”.

The likelihood function for a sample $y_i \stackrel{iid}{\sim} N\left( 0, \sigma \right)$ can be written as a function of our parameters $\mu$ and $\sigma^{2}$ then we have defined our likelihood function
$$L \left(\mu,\sigma^{2}|y_{1},\dots,y_{n}\right)=\frac{1}{\left(2\pi\right)^{n/2}\left[\det\left(\boldsymbol{\Omega}\right)\right]^{1/2}}\exp\left[-\frac{1}{2}\left(\boldsymbol{y}-\boldsymbol{\mu}\right)^{T}\boldsymbol{\Omega}^{-1}\left(\boldsymbol{y}-\boldsymbol{\mu}\right)\right]$$

where the variance/covariance matrix is $\boldsymbol{\Omega}=\sigma I_n$.

We can use to this equation to find the maximum likelihood estimators by either taking the derivatives and setting them equal to zero and solving for the parameters or by using numerical methods. In the normal case, we can find the maximum likelihood estimators (MLEs) using the derivative trick and we find that $$\hat{\mu}_{MLE}=\hat{y}=\bar{y}$$
and
$$\hat{\sigma}_{MLE}^{2}=\frac{1}{n}\sum_{i=1}^{n}\left(y_{i}-\hat{y}\right)^{2}$$
and we notice that this is not our usual estimator $\hat{\sigma}^{2}=s^{2}$ where $s^{2}$ is the sample variance. It turns out that the MLE estimate of $\sigma^{2}$ is biased (the correction is to divide by $n-1$ instead of $n$). This is normally not an issue if our sample size is large, but with a small sample, the bias is not insignificant.

Notice if we happened to know that $\mu=0$, then we could use $$\hat{\sigma}_{MLE}^{2}=\frac{1}{n}\sum_{i=1}^{n}y_{i}^{2}$$ 
and this would be unbiased for $\sigma^{2}$.
 
In general (a not just in the normal case above) the *Likelihood Ratio Test* (LRT) provides a way for us to compare two nested models. Given $m_{0}$ which is a simplification of $m_{1}$ then we could calculate the likelihoods functions of the two models $L\left(\boldsymbol{\theta}_{0}\right)$ and $L\left(\boldsymbol{\theta}_{1}\right)$ where $\boldsymbol{\theta}_{0}$ is a vector of parameters for the null model and $\boldsymbol{\theta}_{1}$ is a vector of parameter for the alternative. Let $\hat{\boldsymbol{\theta}}_{0}$ be the maximum likelihood estimators for the null model and $\hat{\boldsymbol{\theta}}_{1}$ be the maximum likelihood estimators for the alternative. Finally we consider the value of 
$$\begin{aligned}
  D	&=	-2*\log\left[\frac{L\left(\hat{\boldsymbol{\theta}}_{0}\right)}{L\left(\hat{\boldsymbol{\theta}}_{1}\right)}\right] \\
	  &=	-2\left[\log L\left(\hat{\boldsymbol{\theta}}_{0}\right)-\log L\left(\hat{\boldsymbol{\theta}}_{1}\right)\right]
	\end{aligned}$$
 

Under the null hypothesis that $m_{0}$ is the true model, the $D\stackrel{\cdot}{\sim}\chi_{p_{1}-p_{0}}^{2}$ where $p_{1}-p_{0}$ is the difference in number of parameters in the null and alternative models. That is to say that asymptotically $D$ has a Chi-squared distribution with degrees of freedom equal to the difference in degrees of freedom of the two models. 

We could think of $L\left(\hat{\boldsymbol{\theta}}_{0}\right)$ as the maximization of the likelihood when some parameters are held constant (at zero) and all the other parameters are vary. But we are not required to hold it constant at zero. We could chose any value of interest and perform a LRT. 

Because we often regard a confidence interval as the set of values that would not be rejected by a hypothesis test, we could consider a sequence of possible values for a parameter and figure out which would not be rejected by the LRT. In this fashion we can construct confidence intervals for parameter values.

Unfortunately all of this hinges on the asymptotic distribution of $D$ and often this turns out to be a poor approximation. In simple cases more exact tests can be derived (for example the F-tests we have used prior) but sometimes nothing better is currently known. Another alternative is to use permutation methods.

## 1-way ANOVA with a random effect

We first consider the simplest model with two sources of variability, a 1-way ANOVA with a random factor covariate $$y_{ij}=\mu+\gamma_{i}+\epsilon_{ij}$$
where $\gamma_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{\gamma}^{2}\right)$ and $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)$. This model could occur, for example, when looking at the adult weight of domestic rabbits where the random effect is the effect of litter and we are interested in understanding how much variability there is between litters $\left(\sigma_{\gamma}^{2}\right)$ and how much variability there is within a litter $\left(\sigma_{\epsilon}^{2}\right)$.  Another example is the the creation of computer chips. Here a single wafer of silicon is used to create several chips and we might have wafer-to-wafer variability and then within a wafer, you have chip-to-chip variability.

First we should think about what the variances and covariances are for any two observations. 
$$\begin{aligned}
  Var\left(y_{ij}\right)	
   &=	Var\left(\mu+\gamma_{i}+\epsilon_{ij}\right) \\
	 &=	Var\left(\mu\right)+Var\left(\gamma_{i}\right)+Var\left(\epsilon_{ij}\right) \\
	 &=	0+\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} 
	 \end{aligned}$$
and $Cov\left(y_{ij},y_{ik}\right)=\sigma_{\gamma}^{2}$ because the two observations share the same litter $\gamma_{i}$. For two observations in different litters, the covariance is 0. These relationships induce a correlation on observations within the same litter of 
$$\rho=\frac{\sigma_{\gamma}^{2}}{\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2}}$$
 

For example, suppose that we have $I=3$ litters and in each litter we have $J=3$ rabbits per litter. Then the variance-covariance matrix looks like 
$$\boldsymbol{\Omega}	=	\left[\begin{array}{ccccccccc}
\sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & . & . & . & . & . & .\\
\sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & . & . & . & . & . & .\\
\sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & . & . & . & . & . & .\\
. & . & . & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & . & . & .\\
. & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & . & . & .\\
. & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & . & . & .\\
. & . & . & . & . & . & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}\\
. & . & . & . & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2} & \sigma_{\gamma}^{2}\\
. & . & . & . & . & . & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2} & \sigma_{\gamma}^{2}+\sigma_{\epsilon}^{2}
\end{array}\right]$$

Substituting this new variance-covariance matrix into our likelihood function, we now have a likelihood function which we can perform our usual MLE tricks with.

In the more complicated situation where we have a full mixed effects model, we could write $$\boldsymbol{y}=\boldsymbol{X}\boldsymbol{\beta}+\boldsymbol{Z}\boldsymbol{\gamma}+\boldsymbol{\epsilon}$$
where $\boldsymbol{X}$ is the design matrix for the fixed effects, $\boldsymbol{\beta}$ is the vector of fixed effect coefficients, $\boldsymbol{Z}$ is the design matrix for random effects, $\boldsymbol{\gamma}$ is the vector of random effects such that $\gamma_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{\gamma}^{2}\right)$ and finally $\boldsymbol{\epsilon}$ is the vector of error terms such that $\epsilon_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)$. Notice in our rabbit case

$$\boldsymbol{Z}=\left[\begin{array}{ccc}
1 & \cdot & \cdot\\
1 & \cdot & \cdot\\
1 & \cdot & \cdot\\
\cdot & 1 & \cdot\\
\cdot & 1 & \cdot\\
\cdot & 1 & \cdot\\
\cdot & \cdot & 1\\
\cdot & \cdot & 1\\
\cdot & \cdot & 1
\end{array}\right]\;\;\;\;ZZ^{T}=\left[\begin{array}{ccccccccc}
1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\
1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\
1 & 1 & 1 & \cdot & \cdot & \cdot & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & 1 & 1 & 1 & \cdot & \cdot & \cdot\\
\cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & 1\\
\cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & 1\\
\cdot & \cdot & \cdot & \cdot & \cdot & \cdot & 1 & 1 & 1
\end{array}\right]$$

which makes it easy to notice $$\boldsymbol{\Omega}=\sigma_{\gamma}^{2}\boldsymbol{Z}\boldsymbol{Z}^{T}+\sigma_{\epsilon}^{2}\boldsymbol{I}$$
 

In practice we tend to have relatively small numbers of block parameters and thus have a small number of observations in which to estimate $\sigma_{\gamma}^{2}$ which means that the biased nature of MLE estimates will be sub-optimal. If we knew that $\boldsymbol{X}\boldsymbol{\beta}=\boldsymbol{0}$ we could use that fact and have an unbiased estimate of our variance parameters. Because $\boldsymbol{X}$ is known, we can find linear functions $\boldsymbol{k}$ such that $\boldsymbol{k}^{T}\boldsymbol{X}=0$. We can form a matrix $\boldsymbol{K}$ that represents all of these possible transformations and we notice that $$\boldsymbol{K}^{T}\boldsymbol{y} \sim N \left( \boldsymbol{K}^{T}\boldsymbol{X\beta}, \, \boldsymbol{K}^{T}\boldsymbol{\Omega}\boldsymbol{K}\right) = N\left( \boldsymbol{0}, \boldsymbol{K}^{T}\boldsymbol{\Omega}\boldsymbol{K}\right)$$
and perform our maximization on this transformed set of data. Once we have our unbiased estimates of $\sigma_{\gamma}^{2}$ and $\sigma_{\epsilon}^{2}$, we can substitute these back into the untransformed likelihood function and find the MLEs for $\boldsymbol{\beta}$. This process is called Restricted Maximum Likelihood (REML) and is generally preferred over the variance component estimates found simply maximizing the regular likelihood function. As usual, if our experiment is balanced these complications aren't necessary as the REML estimates of $\boldsymbol{\beta}$ are usually the same as the ML estimates.
 
Our first example comes from an experiment to test the paper brightness as affected by the shift operator. The data has 20 observations with 4 different operators. Each operator had 5 different observations made.  The data set is `pulp` in the package `faraway`. We will first analyze this using a fixed-effects one-way ANOVA, but we will use a different model representation. Instead of using the first operator as the reference level, we will use the sum-to-zero constraint (to make it easier to compare with the output of the random effects model).

```{r}
library(faraway)
data(pulp)
# set the contrasts to sum-to-zero constraint
op <- options(contrasts=c('contr.sum', 'contr.poly'))
m <- aov(bright ~ operator, data=pulp)
summary(m)
coef(m)
```

The sum-to-zero constraint forces the operator parameters to sum to zero so we can find the value of the fourth operator as operator4 = -(-0.16-0.34+0.22) = 0.28
 
To fit the random effects model we will use the package `lme4` which stands for Linear Mixed Effects and the 4 represents that the package is written in the S4 dialect of R (which means a few things will be new to us).

```{r, warning=FALSE, message=FALSE}
m2 <- lmer( bright ~ 1 + (1|operator), data=pulp )
summary(m2)
```

Notice that the estimate of the fixed effect (the overall mean) is the same in the fixed-effects ANOVA and in the mixed model. However the fixed effects ANOVA estimates the effect of each operator while the mixed model is interested in estimating the variance between operators. In the model statement the (1|operator) denotes the random effect and this notation tells us to fit a model with a random intercept term for each operator. Here the variance associated with the operators is $\sigma_{\gamma}^{2}=0.068$ while the “pure error” is $\sigma_{\epsilon}^{2}=0.106$. The column for standard deviation is not the variability associated with our estimate, but is simply the square-root of the variance terms $\sigma_{\gamma}$ and $\sigma_{\epsilon}$. This was fit using the REML method. 

We might be interested in the estimated effect of each operator
```{r}
ranef(m2)
```
These effects are smaller than the values we estimated in the fixed effects model due to distributional assumption that penalizes large deviations from the mean. In general, the estimated random effects are of smaller magnitude than the effect size estimated using a fixed effect model.

```{r}
# reset the contrasts to the default
options(contrasts=c("contr.treatment", "contr.poly" ))
```


## Blocks as Random Variables
Blocks are properties of experimental designs and usually we are not interested in the block levels *per se* but need to account for the variability introduced by them. 

Recall the agriculture experiment in the dataset `oatvar` from the `faraway` package. We had 8 different varieties of oats and we had 5 different fields (which we called blocks).  Because of limitations on how we plant, we could only divide the blocks into 8 plots and in each plot we planted one of the varieties.  

```{r}
library(faraway)
ggplot(oatvar, aes(y=yield, x= variety)) + 
  geom_point() +
  facet_wrap(~block, labeller=label_both)
```

In this case, we don't really care about these particular fields (blocks) and would prefer to think about these as a random sample of fields that we might have used in our experiment.

```{r}
model.0 <- lmer( yield ~ (1|block), data=oatvar)
model.1 <- lmer( yield ~ variety + (1|block), data=oatvar)
anova(model.0, model.1)
```
This shows that the variety matters, though this is pretty annoying.  We'd prefer to use the `anova` command with just model and see the p-values for each covariate. R doesn't do this by default because there isn't a completely reliable algorithm for figuring out the correct degrees of freedom for the residual. One way you can get these is to use the `car::Anova` function.

```{r}
car::Anova(model.1, type=3)  # Type III anova table (just as we've had before)
```

Notice that this is using a Chi-squared test statistic. This is actually using the Likelihood Ratio Test which is an approximate test, but should be reasonable. Now that we have chosen our model, we can examine is model.
```{r}
summary(model.1)
```

We start with the Random effects.  This section shows us the *block-to-block* variability (and the square root of that, the Standard Deviation) as well as the "pure-error", labeled residuals, which is an estimate of the variability associated with two different observations (after the difference in variety is accounted for) planted *within* the same block. For this we see that block-to-block variability is only slightly smaller than the within block variability.

Why do we care about this? This actually tells us quite a lot about the spatial variability.  Because yield is affected by soil nutrients, micro-climate, soil water availability, etc, I expect that two identical seedlings planted in slightly different conditions will have slightly different yields. By examining how the yield changes over small distances (the residual within block variability) vs how it changes over long distances (block to block variability) we can get a sense as to the scale at which these background lurking processes operate.  

Next we turn to the fixed effects.  These will be the offsets from the reference group, as we've typically worked with. Here we see that varieties 2,5, and 8 are the best performers (relative to variety 1), but we don't have any p-values denoting if these differences could be due to random chance or if the differences are statistical significant. The reason for this is that in mixed models it is not always clear what the appropriate degrees of freedom are for the residuals, and therefore we don't know what the appropriate t-distribution is to compare the t-values to. In simple balanced designs the degrees of freedom can be calculated, but in complicated unbalanced designs the appropriate degrees of freedom is not known and all proposed heuristic methods (including what is calculated by SAS) can fail spectacularly in certain cases. The authors of `lme4` are adamant that until robust methods are developed, they prefer to not calculate any p-values. A previous version of this software (package `nlme`) will produce p-values but at the cost of flexibility in the types of mixed models that can be fit. 

We are certain that there are differences among the varieties, and we should look at all of the pairwise contrasts among the variety levels. Unfortunately `TukeyHSD` doesn't work with objects created by `lmer` so we have to turn to another package. In this case, we could use either `multcomp` and build the contrast vectors $\boldsymbol{c}$ that we used previously or we could use the package `lsmeans`, which automates much of this.

```{r}
lsmeans( model.1, pairwise ~ variety)
```

This looks remarkably similar to the output from TukeyHSD and just as confusing.  Ideally the function `make_TukeyHSD_letters()` could be modified to handle this model as well.

```{r}
#' Create a data frame with significant groupings 
#'
#' This function runs TukeyHSD on the input model and then creates a data frame
#' with a column for the factor and a second for the Significance Group
#' 
#' @param model The output of a lm(), aov(), or lme().
#' @param variable The variable of interest.
#' @param threshold maximum p-value for statistical significance
#' @output A data frame with a column for factor and another for the signicance group.
#' @examples
#' model <- lm( breaks ~ wool + tension, data=warpbreaks )
#' make_TukeyHSD_letters(model, ~ wool)
#' make_TukeyHSD_letters(model, ~ tension)
#' make_TukeyHSD_letters(model, ~ wool+tension)
make_TukeyHSD_letters <- function(model, formula, threshold=0.05){ 
  formula2 <- update.formula( formula, pairwise ~ .)
  temp <- lsmeans(model, formula2)$contrasts %>%
    summary()  
  temp2 <- temp$p.value
  names(temp2) <- temp$contrast 
  temp <- temp2 %>% vec2mat(sep=' - ') %>% multcompLetters(threshold=threshold)
  out <-  data.frame(group = names(temp$Letters), SigGroup=temp$Letters)
  colnames(out)[1] <-   Reduce(paste, deparse(formula)) %>% str_replace(fixed('~'),'')
  rownames(out) <- NULL
  out
}  
```

```{r}
make_TukeyHSD_letters(model.1, ~ variety)
```

As usual we'll join this information into the original data table and then make a nice summary graph.
```{r}
oatvar <- oatvar %>%
  left_join( make_TukeyHSD_letters(model.1, ~variety) ) %>%
  mutate(LetterHeight=500)

ggplot(oatvar, aes(x=variety, y=yield)) +
  geom_point(aes(color=block)) +
  geom_text(aes(label=SigGroup, y=LetterHeight))
```


************************************************************************

We'll consider a second example using data from the pharmaceutical industry. We are interested in 4 different processes (our treatment variable) used in the biosynthesis and purification of the drug penicillin. The biosynthesis requires a nutrient source (corn steep liquor) as a nutrient source for the fungus and the nutrient source is quite variable. Each batch of the nutrient is is referred to as a 'blend' and each blend is sufficient to create 4 runs of penicillin. We avoid confounding our biosynthesis methods with the blend by using a Randomized Complete Block Design and observing the yield of penicillin from each of the four methods (A,B,D, and D) in each blend.

```{r}
data(penicillin)
ggplot(penicillin, aes(y=yield, x=treat)) +
  geom_point() + 
  facet_wrap( ~ blend, ncol=5)
```

It looks like there is definitely a `Blend` effect (e.g. Blend1 is much better than Blend5) but it isn't clear that there is a treatment effect.

```{r}
model.0 <- lmer(yield ~  1    + (1 | blend), data=penicillin)
model.1 <- lmer(yield ~ treat + (1 | blend), data=penicillin)
anova(model.0, model.1)
```

```{r}
car::Anova(model.1, type=3)
```


It looks like we don't have a significant effect of the treatments. Next we'll examine the simple model to understand the variability.

```{r}
summary(model.0)
```

We see that the noise is more in the within blend rather than the between blends.  If my job were to understand the variability and figure out how to improve production, this suggests that understanding the both how variability is introduced at the blend level *and* at the run level.  The run level has slightly more variability, so I might start there.


## Nested Effects

When the levels of one factor vary only within the levels of another factor, that factor is said to be nested. For example, when measuring the performance of workers at several job locations, if the workers only work at one site, then the workers are nested within site. If the workers work at more than one location, we would say that workers are *crossed* with site.

We've already seen a number of nested designs when we looked at split plot designs. Recall the `AgData` set that I made up that simulated an agricultural experiment with 8 plots and 4 subplots per plot.  We applied an irrigation treatment at the plot level and a fertilizer treatment at the subplot level. I actually have 5 replicate observations per subplot.


```{r, echo=FALSE}
data('AgData')
AgData <- AgData %>% 
  mutate( row=ceiling(as.integer(subplot) / 2),
          col=as.integer(subplot) %% 2 + 1,
          vis.plot = as.integer(plot)) %>%
  group_by(plot) 
ggplot(AgData ) +
   geom_tile(aes(x=col, y=row, fill=Fertilizer),  color='black', size=1) +
   facet_wrap(  ~ plot, labeller=label_both, ncol=4) +
   geom_text( aes(label=paste("Irrigation", Irrigation)), x=1.5, y=2.7) +
   geom_point( aes(x=jitter(col, amount = .3), y=jitter(row, amount=.3))) +
   scale_x_continuous(labels=NULL) + scale_y_continuous(labels=NULL, limits=c(0.5, 2.8)) +
   xlab("") + ylab("")
```

So all together we have 8 plots, 32 subplots, and 5 replicates per subplot. When I analyze the fertilizer, I have 32 experimental units (the thing I have applied my treatment to), but when analyzing the effect of irrigation, I only have 8 experimental units. In other words, I should have 8 random effects for plot, and 32 random effects for subplot. 

```{r}
# The following model definitions are equivalent
model <- lmer(yield ~ Irrigation + Fertilizer + (1|plot) + (1|plot:subplot), data=AgData )
model <- lmer(yield ~ Irrigation + Fertilizer + (1|plot/subplot), data=AgData)
car::Anova(model, type=3)
```
As we saw before, the effect of irrigation is not significant and the fertilizer effect is highly significant. We'll remove the irrigation covariate and refit the model.

```{r}
model <- lmer(yield ~ Fertilizer + (1|plot/subplot), data=AgData)
summary(model)
```
Notice the plant-to-plant noise is about 1/3 of the noise associated with subplot-to-subplot or even plot-to-plot.

**********************************
A number of *in-situ* experiments looking at the addition CO$_2$ and warming on landscapes have been done (typically called Free Air CO$_2$ Experiments (FACE)) and these are interesting from an experimental design perspective because we have limited number of replicates because the cost of exposing plants to different CO$_2$ levels outside a greenhouse is extraordinarily expensive. In the `dsData` package, there is a dataset that is inspired by one of those studies.

The experimental units for the CO$_2$ treatment will be called a ring, and we have nine rings. We have three treatments (A,B,C) which correspond to an elevated CO$_2$ treatment, an ambient CO$_2$ treatment with all the fans, and a pure control. For each ring we'll have some measure of productivity but we have six replicate observations per ring. 

```{r}
data("HierarchicalData", package = 'dsData')
head(HierarchicalData)
```

```{r, echo=FALSE}
# Add some columns so I can plot this nicely
HierarchicalData <- HierarchicalData %>%
  mutate(graph.x = 1, graph.y = 1 )
  
ggplot(HierarchicalData, aes(x=graph.x, y=graph.y, fill=Trt)) +
  geom_tile() +
  facet_wrap(~Ring, labeller = label_both) +
  geom_point(aes(x=jitter(graph.x, amount=.3), y=jitter(graph.y, amount=.3))) +
  scale_x_continuous(breaks=NULL) +
  scale_y_continuous(breaks=NULL) +
  xlab("") + ylab("")
```

We can easily fit this model using random effects for each ring.
```{r}
model <- lmer( y ~ Trt + (1|Ring), data=HierarchicalData )
car::Anova(model, type=3)
```

To think about what is actually going on, it is helpful to consider the predicted values from this model. As usual we will use the `predict` function, but now we have the option of including the random effects or not. 

First lets consider the predicted values if we completely ignore the Ring random effect.

```{r}
HierarchicalData <- HierarchicalData %>%
  mutate( y.hat   = predict(model, re.form= ~ 0),  # don't include any random effects 
          y.hat   = round( y.hat, digits=2),
          my.text = paste('yhat =', y.hat),
          text.height = 1.8)  
```

```{r, echo=FALSE, fig.height=5}
ggplot(HierarchicalData, aes(x=graph.x, y=graph.y, fill=Trt)) +
  geom_tile() +
  facet_wrap(~Ring, labeller = label_both) +
  geom_point(aes(x=jitter(graph.x, amount=.3), y=jitter(graph.y, amount=.3))) +
  scale_x_continuous(breaks=NULL) +
  scale_y_continuous(breaks=NULL) +
  geom_text( aes(label=my.text, y=text.height)) +
  xlab("") + ylab("") +
  ggtitle('Treatment Only Predictions')
```

Now we consider the predicted values, but created using the Ring random effect.  These random effects provide for a slight perturbation up or down depending on the quality of the Ring, but the sum of all 9 Ring effects is *required* to be 0.

```{r}
ranef(model)
sum(ranef(model)$Ring)
```
Also notice that the sum of the random effects *within a treatment* is zero! (Recall Ring 1:3 was treatment A, 4:6 was treatment B, and 7:9 was treatment C).

```{r}
HierarchicalData <- HierarchicalData %>%
  mutate( y.hat   = predict(model, re.form= ~ (1|Ring)),  # Include Ring Random effect
          y.hat   = round( y.hat, digits=2),
          my.text = paste('yhat =', y.hat),
          text.height = 1.8)  
```

```{r, echo=FALSE, fig.height=5}
ggplot(HierarchicalData, aes(x=graph.x, y=graph.y, fill=Trt)) +
  geom_tile() +
  facet_wrap(~Ring, labeller = label_both) +
  geom_point(aes(x=jitter(graph.x, amount=.3), y=jitter(graph.y, amount=.3))) +
  scale_x_continuous(breaks=NULL) +
  scale_y_continuous(breaks=NULL) +
  geom_text( aes(label=my.text, y=text.height)) +
  xlab("") + ylab("") +
  ggtitle('Treatment and Ring Predictions')
```

We interpret the random effect of Ring as a perturbation to expected value of the response that you expect just based on the treatment provided.

********************************

We'll now consider an example with a somewhat ridiculous amount of nesting. We will consider an experiment run to test the consistency between laboratories. A large jar of dried egg power was fully homogenized and divided into a number of samples and the fat content between the samples should be the same. Six laboratories were randomly selected and each lab would receive 4 samples, two labeled H and two labeled G. The labs are instructed to give two samples to two different technicians who are to divide each sample into two sub-samples and measures the fat content twice within a sub sample. So our hierarchy is that observations are nested within sub-samples which are nested within technicians which are nested in labs.

In terms of notation, we will refer to the 6 labs as $L_{i}$ and the lab technicians as $T_{ij}$ and we note that $j$ is either 1 or 2 which doesn't uniquely identify the technician unless we include the lab subscript as well. Finally the sub-samples are nested within the technicians and we denote them as $S_{ijk}$. Finally our “pure” error is the two measurements from the same sub-sample. So the model we wish to fit is:
$$y_{ijkl}=\mu+L_{i}+T_{ij}+S_{ijk}+\epsilon_{ijkl}$$
where $L_{i}\stackrel{iid}{\sim}N\left(0,\sigma_{L}^{2}\right)$, $T_{ij}\stackrel{iid}{\sim}N\left(0,\sigma_{T}^{2}\right)$, $S_{ijk}\stackrel{iid}{\sim}N\left(0,\sigma_{S}^{2}\right)$, $\epsilon_{ijkl}\stackrel{iid}{\sim}N\left(0,\sigma_{\epsilon}^{2}\right)$. 

We need a convenient way to tell `lmer` which factors are nested in which. We can do this by creating data columns that make the interaction terms. For example there are 12 technicians (2 from each lab), but in our data frame we only see two levels, so to create all 12 random effects, we need to create an interaction column (or tell `lmer` to create it and use it). Likewise there are 24 sub-samples and 48 “pure” random effects.

```{r}
data(eggs, package='faraway')
model <- lmer( Fat ~ 1 + (1|Lab) + (1|Lab:Technician) +
                     (1|Lab:Technician:Sample), data=eggs)
model <- lmer( Fat ~ 1 + (1|Lab/Technician/Sample), data=eggs)
```

```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~0))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average Value Only')
```


```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~(1|Lab)))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average With Lab Offset')
```

```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~(1|Lab/Technician)))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average With Lab + Technician Offset')
```

```{r}
eggs <- eggs %>% 
  mutate( yhat = predict(model, re.form=~(1|Lab/Technician/Sample)))
ggplot(eggs, aes(x=Sample, y=Fat)) + 
  geom_point() +
  geom_line(aes(y=yhat, x=as.integer(Sample)), color='red') +
  facet_grid(. ~ Lab+Technician) +
  ggtitle('Average With Lab + Technician + Sample Offset')
```

No that we have an idea of how things vary, we can look at the summary table.
```{r}
summary(model)
```

## Crossed Effects
If two effects are not nested, we say they are *crossed(. In the penicillin example, the treatments and blends were not nested and are therefore crossed. 

An example is a Latin square experiment to look the effects of abrasion on four different material types (A, B, C, and D). We have a machine to do the abrasion test with four positions and we did 4 different machine runs. Our data looks like the following setup:
```{r, echo=FALSE}
data(abrasion, package='faraway')
abrasion %>% dplyr::select(run,position,material) %>%
  mutate(position = paste('Position:',position)) %>%
  spread(key=position, value=material) %>%
  pander()
```
Our model can be written as $$y_{ijk}=\mu+M_{i}+P_{j}+R_{k}+\epsilon_{ijk}$$
and we notice that the position and run effects are not nested within anything else and thus the subscript have just a single index variable. Certainly the run effect should be considered random as these four are a sample from all possible runs, but what about the position variable? Here we consider that the machine being used is a random selection from all possible abrasion machines and any position differences have likely developed over time and could be considered as a random sample of possible position effects. We'll regard both position and run as crossed random effects.

```{r}
data(abrasion, package='faraway')
ggplot(abrasion, aes(x=material, y=wear, color=position, shape=run)) +
  geom_point(size=3)
```

It certainly looks like the materials are different.  I don't think the run matters, but position 2 seems to develop excessive wear compared to the other positions.

```{r}
m <- lmer( wear ~ material + (1|run) + (1|position), data=abrasion)
car::Anova(m, type=3)
```

The material effect is statistically significant and we can figure out the pairwise differences in the usual fashion.
```{r}
make_TukeyHSD_letters(m, ~ material)
```
So material D is in between materials B and C for abrasion resistance.

```{r}
summary(m)
```
Notice that run and the pure error have about the same magnitude, but position is more substantial. Lets see what happens if we remove the run effect.

```{r}
m2 <- lmer( wear ~ material + (1|position), data=abrasion)
anova(m, m2)
```

Notice that R is refitting the model to make an appropriate comparison. The AIC difference between the two models is about 3 units (the larger model have a lower AIC) and so we could interpret this as decent evidence for a run effect.  Similarly the Likelihood Ratio Test gives a p-value of about $0.02$. So while the run effect wasn't visible in our initial graph, it looks like it is a statistically significant effect.

## Repeated Measures / Longitudinal Studies

In repeated measurement experiments, repeated observations are taken on each subject. When those repeated measurements are taken over a sequence of time, we call it a longitudinal study. Typically covariates are also observed at the same time points and we are interested in how the response is related to the covariates. 

In this case the correlation structure is that observations on the same person/object should be more similar than observations between two people/objects.  As a result we need to account for repeated measures by including the person/object as a random effect.

To demonstrate a longitudinal study we turn to the data set `sleepstudy` in the `lme4` library. Eighteen patients participated in a study in which they were allowed only 3 hours of sleep per night and their reaction time in a specific test was observed. On day zero (before any sleep deprivation occurred) their reaction times were recorded and then the measurement was repeated on 9 subsequent days.

```{r}
data(sleepstudy, package='lme4')
ggplot(sleepstudy, aes(y=Reaction, x=Days)) +
    facet_wrap(~ Subject, ncol=6) + 
    geom_point() + 
    geom_line()
```

We want to fit a line to these data, but how should we do this? First we notice that each subject has their own baseline for reaction time and the subsequent measurements are relative to this, so it is clear that we should fit a model with a random intercept.

```{r}
m1 <- lmer( Reaction ~ Days + (1|Subject), data=sleepstudy)
summary(m1)
ranef(m1)
```

To visualize how well this model fits our data, we will plot the predicted values which are lines with y-intercepts that are equal to the sum of the fixed effect of intercept and the random intercept per subject. The slope for each patient is assumed to be the same and is approximately $10.4$.

```{r}
sleepstudy <- sleepstudy %>% 
  mutate(yhat = predict(m1, re.form=~(1|Subject)))
ggplot(sleepstudy, aes(y=Reaction, x=Days)) +
    facet_wrap(~ Subject, ncol=6) + 
    geom_point() + 
    geom_line() +
    geom_line(aes(y=yhat), color='red')
```

This isn't too bad, but I would really like to have each patient have their own slope as well as their own y-intercept. The random slope will be calculated as a fixed effect of slope plus a random offset from that.

```{r}
# Random effects for intercept and Slope 
m2 <- lmer( Reaction ~ Days + ( 1+Days | Subject), data=sleepstudy)

sleepstudy <- sleepstudy %>% 
  mutate(yhat = predict(m2, re.form=~(1+Days|Subject)))
ggplot(sleepstudy, aes(y=Reaction, x=Days)) +
    facet_wrap(~ Subject, ncol=6) + 
    geom_point() + 
    geom_line() +
    geom_line(aes(y=yhat), color='red')

```

This appears to fit the observed data quite a bit better, but it is useful to test this.
```{r}
anova(m1, m2)
```

Here we see that indeed the random effect for each subject in both y-intercept and in slope is a better model that just a random offset in y-intercept.



It is instructive to look at this example from the top down. First we plot the population regression line.
```{r}
sleepstudy <- sleepstudy %>% 
  mutate(yhat = predict(m2, re.form=~0))
ggplot(sleepstudy, aes(x=Days, y=yhat)) +
  geom_line(color='red') + ylab('Reaction') +
  ggtitle('Population Estimated Regression Curve')
```

```{r}
sleepstudy <- sleepstudy %>% 
  mutate(yhat.ind = predict(m2, re.form=~(1+Days|Subject)))
ggplot(sleepstudy, aes(x=Days)) +
  geom_line(aes(y=yhat), size=3) + 
  geom_line(aes(y=yhat.ind, group=Subject), color='red') +
  ylab('Reaction') + ggtitle('Person-to-Person Variation')
```

```{r}
ggplot(sleepstudy, aes(x=Days)) +
  geom_line(aes(y=yhat)) + 
  geom_line(aes(y=yhat.ind, group=Subject), color='red') +
  ylab('Reaction') + ggtitle('Within Person Variation') +
  facet_wrap(~ Subject, ncol=6) + 
  geom_point(aes(y=Reaction))
```

Finally we want to go back and look at the coefficients for the complex model.
```{r}
summary(m2)
```



## Exercises
1. An experiment was conducted to determine the effect of recipe and baking temperature on chocolate cake quality. For each recipe, $15$ batches of cake mix for were prepared (so 45 batches total). Each batch was sufficient for six cakes. Each of the six cakes was baked at a different temperature which was randomly assigned. Several measures of cake quality were recorded of which breaking angle was just one. The dataset is available in the `faraway` package as `choccake`.
    a. For the variables Temperature, Recipe, and Batch, which should be fixed and which should be random?
    b. Inspect the data. How many levels of batch are there and how will that influence your model statements in R?
    c. Build a mixed model using the main effects (no interactions). 
    d. Compare your model in part (c) one models with one or both of the fixed effects removed. Which model is preferred?
    e. Compare your model in part (c) with a more complicated model that includes the interaction between temperature and recipe. Which model is preferred? 
    f. Using the model you selected, discuss the impact of the different variance components.

2. An experiment was conducted to select the supplier of raw materials for production of a component. The breaking strength of the component was the objective of interest. Raw materials from four suppliers were considered. In our factory, we have four operators that can only produce one component per day. We utilized a Latin square design so that each factory operator worked with a different supplier each day. The data set is presented in the `faraway` package as `breaking`.
    a. Explain why it would be natural to treat the operators and days as random effects but the suppliers as fixed effects.
    b. Inspect the data? Does anything seem weird? It turns out that the person responsible for entering the data made an input error.  Fix it making sure to preserve that each day has all 4 suppliers and 4 operators.
    c. Build a model to predict the breaking strength. Describe the variation from operator to operator and from day to day.
    d. Test the significance of the supplier effect.
    e. Is there a significant difference between the operators?

3. An experiment was performed to investigate the effect of ingestion of thyroxine or thiouracil. The researchers took 27 rats and divided them into three groups.  The control group is ten rats that receive no addition to their drinking water. A second group of seven rats has thyroxine added to their drinking water and the final set ten rats have thiouracil added to their water. For each of five weeks, we take a body weight measurement to monitor the rats' growth.
*I suspect that we had 30 rats to begin with and somehow three rats in the thyroxine group had some issue unrelated to the treatment.* The following R code might be helpful for the initial visualization.
    ```{r, eval=FALSE}
    # we need to force ggplot to only draw lines between points for the same
    # rat.  If I haven't already defined some aesthetic that is different
    # for each rat, then it will connect points at the same week but for different
    # rats. The solution is to add an aesthetic that does the equivalent of the
    # dplyr function group_by(). In ggplot2, this aesetheic is "group". 
    ggplot(ratdrink, aes(y=wt, x=weeks, color=treat)) +    
      geom_point(aes(shape=treat)) +
      geom_line(aes(group=subject))  # play with removing the group=subject aesthetic...
    ```
    a. Consider the model with an interaction between Treatment and Week along with a random effect for each subject rat. Does the model with a random offset in the y-intercept perform as well as the model with random offsets in both the y-intercept and slope?
    b. Next consider if you can simplify the model by removing the interaction between Treatment and Week and possibly even the Treatment main effect.     
    c. Comment on the effect of each treatment. 

<!--chapter:end:11_RandomEffects.Rmd-->

---
output:
  pdf_document: default
  html_document: default
---
# Binomial Regression

```{r, results='hide', echo=FALSE}
knitr::opts_chunk$set(echo = TRUE, 
                      fig.height=3)
```
```{r, message=FALSE, warning=FALSE}
# Usual library loading stuff
library(multcomp); library(multcompView)
library(lsmeans)
library(MASS)
library(faraway)
library(ggplot2)
library(dplyr)
```


The general linear model assumes that the observed data is distributed 
$$\boldsymbol{y} = \boldsymbol{X\beta} + \boldsymbol{\epsilon} \;\; \textrm{ where } \epsilon_i \stackrel{iid}{\sim} N(0,\sigma^2)$$
which can be re-written as
$$\boldsymbol{y}\sim N\left(\boldsymbol{\mu}=\boldsymbol{X\beta},\sigma^{2}\boldsymbol{I}\right)$$
and notably this assumes that the data are independent. This model has $E\left[\boldsymbol{y}\right]=\boldsymbol{X\beta}$. This model is quite flexible and includes: 


+--------------------------+-----------------------------+-----------------------------+
|   Model                  |  Predictor Type             | Response                    |
+==========================+=============================+=============================+
| Simple Linear Regression | 1 Continuous                | Continuous Normal Response  |
| 1-way ANOVA              | 1 Categorical               | Continuous Normal Response  |
| 2-way ANOVA              | 2 Categorical               | Continuous Normal Response  |
| ANCOVA                   | 1 Continuous, 1 Categorical | Continuous Normal Response  |
+--------------------------+-----------------------------+-----------------------------+

The general linear model expanded on the linear model and we allow the data points to be correlated 
$$\boldsymbol{y}\sim N\left(\boldsymbol{X\beta},\sigma^{2}\boldsymbol{\Omega}\right)$$
where we assume that $\boldsymbol{\Omega}$ has some known form but may include some unknown correlation parameters. This type of model includes our work with mixed models and time series data.

The study of generalized linear models removes the assumption that the error terms are normally distributed and allows the data to be distributed according to some other distribution such as Binomial, Poisson, or Exponential. These distributions are parameterized differently than the normal (instead of $\mu$ and $\sigma$, we might be interested in $\lambda$ or $p$). However, I am still interested in how my covariates can be used to estimate my parameter of interest.

Critically, I still want to parameterize my covariates as $\boldsymbol{X\beta}$ because we understand the how continuous and discrete covariates added and interpreted and what interactions between them mean. By keeping the $\boldsymbol{X\beta}$ part, we continue to build on the earlier foundations.

## Binomial Regression Model

To remove a layer of abstraction, we will now consider the case of binary regression. In this model, the observations (which we denote by $w_{i}$) are zeros and ones which correspond to some binary observation, perhaps presence/absence of an animal in a plot, or the success or failure of an viral infection. Recall that we could model this as $W_{i}\sim Bernoulli\left(p_{i}\right)$ random variable. 
$$P\left(W_{i}=1\right)	=	p_{i}$$
$$P\left(W_{i}=0\right)	=	\left(1-p_{i}\right)$$
which I can rewrite more formally letting $w_{i}$ be the observed value as $$P\left(W_{i}=w_{i}\right)=p_{i}^{w_{i}}\left(1-p_{i}\right)^{1-w_{i}}$$ and the parameter that I wish to estimate and understand is the probability of a success $p_{i}$ and usually I wish to know how my covariate data $\boldsymbol{X\beta}$ informs these probabilities. 

In the normal distribution case, we estimated the expected value of my response vector ($\boldsymbol{\mu}$) simply using $\hat{\boldsymbol{\mu}}=\boldsymbol{X}\hat{\boldsymbol{\beta}}$ but this will not work for an estimate of $\hat{\boldsymbol{p}}$ because there is no constraint on $\boldsymbol{X}\hat{\boldsymbol{\beta}}$, there is nothing to prevent it from being negative or greater than 1. Because we require the probability of success to be a number between 0 and 1, I have a problem.

Example: Suppose we are interested in the abundance of mayflies in a stream. Because mayflies are sensitive to metal pollution, I might be interested in looking at the presence/absence of mayflies in a stream relative to a pollution gradient. Here the pollution gradient is measured in Cumulative Criterion Units (CCU: CCU is defined as the ratio of the measured metal concentration to the hardness adjusted chronic criterion concentration, and then summed across each metal) where larger values imply more metal pollution. 

```{r}
data(Mayflies, package='dsData')
ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + geom_point()
```

If I just fit a regular linear model to this data, we fit the following:

```{r}
m <- lm( Occupancy ~ CCU, data=Mayflies )
Mayflies <- Mayflies %>% mutate(yhat = predict(m))
ggplot(Mayflies, aes(x=CCU, y=Occupancy)) + 
  geom_point() +
  geom_line(aes(y=yhat), color='red')
```

which is horrible. First, we want the regression line to be related to the probability of occurrence and it is giving me a negative value. Instead, we want it to slowly tail off and give me more of an sigmoid-shaped curve. Perhaps something more like the following:
```{r, echo=FALSE}
m <- glm( Occupancy ~ CCU, data=Mayflies, family=binomial ) 
predicted <- data.frame(CCU = seq(0,5, by=.01)) 
predicted$prob <- ilogit(predict(m, newdata=data.frame(CCU=seq(0,5,by=.01)))) 
ggplot(Mayflies, aes(x=CCU, y=Occupancy)) +    
  geom_point() +   
  geom_line(data=predicted, aes(y=prob), color='red')
```


We need a way to convert our covariate data $\boldsymbol{y}=\boldsymbol{X\beta}$ from something that can take values from $-\infty$ to $+\infty$ to something that is constrained between 0 and 1 so that we can fit the model
$$w_{i}\sim Bernoulli\left(\underset{\textrm{in }[0,1]}{\underbrace{g^{-1}\left(\underset{\textrm{in }\left[-\infty,\infty\right]}{\underbrace{y_{i}}}\right)}}\right)$$
There are several options for the link function $g^{-1}\left(\cdot\right)$ that are commonly used. We use the notation $y_{i}=\boldsymbol{X}_{i,\cdot}\boldsymbol{\beta}$ is unconstrained and can be in $\left(-\infty, +\infty\right)$ while $p_{i}=g^{-1}\left(y_{i}\right)$ is constrained to $\left[0,1\right]$. When convenient, we will drop the $i$
subscript while keeping the domain restrictions.

1. Logit (log odds) transformation. The link function is $$g\left(p\right)=\log\left[\underset{\textrm{odds}}{\underbrace{\frac{p}{1-p}}}\right]=y$$
 with inverse $$g^{-1}\left(y\right)=\frac{1}{1+e^{-y}}$$ and we think of $g\left(p\right)$ as the log odds function. 

2. Probit transformation. The link function is $g\left(p\right)=\Phi^{-1}\left(\boldsymbol{p}\right)$ where $\Phi$ is the standard normal cumulative distribution function and therefore $g^{-1}\left(\boldsymbol{X}\boldsymbol{\beta}\right)=\Phi\left(\boldsymbol{X}\boldsymbol{\beta}\right)$.

3. Complementary log-log transformation: $g\left(p\right)=\log\left[-\log(1-\boldsymbol{p})\right]$.

All of these functions will give a sigmoid shape with higher probability as $y$ increases and lower probability as it decreases. The logit and probit transformations have the nice property that if $y=0$ then $g^{-1}\left(0\right)=\frac{1}{2}$.

Usually the difference in inferences made using these different curves is relatively small and we will usually use the logit transformation because its form lends itself to a nice interpretation of my $\boldsymbol{\beta}$ values. In these cases, a slope parameter in our model will be interpreted as “the change in log odds for every one unit change in the predictor.”

Because we will be using the logit transformation so often, it is useful to make the following definitions: 
$$\textrm{logit}\left(p\right)	=	\log\left[\frac{p}{1-p}\right]$$
$$\textrm{ilogit}\left(y\right)	=	\frac{1}{1+e^{-y}}$$
where $p\in\left[0,1\right]$ and $y\in\left(-\infty,+\infty\right)$.

As in the mixed model case, there are no closed form solution for $\hat{\boldsymbol{\beta}}$ and instead we must rely on numerical solutions to find the maximum likelihood estimators for $\hat{\boldsymbol{\beta}}$. To do this, we must derive the log-likelihood function.

$$\begin{aligned}
 \log\left[L\left(\boldsymbol{\beta}|\boldsymbol{w}\right)\right]	&=	\log\left[\prod_{i=1}^{n}L\left(\boldsymbol{\beta}|w_{i}\right)\right] \\
	&=	\sum_{i=1}^{n}\log L\left(\boldsymbol{\beta}|w_{i}\right) \\
	&=	\sum_{i=1}^{n}\log P\left(w_{i}|\boldsymbol{\beta}\right) \\
	&=	\sum_{i=1}^{n}\log\left[p_{i}^{wi}\left(1-p_{i}\right)^{1-w_{i}}\right]
	\end{aligned}$$
and we recognize that 
$$p_{i}=\textrm{ilogit}\left(\boldsymbol{X\beta}\right)=1/\left(1+e^{-\boldsymbol{X\beta}}\right)$$
and we substitute that into the equation and simplify. Fortunately we don't have to worry about the details of maximizing the log-likelihood function as R will do it for us. 

Often we have more than one response at a particular level of $\boldsymbol{X}$. Let $n_{i}$ be the number of observations observed at the particular value of $\boldsymbol{X}$, and $y_{i}$ be the proportion of successes at that value of $\boldsymbol{X}$. In that case, $w_{i}$ is not a Bernoulli random variable, but rather a binomial random variable. Note that the Bernoulli distribution is the special case of the binomial distribution with $n_{i}=1$.

```{r}
# The following are equivalent
m1 <- glm( cbind(Occupancy, 1-Occupancy) ~ CCU, data=Mayflies, family=binomial )
m1 <- glm(                     Occupancy ~ CCU, data=Mayflies, family=binomial )
```


For binomial response data, we need to know the number of successes and the number of failures at each level of our covariate. In this case it is quite simple because there is only one observation at each CCU level, so the number of successes is Occupancy and the number of failures is just 1-Occupancy. For binomial data, `glm` expect the response to be a two-column matrix where the first column is the number successes and and the second column is the number of failures. The default choice of link function for binomial data is the logit link, but the probit can be easily chosen as well using `family=binomial(link=probit)` in the call to `glm()`. If you only give a single response vector, it is assumed that the second column is to be calculated as 1-first.column.

```{r}
summary(m1)
```

Notice that the summary table includes an estimate of the standard error of each $\hat{\beta}_{j}$ and a standardized value and z-test that are calculated in the usual manner $z_{j}=\frac{\hat{\beta}_{j}-0}{StdErr\left(\hat{\beta}_{j}\right)}$ but these only approximately follow a standard normal distribution (due to the CLT results for Maximum Likelihood Estimators). We should regard the p-values given as approximate.

The sigmoid curve shown prior was the result of the logit model and we can estimate the probability of occupancy for any value of CCU. Surprisingly, R does not have a built-in function for the logit and ilogit function, but the faraway package does include them.

```{r}
new.df <- data.frame(CCU=1)
ilogit( predict(m1, newdata=new.df) )          # The following are the
predict(m1, newdata=new.df, type='response')   # same result

new.df <- data.frame( CCU=seq(0,5, by=.01) )
yhat.df <- new.df %>% mutate(fit = ilogit( predict(m1, newdata=new.df) ) )
ggplot(Mayflies, aes(x=CCU)) +
  geom_point(aes(y=Occupancy)) +
  geom_line( data=yhat.df, aes(y=fit), color='red') 
```



## Deviance

In the normal linear models case, we were very interested in the Sum of Squared Error (SSE)
$$SSE=\sum_{i=1}^{n}\left(w_{i}-\hat{w}_{i}\right)^{2}$$
because it provided a mechanism for comparing the fit of two different models. If a model had a very small SSE, then it fit the observed data well. We used this as a basis for forming our F-test to compare nested models (some rescaling by the appropriate degrees of freedom was necessary, though).

We want an equivalent measure of goodness-of-fit for models that are non-normal, but in the normal case, I would like it to be related to my SSE statistic.

The deviance of a model with respect to some data $\boldsymbol{y}$ is defined by
$$D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{0}\right) = -2\left[\log L\left(\hat{\boldsymbol{\theta}}_{0}|\boldsymbol{w}\right)-\log L\left(\hat{\boldsymbol{\theta}}_{S}|\boldsymbol{w}\right)\right]$$
where $\hat{\boldsymbol{\theta}}_{0}$ are the fitted parameters of the model of interest, and $\hat{\boldsymbol{\theta}}_{S}$ are the fitted parameters under a “saturated” model that has as many parameters as it has observations and can therefore fit the data perfectly. Thus the deviance is a measure of deviation from a perfect model and is flexible enough to handle non-normal distributions appropriately. 

Notice that this definition is very similar to what is calculated during the Likelihood Ratio Test. For any two models under consideration, the LRT can be formed by looking at the difference of the deviances of the two nested models
$$LRT=D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{simple}\right)-D\left(\boldsymbol{w},\hat{\boldsymbol{\theta}}_{complex}\right)\stackrel{\cdot}{\sim}\chi_{df_{complex}-df_{simple}}^{2}$$
 

```{r}
m0 <- glm( Occupancy ~ 1, data=Mayflies, family=binomial )
anova(m0, m1)
1 - pchisq( 22.146, df=1 )
```

A convenient way to get R to calculate the LRT $\chi^{2}$ p-value for you is to use the `drop1()` function.

```{r}
drop1(m1, test='Chi')
```

The inference of this can be confirmed by looking at the AIC values of the two models as well.
```{r}
AIC(m0, m1)
```


## Goodness of Fit

The deviance is a good way to measure if a model fits the data, but it is not the only method. Pearson's $X^{2}$ statistic is also applicable. This statistic takes the general form $X^{2}=\sum_{i=1}^{n}\frac{\left(O_{i}-E_{i}\right)^{2}}{E_{i}}$ where $O_{i}$ is the number of observations observed in category $i$ and $E_{i}$ is the number expected in category $i$. In our case we need to figure out the categories we have. Since we have both the number of success and failures, we'll have two categories per observation $i$. 
$$X^{2}	=	\sum_{i=1}^{n}\left[\frac{\left(w_{i}-n_{i}\hat{p}_{i}\right)^{2}}{n_{i}\hat{p}_{i}}+\frac{\left(\left(n_{i}-w_{i}\right)-n_{i}\left(1-\hat{p}_{i}\right)\right)^{2}}{n_{i}\left(1-\hat{p}_{i}\right)}\right]
	=	\sum_{i=1}^{n}\frac{\left(w_{i}-n_{i}\hat{p}_{i}\right)^{2}}{n_{i}\hat{p}_{i}\left(1-\hat{p}_{i}\right)}$$
and the Pearson residual can be defined as
$$r_{i}=\frac{w_{i}-n_{i}\hat{p}_{i}}{\sqrt{n_{i}\hat{p}_{i}\left(1-\hat{p}_{i}\right)}}$$
 
These can be found in R via the following commands
```{r}
sum( residuals(m1, type='pearson')^2 )
```

Pearson's $X^{2}$ statistic is quite similar to the deviance statistic
```{r}
deviance(m1)
```


## Confidence Intervals

Confidence intervals for the regression could be constructed using normal approximations for the parameter estimates. An approximate $100\left(1-\alpha\right)\%$ confidence interval for $\beta_{i}$ would be 
$$\hat{\beta}_{i}\pm z^{1-\alpha/2}\,StdErr\left(\hat{\beta}_{i}\right)$$
but we know that this is not a good approximation because the the normal approximation will not be good for small sample sizes and it isn't clear what is “big enough”. Instead we will use an inverted LRT to develop confidence intervals for the $\beta_{i}$ parameters. 

We first consider the simplest case, where we have only an intercept and slope parameter. Below is a contour plot of the likelihood surface and the shaded region is the region of the parameter space where the parameters $\left(\beta_{0},\beta_{1}\right)$ would not be rejected by the LRT. This region is found by finding the maximum likelihood estimators $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$, and then finding set of $\beta_{0},\beta_{1}$ pairs such that
$$\begin{aligned}
-2\left[\log L\left(\beta_{0},\beta_{1}\right)-\log L\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)\right]	& \le \chi_{df=2,0.95}^{2} \\
\log L\left(\beta_{0},\beta_{1}\right)	&\ge	-2\chi_{2,0.95}^{2}+\log L\left(\hat{\beta}_{0},\hat{\beta}_{1}\right)
\end{aligned}$$
  
```{r, Profile_CI, echo=FALSE, cache=TRUE}
library(ggplot2)
library(mvtnorm)
n <- 201 
b0 <- seq(0,4, length=n) 
b1 <- seq(0,4, length=n) 
data <- expand.grid(b0=b0, b1=b1) 
data$z <- apply(data, 1, FUN=dmvnorm, mean=c(2,2), sigma=2*matrix(c(1,-.9,-.9,2),nrow=2))
good.data <- data[ which(abs(data$z)>.06), ] 
ggplot(data, aes(x=b0, y=b1)) + 
    geom_tile(data=good.data, fill='red') + 
    stat_contour(bins=20, aes(z=z)) + 
    geom_text(x=2, y=2, label='Confidence Region') + 
    ggtitle('Likelihood Surface') + 
    xlab(expression(beta[0])) + ylab(expression(beta[1])) +
    geom_line(data=data.frame(b0=c(1.15,1.15), b1=c(0,2.8), z=c(0,0)), col='red', size=2) +
    geom_line(data=data.frame(b0=c(2.85,2.85), b1=c(0,1.2), z=c(0,0)), col='red', size=2) 
```



Looking at just the $\beta_{0}$ axis, this translates into a confidence interval of $(1.15,\, 2.85)$. This method is commonly referred to as the “profile likelihood” interval because the interval is created by viewing the contour plot from the one axis. The physical analogy is to viewing a mountain range from afar and asking, "What parts of the mountain are higher than 8000 feet?"

This type of confidence interval is more robust than the normal approximation and should be used whenever practical. In R, the profile likelihood confidence interval for `glm` objects is available in the `MASS` library. 

```{r}

confint(m1) # using defaults
confint(m1, level=.95, parm='CCU') # Just the slope parameter
```

## Interpreting model coefficients

We first consider why we are dealing with odds $\frac{p}{1-p}$ instead of just $p$. They contain the same information, so the choice is somewhat arbitrary, however we've been using probabilities for so long that it feels unnatural to switch to odds. There are two good reasons for this, however.

The first is that the odds $\frac{p}{1-p}$ can take on any value from $0$ to $\infty$ and so part of our translation of $p$ to an unrestricted domain is already done.

The second is that it is easier to compare odds than to compare probabilities. For example, (as of this writing) I have a three month old baby who is prone to spitting up her milk. 

* I think the probability that she will not spit up on me today is $p_{1}=0.10$. My wife disagrees and believes the probability is $p_{2}=0.01$. We can look at those probabilities and recognize that we differ in our assessment by a factor of $10$ because $10=p_{1}/p_{2}$. If we had assessed the chance of her spitting up using odds, I would have calculated $o_{1}=0.1/0.9=1/9$. My wife, on the other hand, would have calculated $o_{2}=.01/.99=1/99$. The odds ratio of these is $\left[1/9\right] / \left[1/99\right] = 99/9 =11$. This shows that she is much more certain that the event will not happen and the multiplying factor of the pair of odds is 11.

* But what if we were to consider the probability that my daughter will spit up? The probabilities assigned by me versus my wife are $p_{1}=0.9$ and $p_{2}=0.99$. How should I assess that our probabilities differ by a factor of 10, because $p_{1}/p_{2}=0.91\ne10$? The odds ratio remains the same calculation, however. The odds I would give are $o_1=.9/.1=9$ vs my wife's odds $o_2 = .99/.01 = 99$. The odds ratio is now $9/99=1/11$ and gives the same information as I calculated from the where we defined a success as my daughter not spitting up.

To try to clear up the verbage we'll consider a few different cases:

+----------------+-------------------------------------------+------------------------+
|  Probablity    |               Odds                        |      Verbage           |
+================+===========================================+========================+
|   $p=.95$      |  $\frac{95}{5} = \frac{19}{1} = 19$       |   19 to 1 odds for     |
+----------------+-------------------------------------------+------------------------+
|   $p=.75$      |  $\frac{75}{25} = \frac{3}{1} = 3$        |   3 to 1 odds for      |
+----------------+-------------------------------------------+------------------------+
|   $p=.50$      |  $\frac{50}{50} = \frac{1}{1} = 1$        |   1 to 1 odds          |
+----------------+-------------------------------------------+------------------------+
|   $p=.25$      |  $\frac{25}{75} = \frac{1}{3} = 0.33$     |  3 to 1 odds against   |
+----------------+-------------------------------------------+------------------------+
|   $p=.05$      |  $\frac{95}{5}  = \frac{1}{19} = 0.0526$  |  19 to 1 odds against  |
+----------------+-------------------------------------------+------------------------+


Given a logistic regression model with two continuous covariates, then using the `logit()` link function we have $$\log\left(\frac{p}{1-p}\right)	=	\beta_{0}+\beta_{1}x_{1}+\beta_{2}x_{2}$$
$$\frac{p}{1-p}	=	e^{\beta_{0}}e^{\beta_{1}x_{1}}e^{\beta_{2}x_{2}}$$
and we can interpret $\beta_{1}$ and $\beta_{2}$ as the increase in the log odds for every unit increase in $x_{1}$ and $x_{2}$. We could alternatively interpret $\beta_{1}$ and $\beta_{2}$ using the notion that a one unit change in $x_{1}$ as a percent change of $e^{\beta_{1}}$ in the odds. That is to say, $e^{\beta_{1}}$ is the odds ratio of that change. 

To investigate how to interpret these effects, we will consider an example of the rates of respiratory disease of babies in the first year based on covariates of gender and feeding method (breast milk, formula from a bottle, or a combination of the two). The data percentages of babies suffering respiratory disease are

+---------------+------------------+------------------+------------------------------+
|               | Formula `f`      | Breast Milk `b`  | Breast Milk + Suppliment `s` |
+===============+==================+==================+==============================+
|  Males `M`    | $\frac{77}{485}$ | $\frac{47}{494}$ | $\frac{19}{147}$             |
+---------------+------------------+------------------+------------------------------+
|  Females `F`  | $\frac{48}{384}$ | $\frac{31}{464}$ | $\frac{16}{127}$             |
+---------------+------------------+------------------+------------------------------+
 

We can fit the saturated model (6 parameters to fit 6 different probabilities) as
```{r}
data(babyfood, package='faraway')
babyfood
```
```{r}
m2 <- glm( cbind(disease,nondisease) ~ sex * food, family=binomial, data=babyfood )
summary(m2)
```


It is nice to look at the single term deletions to see if the interaction term could be dropped from the model.
```{r}
drop1(m2, test='Chi')
```


Given this, we will look use the reduced model with out the interaction and check if we could reduce the model any more.
```{r}
m1 <- glm( cbind(disease, nondisease) ~ sex + food, family=binomial, data=babyfood)
drop1(m1, test='Chi')
```

From this we see that we cannot reduce the model any more and we will interpret the coefficients of this model.
```{r}
coef(m1, digits=5)  # more accuracy
```

We interpret the intercept term as the log odds that a male child fed only formula will develop a respiratory disease in their first year. With that, we could then calculate what the probability of a male formula fed baby developing respiratory disease using following
$$-1.6127=\log\left(\frac{p_{M,f}}{1-p_{M,f}}\right)=\textrm{logit}\left(p_{M,f}\right)$$
thus
$$p_{M,f}=\textrm{ilogit}\left(-1.6127\right)=\frac{1}{1+e^{1.6127}}=0.1662$$
We notice that the odds of respiratory disease disease is
$$\frac{p_{M,f}}{1-p_{M,f}}=\frac{0.1662}{1-0.1662}=0.1993=e^{-1.613}$$
 
For a female child bottle fed only formula, their probability of developing respiratory disease is $$p_{F,f}=\frac{1}{1+e^{-(-1.6127-0.3126)}}=\frac{1}{1+e^{1.9253}}=0.1273$$
 
and the associated odds are 
$$\frac{p_{F,f}}{1-p_{F,f}}=\frac{0.1273}{1-0.1273}=0.1458=e^{-1.6127-0.3126}$$
so we can interpret $e^{-0.3126}=0.7315$ as the percent change in odds from male to female infants. That is to say, it is the *odds ratio* of the female infants to the males is $$e^{-0.3126}=\frac{\left(\frac{p_{F,f}}{1-p_{F,f}}\right)}{\left(\frac{p_{M,f}}{1-p_{M,f}}\right)}=\frac{0.1458}{0.1993}=0.7315$$

The interpretation here is that odds of respiratory infection for females is 73.1\% than that of a similarly feed male child and I might say that being female reduces the odds of respiratory illness by $27\%$ compared to male babies.. Similarly we can calculate the change in odds ratio for the feeding types: 

```{r}
exp( coef(m1) )
```

First we notice that the intercept term can be interpreted as the odds of infection for the reference group. The each of the offset terms are the odds ratios compared to the reference group. We see that breast milk along with formula has only $84\%$ of the odds of respiratory disease as a formula only baby, and a breast milk fed child only has $51\%$ of the odds for respiratory disease as the formula fed baby. We can look at confidence intervals for the odds ratios by the following:

```{r}
exp( confint(m1) )
```

We should be careful in drawing conclusions here because this study was a retrospective study and the decision to breast feed a baby vs feeding with formula is inextricably tied to socio-economic status and we should investigate if the effect measured is due to feeding method or some other lurking variable tied to socio-economic status.

## Prediction and Effective Dose Levels

To demonstrate the ideas in this section, we'll use a toxicology study that examined insect mortality as a function of increasing concentrations of an insecticide. 

```{r}
data(bliss, package='faraway')
```

We first fit the logistic regression model and plot the results

```{r}
m1 <- glm( cbind(alive, dead) ~ conc, family=binomial, data=bliss)
```

Given this, we want to develop a confidence interval for the probabilities by first calculating using the following formula. As usual, we recall that the $y$ values live in $\left(-\infty,\infty\right)$. 
$$CI_{y}:\,\,\,\hat{y}\pm z^{1-\alpha/2}\,StdErr\left(\hat{y}\right)$$
We must then convert this to the $\left[ 0,1 \right]$ space using the $\textrm{ilogit}()$ function. $$CI_{p}=\textrm{ilogit}\left(CI_{y}\right)$$
 

```{r}
probs <- data.frame(conc=seq(0,4,by=.1))
yhat <- predict(m1, newdata=probs, se.fit=TRUE)  # list with two elements fit and se.fit
yhat <- data.frame( fit=yhat$fit, se.fit = yhat$se.fit)
probs <- cbind(probs, yhat)
head(probs)
```
```{r}
probs <- probs %>% mutate(
  phat  = ilogit(fit),
  lwr   = ilogit( fit - 1.96 * se.fit ),
  upr   = ilogit( fit + 1.96 * se.fit ))
ggplot(bliss, aes(x=conc)) +
  geom_point(aes(y=alive/(alive+dead))) + 
  geom_line(data=probs, aes(y=phat), color='red') +
  geom_ribbon(data=probs, aes(ymin=lwr, ymax=upr), fill='red', alpha=.3) +
  ggtitle('Bliss Insecticide Data') +
  xlab('Concentration') + ylab('Proportion Alive')
```

The next thing we want to do is come up with a confidence intervals for the concentration level that results in the death of $100(p)\%$ of the insects. Often we are interested in the case of $p=0.5$. This is often called LD50, which is the lethal dose for 50% of the population. Using the link function you can set the $p$ value and solve for the concentration value to find 
$$\hat{x}_{p}=\frac{\textrm{logit}\left(p\right)-\hat{\beta}_{0}}{\hat{\beta}_{1}}$$
which gives us a point estimate of LD(p). To get a confidence interval we need to find the standard error of $\hat{x}_{p}$. Since this is a non-linear function of $\hat{\beta}_{0}$ and $\hat{\beta}_{1}$ which are correlated, we must be careful in the calculation. The actual calculation is done using the Delta Method Approximation:
$$Var\left(g\left(\hat{\boldsymbol{\theta}}\right)\right)=g'\left(\boldsymbol{\theta}\right)^{T}Var\left(\boldsymbol{\theta}\right)g'\left(\boldsymbol{\theta}\right)$$
Fortunately we don't have to do these calculations by hand and can use the `dose.p()` function in the `MASS` package. 

```{r}
LD <- dose.p(m1, p=c(.25, .5, .75))
LD
```

and we can use these to create approximately confidence intervals for these $\hat{x}_{p}$ values via
$$\hat{x}_{p}\pm z^{1-\alpha/2}\,StdErr\left(\hat{x}_{p}\right)$$
```{r}
# why did the MASS authors make LD a vector of the 
# estimated values and have an additional attribute 
# that contains the standard errors?  Whatever, lets
# turn this into a convential data.frame.
str(LD) 
CI <- data.frame(p = attr(LD,'p'),
                 Dose = as.vector(LD),
                 SE   = attr(LD,'SE')) %>%  # save the output table as LD
  mutate( lwr = Dose - qnorm(.975)*SE,
          upr = Dose + qnorm(.975)*SE )
CI
```


## Overdispersion

In the binomial distribution, the variance is a function of the probability of success and is $$Var\left(W\right)=np\left(1-p\right)$$
but there are many cases where we might be interested in adding an additional variance parameter $\phi$ to the model. A common reason for overdispersion to appear is that we might not have captured all the covariates that influence $p$. 

We can do a quick simulation to demonstrate that additional variability in $p$ leads to addition variability overall. 

```{r}
N <- 1000 
n <- 10   
p <- .6 
overdispersed_p <- p + rnorm(n, mean=0, sd=.05)
sim.data <- NULL
for( i in 1:N ){ 
  sim.data <- sim.data %>% rbind(data.frame(
    var = var( rbinom(N, size=n, prob=p)),
    type = 'Standard'))
  sim.data <- sim.data %>% rbind(data.frame(
    var = var( rbinom(N, size=n, prob=overdispersed_p )),
    type = 'OverDispersed'))
} 
true.var <- p*(1-p)*n
ggplot(sim.data, aes(x=var, y=..density..)) +
  geom_histogram(bins=30) +
  geom_vline(xintercept = true.var, color='red') +
  facet_grid(type~.) +
  ggtitle('Histogram of Sample Variances') 
```

We see that the sample variances fall neatly about the true variance of $2.4$ in the case where the data is distributed with a constant value for $p$. However adding a small amount of random noise about the parameter $p$, and we'd have more variance in the samples.

fig.height=4
N <- 1000 
The extra uncertainty of the probability of success results in extra variability in the responses. 

We can recognize when overdispersion is present by examining the deviance of our model. Because the deviance is approximately distributed 
$$D\left(\boldsymbol{y},\boldsymbol{\theta}\right)\stackrel{\cdot}{\sim}\chi_{df}^{2}$$
where $df$ is the residual degrees of freedom in the model. Because the $\chi_{k}^{2}$ is the sum of $k$ independent, squared standard normal random variables, it has an expectation $k$ and variance $2k$. For binomial data with group sizes (say larger than 5), this approximation isn't too bad and we can detect overdispersion. For binary responses, the approximation is quite poor and we cannot detect overdispersion.

The simplest approach for modeling overdispersion is to introduce an addition dispersion parameter $\sigma^{2}$. This dispersion parameter may be estimated using 
$$\hat{\sigma}^{2}=\frac{X^{2}}{n-p}.$$
With the addition of the overdispersion parameter to the model, the differences between a simple and complex model is no longer distributed $\chi^{2}$ and we must use the following approximate F-statistic
$$F=\frac{\left(D_{simple}-D_{complex}\right)/\left(df_{small}-df_{large}\right)}{\hat{\sigma}^{2}}$$
 
Using the F-test when the the overdispersion parameter is 1 is a less powerful test than the $\chi^{2}$ test, so we'll only use the F-test when the overdispersion parameter must be estimated. 

Example: We consider an experiment where at five different stream locations, four boxes of trout eggs were buried and retrieved at four different times after the original placement. The number of surviving eggs was recorded and the eggs disposed of.

```{r}
data(troutegg, package='faraway') 
troutegg <- troutegg %>% 
  mutate( perish = total - survive) %>% 
  dplyr::select(location, period, survive, perish, total) %>%
  arrange(location, period)

troutegg %>% arrange(location, period)
```

We can first visualize the data
```{r}
ggplot(troutegg, aes(x=period, y=survive/total, color=location)) +    
  geom_point(aes(size=total)) +    
  geom_line(aes(x=as.integer(period)))
```


We can fit the logistic regression model (noting that the model with the interaction of location and period would be saturated):

```{r}
m <- glm(cbind(survive,perish) ~ location + period, family=binomial, data=troutegg)
summary(m)
```

The residual deviance seems a little large. With $12$ residual degrees of freedom, the deviance should be near $12$. We can confirm that the deviance is quite large via:

```{r}
1 - pchisq( 64.5, df=12 )
```

We therefore estimate the overdispersion parameter

```{r}
sigma2 <- sum( residuals(m, type='pearson') ^2 ) / 12 
sigma2
```

and note that this is quite a bit larger than $1$, which is what it should be in the non-overdispersed setting. Using this we can now test the significance of the effects of location and period.

```{r}
drop1(m, scale=sigma2, test='F')

```
and conclude that both location and period are significant predictors of trout egg survivorship. 

We could have avoided having to calculate $\hat{\sigma}^{2}$ by hand by simply using the `quasibinomial` family instead of the binomial.

```{r}
m2 <- glm(cbind(survive,perish) ~ location + period, 
          family=quasibinomial, data=troutegg) 
summary(m2)
drop1(m2, test='F')
```

While each of the time periods is different than the first, it looks like periods 7,8, and 11 are different from each other. As usual, we need to turn to the `lsmeans` package.

```{r}
lsmeans(m2, pairwise~period)
```

We would like the letter groups as well.
```{r}
cld(lsmeans(m2, pairwise~period), Letters=letters)
```

Looking at this experiment, we might consider that location really ought to be a random effect. Fortunately lme4 supports the family option, although it will not accept quasi families, so hand calculation of the scale parameter is necessary and as are many of the test statistics. Linear mixed models are tricky and even more so in the generalized case.

## Exercises

1. The dataset `faraway::wbca` comes from a study of breast cancer in Wisconsin. There are 681 cases of potentially cancerous tumors of which 238 are actually malignant (ie cancerous). Determining whether a tumor is really malignant is traditionally determined by an invasive surgical procedure. The purpose of this study was to determine whether a new procedure called 'fine needle aspiration', which draws only a small sample of tissue, could be effective in determining tumor status.
    a. Fit a binomial regression with `Class` as the response variable and the other nine variables as predictors (for consistency among students, define a success as the tumor being benign and remember that glm wants the response to be a matrix where the first column is the number of successes). Report the residual deviance and associated degrees of freedom. Can this information be used to determine if this model fits the data?
    b. Use AIC as the criterion to determine the best subset of variables using the step function.
    c. Use the reduced model to give the estimated probability that a tumor with associated predictor variables
        ```{r}
        newdata <- data.frame( Adhes=1, BNucl=1, Chrom=3, Epith=2, Mitos=1, 
                               NNucl=1, Thick=4, UShap=1, USize=1)
        ```
        is benign and give a confidence interval for your estimate.
    d. Suppose that a cancer is classified as benign if $\hat{p}>0.5$ and malignant if $\hat{p}\le0.5$. Compute the number of errors of both types that will be made if this method is applied to the current data with the reduced model. *Hint: save the $\hat{p}$ as a column in the wbca data frame and use that to create a new column `Est_Class` which is the estimated class (making sure it is the same encoding scheme as Class). Then use dplyr functions to create a table of how many rows fall into each of the four Class/Est_Class combinations.*
    e. Suppose we changed the cutoff to $0.9$. Compute the number of errors of each type in this case. Discuss the ethical issues in determining the cutoff.

2. Aflatoxin B1 was fed to lab animals at various doses and the number responding with liver cancer recorded and is available in the dataset `faraway::aflatoxin`. 
    a. Build a model to predict the occurrence of liver cancer. Consider a square-root transformation to the dose level.
    b. Compute the ED50 level (effective dose level... same as LD50 but isn't confined to strictly lethal effects) and an approximate $95\%$ confidence interval.

3. The dataset `faraway::pima` is data from a study of adult female Pima Indians living near Phoenix was done and resulted $n=752$ observations after the cases of missing data (obnoxiously coded as 0) were removed. Testing positive for diabetes was the success (`test`) and the predictor variables we will use are: `pregnant`, `glucose`, and `bmi`.
    a. Remove the observations that have missing data (coded as a zero) for either `glucose` or `bmi`. *The researcher's choice of using 0 to represent missing data is a bad idea because 0 is a valid value for the number of pregnancies, so assume a zero in the `pregnant` covariate is a true value. The `dplyr` function `filter` could be used here.*
    b. Fit the logistic regression model for `test` with using the main effects of `glucose`, `bmi`, and `pregnant`.
    c. Produce a graphic that displays the relationship between the variables. *Notice I've done the part (a) for you and the assume that your model produced in part (b) is named `m`. I also split up the pregnancy and bmi values into some logical grouping for the visualization. If you've never used the `cut` function, go look it up because it is extremely handy.*
        ```{r, eval=FALSE}
        pima <- pima %>% filter( bmi != 0, glucose != 0)
        pima <- pima %>% mutate( 
          phat=ilogit(predict(m)),
          pregnant.grp = cut(pregnant, c(0,1,3,6,100), right = FALSE, labels = c('0','1:2','3:5','6+')),
          bmi.grp = cut(bmi, c(0,18,25,30,100), labels=c('Underweight','Normal','Overweight','Obese')))
        ggplot(pima, aes(y=test, x=glucose)) +
          geom_point() +
          geom_line(aes(y=phat), color='red') +
          facet_grid(bmi.grp ~ pregnant.grp)
        ```
    d. Discuss the quality of your predictions based on the graphic above and modify your model accordingly.    
    e. Give the probability of testing positive for diabetes for a Pima woman who had had no pregnancies, had `bmi=28` and a glucose level of `110`.
    f. Give the odds that the same woman would test positive for diabetes.
    e. How do her odds change to if she were to have a child? That is to say, what is the odds ratio for that change?


<!--chapter:end:12_BinomialRegression.Rmd-->

