[
["inference.html", "Chapter 3 Inference 3.1 F-tests 3.2 Prediction and Confidence Intervals for a response 3.3 InterpretationSections 3.7 - 3.9 in Faraway", " Chapter 3 Inference 3.1 F-tests We wish to develop a rigorous way to compare nested models and decide if a complicated model explains enough more variability than a simple model to justify the additional intellectual effort of thinking about the data in the complicated fashion. It is important to specify that we are developing a way of testing nested models. By nested, we mean that the simple model can be created from the full model just by setting one or more model parameters to zero. 3.1.1 Theory Recall that in the simple regression and ANOVA cases we were interested in comparing a simple model versus a more complex model. For each model we computed the residual sum of squares (RSS) and said that if the complicated model performed much better than the simple then \\(RSS_{simple}\\gg RSS_{complex}\\). To do this we needed to standardize by the number of parameters added to the model and the degrees of freedom remaining in the full model. We first defined \\(RSS_{diff}=RSS_{simple}-RSS_{complex}\\) and let \\(df_{diff}\\) be the number of parameters difference between the simple and complex models. Then we had \\[F=\\frac{RSS_{difference}/df_{diff}}{RSS_{complex}/df_{complex}}\\] and we claimed that if the null hypothesis was true (i.e. the complex model is an unnecessary obfuscation of the simple), then this ratio follows an F -distribution with degrees of freedom \\(df_{diff}\\) and \\(df_{complex}\\). The critical assumption for the F-test to be appropriate is that the error terms are independent and normally distributed with constant variance. We will consider a data set from Johnson and Raven (1973) which also appears in Weisberg (1985). This data set is concerned with the number of tortoise species on \\(n=30\\) different islands in the Galapagos. The variables of interest in the data set are: Variable Description Species Number of tortoise species found on the island Endimics Number of tortoise species endemic to the island Elevation Elevation of the highest point on the island Area Area of the island (km\\(^2\\)) Nearest Distance to the nearest neighboring island (km) Scruz Distance to the Santa Cruz islands (km) Adjacent Area of the nearest adjacent island (km\\(^2\\)) We will first read in the data set from the package faraway. library(faraway) # load the library data(gala) # import the data set head(gala) # show the first couple of rows ## Species Endemics Area Elevation Nearest Scruz Adjacent ## Baltra 58 23 25.09 346 0.6 0.6 1.84 ## Bartolome 31 21 1.24 109 0.6 26.3 572.33 ## Caldwell 3 3 0.21 114 2.8 58.7 0.78 ## Champion 25 9 0.10 46 1.9 47.4 0.18 ## Coamano 2 1 0.05 77 1.9 1.9 903.82 ## Daphne.Major 18 11 0.34 119 8.0 8.0 1.84 First we will create the full model that predicts the number of species as a function of elevation, area, nearest, scruz and adjacent. Notice that this model has \\(p=6\\) \\(\\beta_{i}\\) values (one for each coefficient plus the intercept). M.c &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala) summary(M.c) ## ## Call: ## lm(formula = Species ~ Area + Elevation + Nearest + Scruz + Adjacent, ## data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -111.679 -34.898 -7.862 33.460 182.584 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.068221 19.154198 0.369 0.715351 ## Area -0.023938 0.022422 -1.068 0.296318 ## Elevation 0.319465 0.053663 5.953 3.82e-06 *** ## Nearest 0.009144 1.054136 0.009 0.993151 ## Scruz -0.240524 0.215402 -1.117 0.275208 ## Adjacent -0.074805 0.017700 -4.226 0.000297 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.98 on 24 degrees of freedom ## Multiple R-squared: 0.7658, Adjusted R-squared: 0.7171 ## F-statistic: 15.7 on 5 and 24 DF, p-value: 6.838e-07 3.1.2 Testing All Covariates The first test we might want to do is to test if any of the covariates are significant. That is to say that we want to test the full model versus the simple null hypothesis model \\[y_{i}=\\beta_{0}+\\epsilon_{i}\\] that has no covariates and only a y-intercept. So we will create a simple model M.s &lt;- lm(Species ~ 1, data=gala) and calculate the appropriate Residual Sums of Squares (RSS) for each model, along with the difference in degrees of freedom between the two models. RSS.c &lt;- sum(resid(M.c)^2) RSS.s &lt;- sum(resid(M.s)^2) df.diff &lt;- 5 # complex model has 5 additional parameters df.c &lt;- 30 - 6 # complex model has 24 degrees of freedom left The F-statistic for this test is therefore F.stat &lt;- ( (RSS.s - RSS.c) / df.diff ) / ( RSS.c / df.c ) F.stat ## [1] 15.69941 and should be compared against the F-distribution with \\(5\\) and \\(24\\) degrees of freedom. Because a large difference between RSS.s and RSS.c would be evidence for the alternative, larger model, the p-value for this test is \\[p-value=P\\left(F_{5,24}\\ge\\mathtt{F.stat}\\right)\\] p.value &lt;- 1 - pf(15.699, 5, 24) p.value ## [1] 6.839486e-07 Both the F.stat and its p-value are given at the bottom of the summary table. However, I might be interested in creating an ANOVA table for this situation. Source df Sum Sq Mean Sq F p-value Difference \\(p-1\\) \\(RSS_d\\) \\(MSE_d = RSS_d / (p-1)\\) \\(MSE_d/MSE_c\\) \\(P(F &gt; F_{p-1,n-p})\\) Complex \\(n-p\\) \\(RSS_c\\) \\(MSE_c = RSS_c / (n-p)\\) Simple \\(n-1\\) \\(RSS_s\\) This table can be obtained from R by using the anova() function on the two models of interest. As usual with R, it does not show the simple row, but rather concentrates on the difference row. anova(M.s, M.c) ## Analysis of Variance Table ## ## Model 1: Species ~ 1 ## Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 29 381081 ## 2 24 89231 5 291850 15.699 6.838e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 3.1.3 Testing a Single Covariate For a particular covariate, \\(\\beta_{j}\\), we might wish to perform a test to see if it can be removed from the model. It can be shown that the F-statistic can be re-written as \\[\\begin{aligned} F &amp;= \\frac{\\left[RSS_{s}-RSS_{c}\\right]/1}{RSS_{c}/\\left(n-p\\right)}\\\\ &amp;= \\vdots\\\\ &amp;= \\left[\\frac{\\hat{\\beta_{j}}}{SE\\left(\\hat{\\beta}_{j}\\right)}\\right]^{2}\\\\ &amp;= t^{2} \\end{aligned}\\] where \\(t\\) has a t-distribution with \\(n-p\\) degrees of freedom under the null hypothesis that the simple model is sufficient. We consider the case of removing the covariate Area from the model and will calculate our test statistic using both methods. M.c &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala) M.s &lt;- lm(Species ~ Elevation + Nearest + Scruz + Adjacent, data=gala) RSS.c &lt;- sum( resid(M.c)^2 ) RSS.s &lt;- sum( resid(M.s)^2 ) df.d &lt;- 1 df.c &lt;- 30-6 F.stat &lt;- ((RSS.s - RSS.c)/1) / (RSS.c / df.c) F.stat ## [1] 1.139792 1 - pf(F.stat, 1, 24) ## [1] 0.296318 sqrt(F.stat) ## [1] 1.067611 To calculate it using the estimated coefficient and its standard error, we must grab those values from the summary table temp &lt;- summary(M.c) temp$coefficients ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.068220709 19.15419782 0.369016796 7.153508e-01 ## Area -0.023938338 0.02242235 -1.067610554 2.963180e-01 ## Elevation 0.319464761 0.05366280 5.953187968 3.823409e-06 ## Nearest 0.009143961 1.05413595 0.008674366 9.931506e-01 ## Scruz -0.240524230 0.21540225 -1.116628222 2.752082e-01 ## Adjacent -0.074804832 0.01770019 -4.226216850 2.970655e-04 beta.area &lt;- temp$coefficients[2,1] SE.beta.area &lt;- temp$coefficients[2,2] t &lt;- beta.area / SE.beta.area t ## [1] -1.067611 2 * pt(t, 24) ## [1] 0.296318 All that hand calculation is tedious, so we can again use the anova() command to compare the two models. anova(M.s, M.c) ## Analysis of Variance Table ## ## Model 1: Species ~ Elevation + Nearest + Scruz + Adjacent ## Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 25 93469 ## 2 24 89231 1 4237.7 1.1398 0.2963 3.1.4 Testing a Subset of Covariates Often a researcher will want to remove a subset of covariates from the model. In the Galapagos example, Area, Nearest, and Scruz all have non-significant p-values and would be removed when comparing the full model to the model without that one covariate. While each of them might be non-significant, is the sum of all three significant? Because the individual \\(\\hat{\\beta}_{j}\\) values are not independent, then we cannot claim that the subset is not statistically significant just because each variable in turn was insignificant. Instead we again create simple and complex models in the same fashion as we have previously done. M.c &lt;- lm(Species ~ Area + Elevation + Nearest + Scruz + Adjacent, data=gala) M.s &lt;- lm(Species ~ Elevation + Adjacent, data=gala) anova(M.s, M.c) ## Analysis of Variance Table ## ## Model 1: Species ~ Elevation + Adjacent ## Model 2: Species ~ Area + Elevation + Nearest + Scruz + Adjacent ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 27 100003 ## 2 24 89231 3 10772 0.9657 0.425 We find a large p-value associated with this test and can safely stay with the null hypothesis, that the simple model is sufficient to explain the observed variability in the number of species of tortoise. 3.1.5 Confidence Intervals for location parameters Recall that \\[\\hat{\\boldsymbol{\\beta}}\\sim N\\left(\\boldsymbol{\\beta},\\,\\sigma^{2}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\right)\\] and it is easy to calculate the estimate of \\(\\sigma^{2}\\). This estimate will be the “average” squared residual \\[\\hat{\\sigma}^{2}=\\frac{RSS}{df}\\] where \\(RSS\\) is the residual sum of squares and \\(df\\) is the degrees of freedom \\(n-p\\) where \\(p\\) is the number of \\(\\beta_{j}\\) parameters. Therefore the standard error of the \\(\\hat{\\beta}_{j}\\) values is \\[SE\\left(\\hat{\\beta}_{j}\\right)=\\sqrt{\\hat{\\sigma}^{2}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)_{jj}^{-1}}\\] We can see this calculation in the summary regression table. We again consider the Galapagos Island data set. First we must create the design matrix y &lt;- gala$Species X &lt;- cbind( rep(1,30), gala$Elevation, gala$Adjacent ) And then create \\(\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)^{-1}\\) XtXinv &lt;- solve( t(X) %*% X ) XtXinv ## [,1] [,2] [,3] ## [1,] 6.094829e-02 -8.164025e-05 9.312123e-06 ## [2,] -8.164025e-05 2.723835e-07 -7.126027e-08 ## [3,] 9.312123e-06 -7.126027e-08 6.478031e-08 diag(XtXinv) ## [1] 6.094829e-02 2.723835e-07 6.478031e-08 Eventually we will need \\(\\hat{\\boldsymbol{\\beta}}\\) beta.hat &lt;- XtXinv %*% t(X) %*% y beta.hat ## [,1] ## [1,] 1.4328722 ## [2,] 0.2765683 ## [3,] -0.0688855 And now find the estimate \\(\\hat{\\sigma}\\) H &lt;- X %*% XtXinv %*% t(X) y.hat &lt;- H %*% y RSS &lt;- sum( (y-y.hat)^2 ) sigma.hat &lt;- sqrt( RSS/(30-3) ) sigma.hat ## [1] 60.85898 The standard errors of \\(\\hat{\\beta}\\) is thus sqrt( sigma.hat^2 * diag(XtXinv) ) ## [1] 15.02468680 0.03176253 0.01548981 We can double check that this is what R calculates in the summary table model &lt;- lm(Species ~ Elevation + Adjacent, data=gala) summary(model) ## ## Call: ## lm(formula = Species ~ Elevation + Adjacent, data = gala) ## ## Residuals: ## Min 1Q Median 3Q Max ## -103.41 -34.33 -11.43 22.57 203.65 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.43287 15.02469 0.095 0.924727 ## Elevation 0.27657 0.03176 8.707 2.53e-09 *** ## Adjacent -0.06889 0.01549 -4.447 0.000134 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.86 on 27 degrees of freedom ## Multiple R-squared: 0.7376, Adjusted R-squared: 0.7181 ## F-statistic: 37.94 on 2 and 27 DF, p-value: 1.434e-08 It is highly desirable to calculate confidence intervals for the regression parameters. Recall that the general form of a confidence interval is \\[Estimate\\;\\pm Critical\\,Value\\;\\cdot\\;StandardError\\left(Estimate\\right)\\] For any specific \\(\\beta_{j}\\) we will have \\[\\hat{\\beta}_{j}\\pm t_{n-p}^{1-\\alpha/2}\\,\\hat{\\sigma}\\sqrt{\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)_{jj}^{-1}}\\] where \\(\\hat{\\sigma}^{2}\\left(\\mathbf{X}^{T}\\mathbf{X}\\right)_{jj}^{-1}\\) is the \\([j,j]\\) element of the variance/covariance of \\(\\hat{\\boldsymbol{\\beta}}\\). To demonstrate this, we return to the Galapagos Island data set. Finally we can calculate confidence intervals for our three \\(\\beta_{j}\\) values lower &lt;- beta.hat - qt(.975, 27) * sigma.hat * sqrt( diag(XtXinv) ) upper &lt;- beta.hat + qt(.975, 27) * sigma.hat * sqrt( diag(XtXinv) ) cbind(lower, upper) ## [,1] [,2] ## [1,] -29.395239 32.26098305 ## [2,] 0.211397 0.34173962 ## [3,] -0.100668 -0.03710303 That is certainly a lot of work to do by hand (even with R doing all the matrix multiplication) but we can get these from R by using the confint() command. confint(model) ## 2.5 % 97.5 % ## (Intercept) -29.395239 32.26098305 ## Elevation 0.211397 0.34173962 ## Adjacent -0.100668 -0.03710303 3.2 Prediction and Confidence Intervals for a response Given a vector of predictor covariates \\(\\boldsymbol{x}_{0}\\) (think of \\(\\boldsymbol{x}_{0}^{T}\\) as potentially one row in \\(\\boldsymbol{X}\\). Because we might want to predict some other values than what we observe, we do not restrict ourselves to only rows in ), we want to make inference on the expected value \\(\\hat{y}_{0}\\). We can calculate the value by \\[\\hat{y}_{0}=\\boldsymbol{x}_{0}^{T}\\hat{\\boldsymbol{\\beta}}\\] and we are interested in two different types of predictions. We might be interested in the uncertainty of a new data point. This uncertainty has two components: the uncertainty of the regression model and uncertainty of a new data point from its expected value. Second, we might be interested in only the uncertainty about the regression model. We note that because \\(\\boldsymbol{x}_{0}^{T}\\) is just a constant, we can calculate the variance of this value as \\[ \\begin{aligned} Var\\left(\\boldsymbol{x}_{0}^{T}\\hat{\\boldsymbol{\\beta}}\\right) &amp;= \\boldsymbol{x}_{0}^{T}\\,Var\\left(\\hat{\\boldsymbol{\\beta}}\\right)\\,\\boldsymbol{x}_{0} \\\\ &amp;= \\boldsymbol{x}_{0}^{T}\\,\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\sigma^{2}\\,\\boldsymbol{x}_{0} \\\\ &amp;= \\boldsymbol{x}_{0}^{T}\\,\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\,\\boldsymbol{x}_{0}\\,\\sigma^{2} \\end{aligned}\\] and use this to calculate two types of intervals. First, a prediction interval for a new observation is \\[\\hat{y}_{0}\\pm t_{n-p}^{1-\\alpha/2}\\,\\hat{\\sigma}\\sqrt{1+\\boldsymbol{x}_{0}^{T}\\,\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\,\\boldsymbol{x}_{0}}\\] and a confidence interval for the mean response for the given \\(\\boldsymbol{x}_{0}\\) is \\[\\hat{y}_{0}\\pm t_{n-p}^{1-\\alpha/2}\\,\\hat{\\sigma}\\sqrt{\\boldsymbol{x}_{0}^{T}\\,\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\,\\boldsymbol{x}_{0}}\\] Again using the Galapagos Island data set as an example, we might be interested in predicting the number of tortoise species of an island with highest point \\(400\\) meters and nearest adjacent island with area \\(200 km^{2}\\). We then have \\[\\boldsymbol{x}_{0}^{T} = \\left[\\begin{array}{ccc}1 &amp; 400 &amp; 200\\end{array}\\right]\\] and we can calculate x0 &lt;- c(1, 400, 200) y0 &lt;- t(x0) %*% beta.hat y0 ## [,1] ## [1,] 98.28309 and then calculate \\(\\boldsymbol{x}_{0}^{T}\\,\\left(\\boldsymbol{X}^{T}\\boldsymbol{X}\\right)^{-1}\\,\\boldsymbol{x}_{0}\\) xt.XtXinv.x &lt;- t(x0) %*% solve( t(X) %*% X ) %*% x0 Thus the prediction interval will be c(y0 - qt(.975, 27) * sigma.hat * sqrt(1 + xt.XtXinv.x), y0 + qt(.975, 27) * sigma.hat * sqrt(1 + xt.XtXinv.x)) ## [1] -28.70241 225.26858 while a confidence interval for the expectation is c(y0 - qt(.975, 27) * sigma.hat * sqrt(xt.XtXinv.x), y0 + qt(.975, 27) * sigma.hat * sqrt(xt.XtXinv.x)) ## [1] 75.21317 121.35301 These prediction and confidence intervals can be calculated in R using the predict() function x0 &lt;- data.frame(Elevation=400, Adjacent=200) predict(model, newdata=x0, interval=&#39;prediction&#39;) ## fit lwr upr ## 1 98.28309 -28.70241 225.2686 predict(model, newdata=x0, interval=&#39;confidence&#39;) ## fit lwr upr ## 1 98.28309 75.21317 121.353 3.3 InterpretationSections 3.7 - 3.9 in Faraway The standard interpretation of the slope parameter is that _{j} is the amount of increase in y if the j th covariate were to increase by 1 , provided that all other covariates stayed the same. The difficulty with this interpretation is that covariates are often related, and the phrase “all other covariates stayed the same” is often not reasonable. For example, if we have a dataset that models the mean annual temperature of a location as a function of latitude, longitude, and elevation, then it is not physically possible to hold latitude, and longitude constant while changing elevation. One common issue that make interpretation difficult is that covariates can be highly correlated. Perch Example: We might be interested in estimating the weight of a fish based off of its length and width. The dataset we will consider is from fishes are caught from the same lake (Laengelmavesi) near Tampere in Finland. The following variables were observed: We first look at the data and observe the expected relationship between length and weight. Naively, we might consider the linear model with all the length effects present. "],
["setwddropboxnauteachingsta-571notesc1-3-inference.html", "Chapter 4 setwd(‘~/Dropbox/NAU/Teaching/STA 571/Notes/c1.3_Inference’)", " Chapter 4 setwd(‘~/Dropbox/NAU/Teaching/STA 571/Notes/c1.3_Inference’) &lt;&lt;fig.height=4&gt;&gt;=fish &lt;- read.table(‘~/Dropbox/NAU/Teaching/STA 571/Notes/c1.3_Inference/Fish.csv’, header=TRUE, skip=111, sep=‘,’) pairs(fish[,c(‘Weight’,‘Length.1’,‘Length.2’,‘Length.3’,‘Height’,‘Width’)])@ We might then naively create a linear model that has all of the length measurements. model &lt;- lm(Weight ~ Length.1 + Length.2 + Length.3 + Height + Width, data=fish) This is crazy. There is a negative relationship between Length.2 and Weight. That doesn’t make any sense unless you realize that this is the effect of Length.2 assuming the other covariates are in the model and can be held constant while changing the value of Length.2, which is obviously ridiculous. If we remove the highly correlated covariates then we see a much better behaved model model &lt;- lm(Weight ~ Length.2 + Height + Width, data=fish) When you have two variables in a model that are highly positively correlated, you often find that one will have a positive coefficient and the other will be negative. Likewise, if two variables are highly negatively correlated, the two regression coefficients will often be the same sign. In this case the sum of the three length variables was approximately 31 in both cases, but with three length variables, the second could be negative the third be positive with approximately the same magnitude and we get approximately the same model as with both the second and third length variables missing from the model. In general, you should be very careful with the interpretation of the regression coefficients when the covariates are highly correlated. We will talk about how to recognize these situations and what to do about them later in the course. "]
]
